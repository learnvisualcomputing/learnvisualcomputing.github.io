<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>17&nbsp; Image Sensor Architecture – Foundations of Visual Computing</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./imaging-noise.html" rel="next">
<link href="./imaging-optics.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-485d01fc63b59abcd3ee1bf1e8e2748d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://hypothes.is/embed.js"></script>
<script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./imaging.html">Imaging</a></li><li class="breadcrumb-item"><a href="./imaging-sensor.html"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Image Sensor Architecture</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Foundations of Visual Computing</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/learnvisualcomputing/learnvisualcomputing.github.io" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./Foundations-of-Visual-Computing.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.linkedin.com/sharing/share-offsite/?url=|url|">
              <i class="bi bi-linkedin pe-1"></i>
            LinkedIn
            </a>
          </li>
      </ul>
    </div>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">An Invitation to Visual Computing</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./hvs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Human Visual System</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./hvs-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">From Light to Vision</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./hvs-receptor.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Photoreceptors</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./hvs-color.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Color Vision</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./hvs-colorimetry.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Colorimetry</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./hvs-retinalmodel.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Modeling Retinal Computation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./hvs-adaptation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Visual Adaptations and Constancy</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./rendering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Rendering</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./rendering-overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Overview</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./rendering-radiometry.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Radiometry and Photometry</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./rendering-lightfield.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Light Field</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./rendering-re.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Rendering Surface Scattering</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./rendering-surface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Modeling Material Surface</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./rendering-sss.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Volume and Subsurface Scattering Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./rendering-rte.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Rendering Volume and Subsurface Scattering</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./rendering-nflux.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">The N-Flux Theory</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./imaging.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Imaging</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./imaging-optics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Imaging Optics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./imaging-sensor.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Image Sensor Architecture</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./imaging-noise.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Noise</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./imaging-isp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Image Signal Processing</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./display.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Display</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./display-optics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Optical Mechanisms</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./display-electronics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Backplane and Processing Architecture</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-chpt-imaging-sensor-ov" id="toc-sec-chpt-imaging-sensor-ov" class="nav-link active" data-scroll-target="#sec-chpt-imaging-sensor-ov"><span class="header-section-number">17.1</span> Overview</a></li>
  <li><a href="#sec-chpt-imaging-sensor-pixel" id="toc-sec-chpt-imaging-sensor-pixel" class="nav-link" data-scroll-target="#sec-chpt-imaging-sensor-pixel"><span class="header-section-number">17.2</span> From Photons to Charges and Digital Numbers</a>
  <ul class="collapse">
  <li><a href="#sec-chpt-imaging-sensor-pixel-pe" id="toc-sec-chpt-imaging-sensor-pixel-pe" class="nav-link" data-scroll-target="#sec-chpt-imaging-sensor-pixel-pe"><span class="header-section-number">17.2.1</span> Photons to Charges</a></li>
  <li><a href="#sec-chpt-imaging-sensor-pixel-charge" id="toc-sec-chpt-imaging-sensor-pixel-charge" class="nav-link" data-scroll-target="#sec-chpt-imaging-sensor-pixel-charge"><span class="header-section-number">17.2.2</span> Measuring Charges</a></li>
  <li><a href="#sec-chpt-imaging-sensor-pixel-ro" id="toc-sec-chpt-imaging-sensor-pixel-ro" class="nav-link" data-scroll-target="#sec-chpt-imaging-sensor-pixel-ro"><span class="header-section-number">17.2.3</span> Read-out Circuitry</a></li>
  <li><a href="#sec-chpt-imaging-sensor-pixel-dr" id="toc-sec-chpt-imaging-sensor-pixel-dr" class="nav-link" data-scroll-target="#sec-chpt-imaging-sensor-pixel-dr"><span class="header-section-number">17.2.4</span> Dynamic Range</a></li>
  </ul></li>
  <li><a href="#sec-chpt-imaging-sensor-arch" id="toc-sec-chpt-imaging-sensor-arch" class="nav-link" data-scroll-target="#sec-chpt-imaging-sensor-arch"><span class="header-section-number">17.3</span> Global Architecture</a>
  <ul class="collapse">
  <li><a href="#sec-chpt-imaging-sensor-arch-col" id="toc-sec-chpt-imaging-sensor-arch-col" class="nav-link" data-scroll-target="#sec-chpt-imaging-sensor-arch-col"><span class="header-section-number">17.3.1</span> Column-Parallel Readout</a></li>
  <li><a href="#sec-chpt-imaging-sensor-arch-shut" id="toc-sec-chpt-imaging-sensor-arch-shut" class="nav-link" data-scroll-target="#sec-chpt-imaging-sensor-arch-shut"><span class="header-section-number">17.3.2</span> Rolling vs.&nbsp;Global Shutter</a></li>
  <li><a href="#sec-chpt-imaging-sensor-arch-ro" id="toc-sec-chpt-imaging-sensor-arch-ro" class="nav-link" data-scroll-target="#sec-chpt-imaging-sensor-arch-ro"><span class="header-section-number">17.3.3</span> Pixel-Parallel and Chip-Level Readout</a></li>
  <li><a href="#sec-chpt-imaging-sensor-arch-ccd" id="toc-sec-chpt-imaging-sensor-arch-ccd" class="nav-link" data-scroll-target="#sec-chpt-imaging-sensor-arch-ccd"><span class="header-section-number">17.3.4</span> CMOS vs.&nbsp;CCD Sensor</a></li>
  <li><a href="#sec-chpt-imaging-sensor-arch-comp" id="toc-sec-chpt-imaging-sensor-arch-comp" class="nav-link" data-scroll-target="#sec-chpt-imaging-sensor-arch-comp"><span class="header-section-number">17.3.5</span> Computational and Stacked CMOS Image Sensors</a></li>
  </ul></li>
  <li><a href="#sec-chpt-imaging-sensor-optics" id="toc-sec-chpt-imaging-sensor-optics" class="nav-link" data-scroll-target="#sec-chpt-imaging-sensor-optics"><span class="header-section-number">17.4</span> In-Sensor Optics</a>
  <ul class="collapse">
  <li><a href="#sec-chpt-imaging-sensor-optics-filters" id="toc-sec-chpt-imaging-sensor-optics-filters" class="nav-link" data-scroll-target="#sec-chpt-imaging-sensor-optics-filters"><span class="header-section-number">17.4.1</span> IR/UV Cut-Off Filters</a></li>
  <li><a href="#sec-chpt-imaging-sensor-optics-ml" id="toc-sec-chpt-imaging-sensor-optics-ml" class="nav-link" data-scroll-target="#sec-chpt-imaging-sensor-optics-ml"><span class="header-section-number">17.4.2</span> Microlenses</a></li>
  <li><a href="#sec-chpt-imaging-sensor-optics-aa" id="toc-sec-chpt-imaging-sensor-optics-aa" class="nav-link" data-scroll-target="#sec-chpt-imaging-sensor-optics-aa"><span class="header-section-number">17.4.3</span> Anti-Aliasing Filters</a></li>
  </ul></li>
  <li><a href="#sec-chpt-imaging-sensor-optics-monomodel" id="toc-sec-chpt-imaging-sensor-optics-monomodel" class="nav-link" data-scroll-target="#sec-chpt-imaging-sensor-optics-monomodel"><span class="header-section-number">17.5</span> Monochromatic, Noise-Free Sensor Model</a>
  <ul class="collapse">
  <li><a href="#sec-chpt-imaging-sensor-optics-monomodel-ssf" id="toc-sec-chpt-imaging-sensor-optics-monomodel-ssf" class="nav-link" data-scroll-target="#sec-chpt-imaging-sensor-optics-monomodel-ssf"><span class="header-section-number">17.5.1</span> Spectral Sensitivity Function</a></li>
  </ul></li>
  <li><a href="#sec-chpt-imaging-sensor-optics-pixel" id="toc-sec-chpt-imaging-sensor-optics-pixel" class="nav-link" data-scroll-target="#sec-chpt-imaging-sensor-optics-pixel"><span class="header-section-number">17.6</span> What is a Pixel?</a></li>
  <li><a href="#sec-chpt-imaging-sensor-color" id="toc-sec-chpt-imaging-sensor-color" class="nav-link" data-scroll-target="#sec-chpt-imaging-sensor-color"><span class="header-section-number">17.7</span> Color Sensing</a>
  <ul class="collapse">
  <li><a href="#sec-chpt-imaging-sensor-color-goal" id="toc-sec-chpt-imaging-sensor-color-goal" class="nav-link" data-scroll-target="#sec-chpt-imaging-sensor-color-goal"><span class="header-section-number">17.7.1</span> Goal of Color Sensing</a></li>
  <li><a href="#sec-chpt-imaging-sensor-color-impl" id="toc-sec-chpt-imaging-sensor-color-impl" class="nav-link" data-scroll-target="#sec-chpt-imaging-sensor-color-impl"><span class="header-section-number">17.7.2</span> Implementing Three “Classes of Pixels”</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/learnvisualcomputing/learnvisualcomputing.github.io/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./imaging.html">Imaging</a></li><li class="breadcrumb-item"><a href="./imaging-sensor.html"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Image Sensor Architecture</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-chpt-imaging-sensor" class="quarto-section-identifier"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Image Sensor Architecture</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>This chapter discusses image sensors, the devices that transform optical signals to electrical signals. We start from the basic principle that governs this signal transduction inside a pixel and then discuss how pixels are architected together to form an image sensor. We then turn to various in-sensor optics, which are not necessarily important for forming images but are important for forming <em>visually pleasing</em> images that, for instance, have realistic colors and are free of aliasing effects.</p>
<section id="sec-chpt-imaging-sensor-ov" class="level2" data-number="17.1">
<h2 data-number="17.1" class="anchored" data-anchor-id="sec-chpt-imaging-sensor-ov"><span class="header-section-number">17.1</span> Overview</h2>
<p>The main job of the sensor is to turn optical signals, i.e., the optical image impinging on the sensor plane, into electrical signals, i.e., digital images. This conversion is broken down into two steps, first by converting photons to charges followed by turning charges into digital numbers.</p>
<div id="fig-sensor_overview" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sensor_overview-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figs/sensor_overview_new.svg" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sensor_overview-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.1: (a): a conceptual, cross-sectional view of the sensor with the optical elements, photodiodes, and the peripheral circuitries. (b): comparison between 1) front-illuminated sensor, where lights have to first traverse through the peripheral circuitries before reaching the light-sensitive photodiodes, and 2) back-illuminated sensor, where lights can directly reach the photodiodes; from <span class="citation" data-cites="cis">Cmglee (<a href="references.html#ref-cis" role="doc-biblioref">2019</a>)</span>.
</figcaption>
</figure>
</div>
<p><a href="#fig-sensor_overview" class="quarto-xref">Figure&nbsp;<span>17.1</span></a> (a) shows a cross-sectional view of the sensor hardware, which has three main components.</p>
<ul>
<li>First, there is a set of optical elements sitting on the sensor. These optical elements are not the imaging optics we discussed in the previous chapter because their main goal is not to form an image.</li>
<li>Second, under these optical elements are the photodiodes, which turn optical signals carried in photons to electrical signals in the form of electric charges.</li>
<li>Third, interleaved with the photodiodes is the circuitry that processes the output of the photodiodes, turning charges into digital values.</li>
</ul>
<div id="fig-sensor_signal_processing" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sensor_signal_processing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figs/sensor_signal_processing.svg" class="img-fluid figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sensor_signal_processing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.2: Image sensor can be seen as a chain of processing, or a transfer function, that transfers the optical signal, a random variable, with a mean <span class="math inline">\(\mu_p\)</span> and standard deviation <span class="math inline">\(\sigma_p\)</span> to the electrical signal, another random variable, with a mean <span class="math inline">\(\mu_y\)</span> and standard deviation <span class="math inline">\(\sigma_y\)</span>. Redrawn based on European Machine Vision Association Standard 1288 <span class="citation" data-cites="emva1288">EMVA (<a href="references.html#ref-emva1288" role="doc-biblioref">2021, fig. 1</a>)</span>.
</figcaption>
</figure>
</div>
<p>From a computational perspective, we can model an image sensor as a signal processing chain, a transfer function <span class="math inline">\(f\)</span>, that transfers the optical signal to the electrical signal. <a href="#fig-sensor_signal_processing" class="quarto-xref">Figure&nbsp;<span>17.2</span></a> visualizes this chain of signal processing. This chain of processing is best understood as computing on random variables. The input optical signal can be seen a random variable <span class="math inline">\(R_o(\mu_o, \sigma_o)\)</span> with a mean and standard deviation of <span class="math inline">\(\mu_o\)</span> and <span class="math inline">\(\sigma_o\)</span>, respectively. Every step in the signal processing chain not only manipulates the signal itself but also introduces/affects the noise. As a result, the output electrical signal is another random variable <span class="math inline">\(R_e(\mu_e, \sigma_e)\)</span>. So the transfer function, viewed this way, is:</p>
<p><span class="math display">\[
    f: (\mu_o, \sigma_o) \mapsto (\mu_e, \sigma_e),
\]</span></p>
<p>Any imaging session can be seen as drawing a concrete value from the distribution of <span class="math inline">\(R_o\)</span>, and its output (raw pixel values) can be seen as drawing a value from the distribution of <span class="math inline">\(R_e\)</span>. An important goal of our study is to build an analytical model for this transfer function <span class="math inline">\(f\)</span>. For simplicity, we will first ignore noise as if <span class="math inline">\(f\)</span> operates only on the mean signal. We will then discuss the sources of noise and how to model them.</p>
<p>There are two ways the pixels and the wires that read out the pixel outputs are physically arranged, shown in <a href="#fig-sensor_overview" class="quarto-xref">Figure&nbsp;<span>17.1</span></a> (b). In the <strong>back-side illumination</strong> (BSI) arrangement, the wiring of the circuitries is behind the photodiodes, which directly interface with the lights. In the <strong>front-side illumination</strong> (FSI) arrangement, the metal wiring sits between the light and the photodiodes. This means light could be absorbed and scattered through the metal layer before reaching the photodiodes, reducing the chance of a photon being properly captured. While earlier image sensors used FSI because it is easier to manufacture, almost all commercial image sensors use BSI now <span class="citation" data-cites="swain2008back">(<a href="references.html#ref-swain2008back" role="doc-biblioref">Swain and Cheskis 2008</a>)</span>.</p>
<p>FSI is actually quite similar to the structure of human eyes, where, if you recall, the photoreceptors are “hiding” behind other retinal neurons such as the retinal ganglion cells, which are functionally the last layer of retinal processing but anatomically sitting at the first layer on the retina. Different from the FSI sensor, however, the non-photoreceptor neurons on the retina do very little to light: they do not absorb or scatter light much and can be generally thought of as transparent. Metal wires, of course, disrupt incident photons significantly.</p>
</section>
<section id="sec-chpt-imaging-sensor-pixel" class="level2" data-number="17.2">
<h2 data-number="17.2" class="anchored" data-anchor-id="sec-chpt-imaging-sensor-pixel"><span class="header-section-number">17.2</span> From Photons to Charges and Digital Numbers</h2>
<p>We will talk about how optical signals are first converted to electrical signals in the form of charges, and then talk about how the charges are detected, at which point the electrical signals are manifested as voltage potentials. The voltage potentials are then quantized as digital numbers, which are the raw pixel values. We will focus on the basic building blocks that enable these conversions and leave it to <a href="#sec-chpt-imaging-sensor-arch" class="quarto-xref"><span>Section 17.3</span></a> to discuss how these building blocks are connected in a global sensor architecture. The discussion here assumes monochromatic sensing without noise. We will talk about color sensing and the noise issue later.</p>
<section id="sec-chpt-imaging-sensor-pixel-pe" class="level3" data-number="17.2.1">
<h3 data-number="17.2.1" class="anchored" data-anchor-id="sec-chpt-imaging-sensor-pixel-pe"><span class="header-section-number">17.2.1</span> Photons to Charges</h3>
<p>What turns optical signals to electrical signals is the light-sensitive photodiode in a pixel. A photodiode is a p-n junction made of silicon, a semiconductor material. When a photon hits silicon and is absorbed, an electron from the silicon <em>might</em> be freed/emitted, transforming optical signals to electrical signals. This is called the <strong>photoelectric effect</strong> <span class="citation" data-cites="einstein1905erzeugung einstein1905heuristic">(<a href="references.html#ref-einstein1905erzeugung" role="doc-biblioref">Einstein 1905b</a>, <a href="references.html#ref-einstein1905heuristic" role="doc-biblioref">1905a</a>)</span>, the discovery of which won Albert Einstein his Nobel Prize.</p>
<p>In particular, when a photon is absorbed, if its energy is greater than or equal to the <strong>work function</strong> <span class="math inline">\(\phi\)</span> of the material, which is the minimum energy needed to free an electron from the surface of the material, the photon can transfer its energy to an electron and free the electron. A photon’s energy is given by the <strong>Planck’s relation</strong>:</p>
<p><span id="eq-photon_energy"><span class="math display">\[
    \mathcal{E} = h f = \frac{hc}{\lambda},
\tag{17.1}\]</span></span></p>
<p>where <span class="math inline">\(h\)</span> is the Planck constant, <span class="math inline">\(f\)</span> is the photon frequency, and <span class="math inline">\(c\)</span> is the speed of light. So if <span class="math inline">\(h f &gt; \phi\)</span>, an absorbed photon can free an electron. Interestingly, the residual energy <span class="math inline">\(hf - \phi\)</span> becomes the kinetic energy of the electron, so a photon with a shorter wavelength (i.e., higher frequency) would allow the emitted electron to move faster.</p>
<p>It is clear that there is a frequency threshold <span class="math inline">\(\phi/h\)</span>, lower than which a photon would never be able to free an electron. Higher than the threshold, there is generally a one-to-one mapping between an absorbed photon and an emitted electron: an absorbed photon always frees an electron. Since the work function of silicon is about 1.1 eV (electron volt), absorption of photons with wavelengths longer than 1,100 nm would not emit any electron.</p>
<section id="sec-chpt-imaging-sensor-pixel-pe-qe" class="level4">
<h4 class="anchored" data-anchor-id="sec-chpt-imaging-sensor-pixel-pe-qe">Quantum Efficiency</h4>
<p>A key figure of merit in image sensing is the notion of <strong>quantum efficiency</strong> (QE), which is the ratio between the number of electrons collected and the number of incident photons:</p>
<p><span id="eq-qe"><span class="math display">\[
    QE = \frac{\#\text{~of electrons collected}}{\#\text{~of incident photons}}.
\tag{17.2}\]</span></span></p>
<p><a href="#fig-silicon_abs_qe" class="quarto-xref">Figure&nbsp;<span>17.3</span></a> (a) shows the QE spectrum of an image sensor in the Hubble Space Telescope. It might come as a surprise that QE is lower than 1 (even for wavelengths well within the 1,000 nm threshold) and is actually wavelength dependent: shouldn’t every absorbed photon (within the wavelength threshold) always free an electron? There are two reasons.</p>
<div id="fig-silicon_abs_qe" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-silicon_abs_qe-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figs/silicon_abs_qe.svg" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-silicon_abs_qe-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.3: (a): quantum efficiency of a sensor on the Hubble Space Telescope; from <span class="citation" data-cites="ccd_qe">Eric Bajart (<a href="references.html#ref-ccd_qe" role="doc-biblioref">2010</a>)</span> with data from <span class="citation" data-cites="mcmaster2008wide">Biretta and McMaster (<a href="references.html#ref-mcmaster2008wide" role="doc-biblioref">2008, fig. 4.2</a>)</span>. (b): silicon absorption coefficient (left axis) and mean free path (right axis) as a function of wavelength; data from <span class="citation" data-cites="green1995optical">Green and Keevers (<a href="references.html#ref-green1995optical" role="doc-biblioref">1995</a>)</span>.
</figcaption>
</figure>
</div>
<p>First, the denominator in the QE definition is the number of <em>incident</em> photons, not the number of absorbed photons. Not all photons that hit the photodetector will be absorbed. <a href="#fig-silicon_abs_qe" class="quarto-xref">Figure&nbsp;<span>17.3</span></a> (b) shows the spectral absorption coefficient <span class="math inline">\(\sigma\)</span> (unit 1/cm) of silicon on the left <span class="math inline">\(y\)</span>-axis, and the right <span class="math inline">\(y\)</span>-axis shows the corresponding mean free path <span class="math inline">\(l\)</span> (i.e., the expected length a photon can travel within silicon before being absorbed) at different wavelengths; recall from <a href="rendering-sss.html#eq-mfp" class="quarto-xref">Equation&nbsp;<span>13.7</span></a> that <span class="math inline">\(l = 1/\sigma\)</span>. We can see that absorption is strongest for the blue-ish lights but decays very rapidly toward the longer wavelengths. This definition of QE is different from how QE is defined in human vision. Recall from <a href="hvs-receptor.html#sec-chpt-hvs-receptor-counting" class="quarto-xref"><span>Section 3.1</span></a>; there, QE is the probability of pigment excitation once the pigment actually absorbs a photon; there, the QE of photopigment is roughly two-thirds and is not wavelength-sensitive.</p>
<p>Second, the nominator in the QE definition is the number of <em>collected</em>, not emitted, electrons: even if an electron is freed by an absorbed photon, that electron might not actually be collected and contribute to the electrical signal. Depending on where the electrons are freed, some of them need to go through a random walk (think of it as Brownian motion) before being collected, and you can imagine some electrons can be recombined with the holes during the walk. <!-- %https://hamamatsu.magnet.fsu.edu/articles/quantumefficiency.html
%https://isl.stanford.edu/~abbas/ee392b/lect01.pdf; the QE derived is the number of collected electrons over the number of incident photons on the photodetector (so it ignores any photon loss before reaching the PD but includes absorption spectrum and random walks) --></p>
<p>Given QE, the total number of emitted electrons after an exposure time <span class="math inline">\(t_{exp}\)</span> is given by:</p>
<p><span id="eq-int_n_a"><span class="math display">\[
    N = \int_{\lambda} QE(\lambda) Y(\lambda) \text{d}\lambda,
\tag{17.3}\]</span></span></p>
<p>where <span class="math inline">\(Y(\lambda)\)</span> is the number of photons incident on a photodiode at a particular wavelength <span class="math inline">\(\lambda\)</span> at time <span class="math inline">\(t\)</span> during the exposure time <span class="math inline">\(t_{exp}\)</span>, assuming that <span class="math inline">\(Y\)</span> is invariant during <span class="math inline">\(t_{exp}\)</span> here.</p>
<p>According to the Planck’s relation (<a href="#eq-photon_energy" class="quarto-xref">Equation&nbsp;<span>17.1</span></a>), <span class="math inline">\(Y(\lambda)\)</span> is related to the spectral power distribution (SPD) of the incident light <span class="math inline">\(\Phi(\lambda)\)</span> by: <span class="math inline">\(Y(\lambda) = \frac{\Phi(\lambda, t) t_{exp} \lambda}{hc}\)</span>, where <span class="math inline">\(\Phi(\lambda) t_{exp}\)</span> is spectral energy distribution. Therefore, we have:</p>
<p><span id="eq-int_n_b"><span class="math display">\[
    N = \int_\lambda QE(\lambda) \frac{\Phi(\lambda) t_{exp} \lambda}{hc}  \text{d}\lambda.
\tag{17.4}\]</span></span></p>
<p>Note that we define QE for the photodiode itself: the denominator in <a href="#eq-qe" class="quarto-xref">Equation&nbsp;<span>17.2</span></a> refers to the number of photons incident on the photodiode, not those that enter the camera system. This is an important distinction, because many photons that enter the camera would not even make their way to the photodiode; some of them are reflected at the lens surfaces, and others are absorbed by the various filters (<a href="#sec-chpt-imaging-sensor-optics" class="quarto-xref"><span>Section 17.4</span></a>). In many contexts, the QE is reported with respect to the entire camera system, where the denominator <em>is</em> the number of photons entering the camera, in which case the QE would be lower than that of the photodiode. Always ask what the precise definition of a QE is when reading the literature.</p>
</section>
</section>
<section id="sec-chpt-imaging-sensor-pixel-charge" class="level3" data-number="17.2.2">
<h3 data-number="17.2.2" class="anchored" data-anchor-id="sec-chpt-imaging-sensor-pixel-charge"><span class="header-section-number">17.2.2</span> Measuring Charges</h3>
<section id="basic-principle" class="level4">
<h4 class="anchored" data-anchor-id="basic-principle">Basic Principle</h4>
<p>Now that we have turned photons to charges — the freed electrons move to the n region and the holes move to the p region of the p-n junction — the next step is to measure the charges. The basic principle of doing so is using a capacitor: we use the electrons to discharge a capacitor with a known capacitance; by measuring the voltage difference before and after the discharge, we can then estimate the number of emitted electrons:</p>
<p><span id="eq-charge_measure"><span class="math display">\[
    \Delta V = \frac{\mathcal{Q}_{sig}}{C_{FD}} \times g = \frac{N q}{C_{FD}} \times g,
\tag{17.5}\]</span></span></p>
<p>where <span class="math inline">\(\mathcal{Q}_{sig}\)</span> is the charge in the signal used to discharge the capacitor, which is usually a floating diffusion (see later) that has a capacitance of <span class="math inline">\(C_{FD}\)</span>, and <span class="math inline">\(g\)</span> is the voltage gain of whatever device is used to read out the voltage, usually a source follower (see later). <span class="math inline">\(\mathcal{Q}_{sig}\)</span> itself is the product of <span class="math inline">\(N\)</span>, the number of charges in the signal, and <span class="math inline">\(q\)</span>, the elementary charge.</p>
<p><span class="math inline">\(\frac{q}{C_{FD}}\)</span> is also called the <strong>conversion gain</strong> (CG) of the pixel. CG has a unit of <span class="math inline">\(\text{Volt}/\text{e}^-\)</span> and can be interpreted as the amount of voltage change per charge. CG is a very important quantity. A high CG means the output voltage change is very sensitive to small amount of input light change, which is good for improving the signal-to-noise ratio (SNR). In contrast, a small CG means the output voltage change is small given the same amount of light change, and that small voltage change becomes very difficult to detect in the presence of noises, resulting in a low SNR. While desirable from a noise perspective, a high CG necessarily means a smaller capacitor, which is easier to fill up (saturate). We will get back to this point when discussing dynamic range (<a href="#sec-chpt-imaging-sensor-pixel-dr" class="quarto-xref"><span>Section 17.2.4</span></a>).</p>
<p>We can see that once we can measure <span class="math inline">\(\Delta V\)</span>, we can get an estimate of <span class="math inline">\(N\)</span>. Why do we care about <span class="math inline">\(N\)</span>? Intuitively, the incident light luminance is positively related to <span class="math inline">\(N\)</span>: more incident photons means higher luminance. Luminance <span class="math inline">\(L\)</span>, if we are interested in only grayscale, monochromatic imaging, is ultimately what we want to estimate.</p>
<p>It is important to realize that the actual relationship between <span class="math inline">\(L\)</span> and <span class="math inline">\(N\)</span> is not linear. We know that luminance is defined as:</p>
<p><span id="eq-lum"><span class="math display">\[
    L = \int_{\lambda} V(\lambda) \Phi(\lambda) \text{d}\lambda,
\tag{17.6}\]</span></span></p>
<p>where <span class="math inline">\(V(\lambda)\)</span> is the luminance efficiency function (LEF) and <span class="math inline">\(\Phi(\lambda)\)</span> is the SPD of the incident light. Taking <a href="#eq-lum" class="quarto-xref">Equation&nbsp;<span>17.6</span></a> and <a href="#eq-int_n_b" class="quarto-xref">Equation&nbsp;<span>17.4</span></a> together, we can see that given <span class="math inline">\(N\)</span>, we cannot quite estimate <span class="math inline">\(L\)</span>, because <span class="math inline">\(L\)</span> depends on <span class="math inline">\(\Phi(\lambda)\)</span>, but estimating <span class="math inline">\(\Phi(\lambda)\)</span> from <span class="math inline">\(N\)</span> is an under-determined problem, as <a href="#eq-int_n_b" class="quarto-xref">Equation&nbsp;<span>17.4</span></a> shows. To be exact, <span class="math inline">\(L\)</span> does not necessarily scale linearly with <span class="math inline">\(N\)</span> — it does not even necessarily scale positively with <span class="math inline">\(N\)</span>, but it is perhaps not terribly wrong to informally say a higher charge count means a higher luminance in the scene.</p>
</section>
<section id="t-design" class="level4">
<h4 class="anchored" data-anchor-id="t-design">4T Design</h4>
<p>The photodiode (PD) technically acts as a capacitor itself (the n-side neutral region holds electrons and the p-side neutral region holds holes), so we could simply use the PD for that purpose. This is indeed how an earlier pixel design works, which we will return to shortly. Modern pixels actually transfer the charges from the PD to a separate measurement node, which we focus on here.</p>
<p><a href="#fig-4taps" class="quarto-xref">Figure&nbsp;<span>17.4</span></a> (a) shows the circuit diagram of a typical pixel design that detects and measures the charges. The design has a PD and four transistors, so it is usually called the 4T design. The <code>M-TX</code> switch controls the transfer of the charges accumulated in the PD to the <strong>Floating Diffusion</strong> (FD)<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>, another capacitive area, and is sometimes called the <strong>measurement node</strong>, the <em>sense node</em>, or the <em>conversion node</em>, because that is where the charges are actually being measured. The FD is connected to the NMOS <strong>Source Follower</strong> (SF) transistor <code>M-SF</code>, where the gate terminal is its input and is connected to the FD voltage, the drain is connected to the supply voltage, and the source is the output that faithfully follows/transfers the input with a gain of about 0.9 (<span class="math inline">\(g\)</span> in <a href="#eq-charge_measure" class="quarto-xref">Equation&nbsp;<span>17.5</span></a>).</p>
<div id="fig-4taps" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-4taps-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figs/4taps.svg" class="img-fluid figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-4taps-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.4: (a): circuit diagram of a typical 4T pixel design; adapted from <span class="citation" data-cites="ma2024efficient">Ma (<a href="references.html#ref-ma2024efficient" role="doc-biblioref">2024, fig. 2.5(a)</a>)</span>. (b): timing diagram of operating a 4T pixel.
</figcaption>
</figure>
</div>
<p>The sequence of operation goes roughly like the following, and <a href="#fig-4taps" class="quarto-xref">Figure&nbsp;<span>17.4</span></a> (b) shows the corresponding timing diagram:</p>
<ul>
<li>Before the exposure, we turn on the <code>M-RST</code> switch <em>and</em> the <code>M-TX</code> to drain the charges (electrons) at the PD, which will also, as a byproduct, drain the charges in the FD, resetting their voltage potentials both to <span class="math inline">\(V_{RST}\)</span>. Resetting the FD voltage at this step is of no functional use, as we will shortly see.</li>
<li>We then turn off <code>M-RST</code> and <code>M-TX</code>, and the exposure begins, during which the charges are collected inside the PD. We can see from <a href="#eq-charge_measure" class="quarto-xref">Equation&nbsp;<span>17.5</span></a> that in order to measure the charges we need to measure the voltage difference <em>at the FD node</em> before and after the charges are transferred. So toward the end of the exposure, we turn on the <code>M-RST</code> switch again while, importantly, keeping the <code>M-TX</code> switch off. This would allow us to reset the FD voltage to <span class="math inline">\(V_{rst}\)</span>, which will be measured through <code>M-SF</code> as <span class="math inline">\(V_1\)</span> in <a href="#fig-4taps" class="quarto-xref">Figure&nbsp;<span>17.4</span></a> (b)<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>.</li>
<li>We then turn on the <code>M-TX</code> switch, which transfers the charges from the PD to the FD. After that, we turn off <code>M-TX</code> and read the voltage from <code>M-SF</code> for the second time, this time for the voltage at FD after the charge transfer. This is the <span class="math inline">\(V_2\)</span> in <a href="#fig-4taps" class="quarto-xref">Figure&nbsp;<span>17.4</span></a> (b). The difference between <span class="math inline">\(V_1\)</span> and <span class="math inline">\(V_2\)</span> is the <span class="math inline">\(\Delta V\)</span> in <a href="#eq-charge_measure" class="quarto-xref">Equation&nbsp;<span>17.5</span></a>.</li>
</ul>
<p>As we can see, we read the voltage of the FD twice to obtain the voltage difference caused by the charges collected during the exposure. This is called <strong>Correlated Double Sampling</strong> (CDS), which turns out to also be very important to mitigate many noise sources, which we will discuss later.</p>
<p>To read out the voltage from the SF, the <code>M-SEL</code> switch needs to be turned on, which is omitted from <a href="#fig-4taps" class="quarto-xref">Figure&nbsp;<span>17.4</span></a> (b) for simplicity. As we will shortly see in <a href="#sec-chpt-imaging-sensor-arch" class="quarto-xref"><span>Section 17.3</span></a>, in most cases (although not all), pixels are read out row by row, so the <code>M-SEL</code> switches of all pixels in the same row are connected to the same signal, usually called the row select signal.</p>
<p>The timing diagram in <a href="#fig-4taps" class="quarto-xref">Figure&nbsp;<span>17.4</span></a> (b) is illustrative of the major operations (omitting <code>M-SF</code>) but not drawn to scale. The exposure time is usually at the tens of milliseconds scale (e.g., 30 FPS means roughly a 33.3 ms exposure time), but the timescale to operate the transistors/switches is at the microsecond level. Also observe, in <a href="#fig-4taps" class="quarto-xref">Figure&nbsp;<span>17.4</span></a> (b), that during the exposure the voltage at the FD (<span class="math inline">\(V_{FD}\)</span>) slowly reduces from <span class="math inline">\(V_{rst}\)</span> after the first reset — because of the charge leakage in the FD, just like how DRAM cells leak. This is why we need the second reset to bring the voltage at FD back to <span class="math inline">\(V_{rst}\)</span> before charge transfer. This is also why we say the first reset is of no functional use to the FD (but of course very important to the PD because we want the PD to collect only electrons emitted from the current exposure).</p>
</section>
<section id="t-aps-vs.-3t-aps-vs.-pps" class="level4">
<h4 class="anchored" data-anchor-id="t-aps-vs.-3t-aps-vs.-pps">4T APS vs.&nbsp;3T APS vs.&nbsp;PPS</h4>
<p>The (4T) pixel design described above is called an <strong>Active Pixel Sensor</strong> (APS) design, first conceived by <span class="citation" data-cites="noble1968self">Noble (<a href="references.html#ref-noble1968self" role="doc-biblioref">1968</a>)</span> (see <span class="citation" data-cites="fossum1993active">Fossum (<a href="references.html#ref-fossum1993active" role="doc-biblioref">1993</a>)</span> for a more modern perspective). An APS has a per-pixel SF (a common-drain amplifier) that “actively” reads out the signal for each pixel by turning its charges to voltage. We briefly discuss the other, older pixel designs that are less commonly used now. See <span class="citation" data-cites="el2005cmos">El Gamal and Eltoukhy (<a href="references.html#ref-el2005cmos" role="doc-biblioref">2005</a>)</span> for a more detailed discussion and visual comparisons.</p>
<div id="fig-pixel_design" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pixel_design-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figs/pixel_design.svg" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pixel_design-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.5: Left: 3T APS vs.&nbsp;4T APS. Top right: Passive Pixel Sensor (PPS). Bottom right: Digital Pixel Sensor (DPS). Adapted from <span class="citation" data-cites="el2005cmos">El Gamal and Eltoukhy (<a href="references.html#ref-el2005cmos" role="doc-biblioref">2005, figs. 5, 10, 11</a>)</span>.
</figcaption>
</figure>
</div>
<p>A simpler and earlier version of the APS design uses only three transistors (3T) without the gate. <a href="#fig-pixel_design" class="quarto-xref">Figure&nbsp;<span>17.5</span></a> (left) compares the 4T APS with the 3T APS. Without the transfer gate, the PD is used as the measurement/sensor node itself, so the <span class="math inline">\(C_{FD}\)</span> in <a href="#eq-charge_measure" class="quarto-xref">Equation&nbsp;<span>17.5</span></a> is effectively the capacitance of the PD itself. The 3T APS simplifies the pixel design and, thus, increases the fill factor (without the microlenses). It, however, generally suffers from a lower signal-to-noise ratio (SNR) for a variety of reasons. For instance, the PD has a large inherent photodiode capacitance, so the signal (<span class="math inline">\(\Delta V\)</span> in <a href="#eq-charge_measure" class="quarto-xref">Equation&nbsp;<span>17.5</span></a>) read from the PD is low, making it more vulnerable to noise. In contrast, we get to control the FD in the 4T APS, which can be made to have a much lower capacitance, leading to a higher SNR. The CDS for 3T APS is also much less effective in suppressing noise, as we will discuss later.</p>
<p>A precursor to APS was the <strong>Passive Pixel Sensor</strong> (PPS), first suggested in <span class="citation" data-cites="weckler1967operation">Weckler (<a href="references.html#ref-weckler1967operation" role="doc-biblioref">1967</a>)</span> and <span class="citation" data-cites="dyck1968integrated">Dyck and Weckler (<a href="references.html#ref-dyck1968integrated" role="doc-biblioref">1968</a>)</span>. A PPS has only one transistor, as shown in the top-right panel in <a href="#fig-pixel_design" class="quarto-xref">Figure&nbsp;<span>17.5</span></a>. The PPS has no SF that reads out voltage from the PD charges. Instead, the charges (not voltage) in the PD “passively” flow through a column bus and are turned to voltage there through a charge amplifier <span class="citation" data-cites="aoki19822">(<a href="references.html#ref-aoki19822" role="doc-biblioref">Aoki et al. 1982</a>)</span>. <!-- % what's drawn here is a particular charge amplifier (CTIA), which is different from the subsequent column follower amplifier and chip follower amplifier (omitted): https://isl.stanford.edu/~abbas/ee392b/lect04.pdf, p. 5-6. --> The PPS design is simpler (as only one transistor is needed) but leads to a much worse noise profile because of the large (parasitic) capacitance of the column bus. The SF in APS acts as an active amplifier, which isolates the sense node (whether it is the PD or the FD) from the large column bus capacitance, providing a much higher output current and lower output impedance than a PD does and, thus, improving the SNR <span class="citation" data-cites="kozlowski1998comparison ohta2020smart">(<a href="references.html#ref-kozlowski1998comparison" role="doc-biblioref">Kozlowski et al. 1998</a>; <a href="references.html#ref-ohta2020smart" role="doc-biblioref">Ohta 2020</a>, Chpt. 2.5)</span>.</p>
</section>
<section id="electronic-shutter" class="level4">
<h4 class="anchored" data-anchor-id="electronic-shutter">Electronic Shutter</h4>
<p>Ideally, when we are not capturing light, the photodiodes should not be exposed to lights. This is achieved by a shutter. <strong>Mechanical shutters</strong> do so by physically blocking lights. The sensor is <em>not</em> exposed to light normally, blocked by the shutter. The shutter then mechanically opens to expose the sensor to light. There are many types of mechanical shutters, of which the most popular one is the focal plane shutter shown in <a href="#fig-rolling_shutter" class="quarto-xref">Figure&nbsp;<span>17.6</span></a> (a). The shutter has two curtains that move in sync with a gap that allows lights in. The size of the shutter opening and the speed of the movement dictate the exposure time: a larger opening and slower speed mean longer exposure time. This is called a focal plane shutter because the shutter is located in front of the focal plane (sensor). There is also the leaf shutter, which is usually located at the aperture plane with the lenses.</p>
<div id="fig-rolling_shutter" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rolling_shutter-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figs/rolling_shutter.svg" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rolling_shutter-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.6: (a): a mechanical focal-plane shutter, which is inherently a rolling shutter; adapted from <span class="citation" data-cites="focal_plane_shutter">Ommnomnomgulp (<a href="references.html#ref-focal_plane_shutter" role="doc-biblioref">2008</a>)</span>. (b): rolling shutter artifact; from <span class="citation" data-cites="rs_artifacts">BrayLockBoy (<a href="references.html#ref-rs_artifacts" role="doc-biblioref">2018</a>)</span>.
</figcaption>
</figure>
</div>
<p>The 4T pixel design above essentially implements an <strong>electronic shutter</strong> (ES). With an ES, we expose photodiodes to lights <em>all the time</em>. The way we mark the start of the exposure is through the <code>M-RST</code> switch, which resets the PDs, and the way we mark the end of the exposure is through the <code>M-TX</code> switch, which transfers the PD charges for measurement. The time difference between these two steps dictates the exposure time. As you can imagine, the shutter speed (inverse of the exposure time) of an electronic shutter can be much faster than that of a mechanical shutter, since there are no mechanical moving parts.</p>
</section>
</section>
<section id="sec-chpt-imaging-sensor-pixel-ro" class="level3" data-number="17.2.3">
<h3 data-number="17.2.3" class="anchored" data-anchor-id="sec-chpt-imaging-sensor-pixel-ro"><span class="header-section-number">17.2.3</span> Read-out Circuitry</h3>
<p>Following the pixel circuitry is the read-out circuitry, which usually has two main components: the programmable-gain amplifier and the analog-to-digital Converter (ADC). <a href="#fig-readout" class="quarto-xref">Figure&nbsp;<span>17.7</span></a> illustrates the common, simplified designs of the two components.</p>
<div id="fig-readout" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-readout-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figs/readout.svg" class="img-fluid figure-img" style="width:90.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-readout-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.7: (a): analog CDS and programmable amplifier; from <span class="citation" data-cites="ma2024efficient">Ma (<a href="references.html#ref-ma2024efficient" role="doc-biblioref">2024, fig. 2.5(b)</a>)</span>. (b): a single-slope ADC typically used in image sensors; adapted from <span class="citation" data-cites="ma2024efficient">Ma (<a href="references.html#ref-ma2024efficient" role="doc-biblioref">2024, fig. 2.5(c)</a>)</span>.
</figcaption>
</figure>
</div>
<p>The amplifier is there to amplify the voltage read from the pixel, and the <strong>gain</strong> of the amplifier is programmable. A programmable gain is useful in imaging and photography to artificially shorten or extend the exposure time (e.g., through the ISO setting in a digital camera). The particular design shown in <a href="#fig-readout" class="quarto-xref">Figure&nbsp;<span>17.7</span></a> (a) combines CDS with a classical amplifier design with two capacitors. Specifically, the two voltages read out from the FD (one right after the reset and the other right after the charge transfer) are sampled by the <span class="math inline">\(C_{in}\)</span> capacitor sequentially, which essentially performs an analog-domain subtraction that is required by CDS. The voltage difference is then amplified with a gain <span class="math inline">\(\frac{C_{in}}{C_{feedback}}\)</span>. <span class="math inline">\(C_{feedback}\)</span> is usually programmable, allowing us to control the gain.</p>
<p>The amplified voltage difference then goes through an ADC to obtain the digital value. There is a huge amount of ADC designs <span class="citation" data-cites="adc_survey">(<a href="references.html#ref-adc_survey" role="doc-biblioref">Murmann 2014</a>)</span>. The design that is commonly used in image sensors is the single-slope (SS) design, whose simplified diagram is shown in <a href="#fig-readout" class="quarto-xref">Figure&nbsp;<span>17.7</span></a> (b). An SS ADC consists of a comparator, a ramp signal generator, and a counter. The ramp generator provides a monotonically increasing or decreasing ramp signal, which is compared with the to-be-quantized analog signal (output of the amplifier). At every clock cycle, the comparator compares the two inputs while the counter increments. When the two input signals cross, the counter value is recorded and represents the quantized digital value of the analog signal.</p>
<p>The designs in <a href="#fig-readout" class="quarto-xref">Figure&nbsp;<span>17.7</span></a> perform CDS in the analog domain (through <span class="math inline">\(C_{in}\)</span>). In many image sensors today, the CDS is performed in the digital domain after the ADC <span class="citation" data-cites="nitta2006high">(<a href="references.html#ref-nitta2006high" role="doc-biblioref">Nitta et al. 2006</a>)</span>. You would think that such a design might require twice the ADC overhead plus the additional digital subtraction overhead. In reality, the design is quite clever. The ADC would first quantize the first sample (before reset), and the resulting counter value represents the digital value of the first sample. For the second sample, instead of counting from scratch, we would simply turn the counter around so that it counts backward. At the end, the counter value is naturally the digital difference of the two samples.</p>
</section>
<section id="sec-chpt-imaging-sensor-pixel-dr" class="level3" data-number="17.2.4">
<h3 data-number="17.2.4" class="anchored" data-anchor-id="sec-chpt-imaging-sensor-pixel-dr"><span class="header-section-number">17.2.4</span> Dynamic Range</h3>
<p>We can intuitively think of each pixel as a well (a pixel well) that collects electrons. <a href="#eq-int_n_b" class="quarto-xref">Equation&nbsp;<span>17.4</span></a> indicates that there are two main factors that determine the number of electrons going into a particular pixel well: the incident light power and the exposure time. A pixel cannot indefinitely collect electrons. The <strong>full-well capacity</strong> (FWC) is the max amount of electrons that can be held by a pixel’s photodiode. More electrons than the FWC would <em>saturate</em> the well, at which point no charges will be stored by the pixel. When a pixel well is saturated, photographers call that pixel “over-exposed”. This is illustrated in <a href="#fig-dr" class="quarto-xref">Figure&nbsp;<span>17.8</span></a>, where, ordinarily, the number of charges collected is proportional to the incident light luminance until the pixel well is full.</p>
<div id="fig-dr" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-dr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figs/dr.svg" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-dr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.8: Illustration of dynamic range, which is the ratio of the FWC and the noise floor; adapted from <span class="citation" data-cites="overexposure2">Axel Jacobs (<a href="references.html#ref-overexposure2" role="doc-biblioref">2006</a>)</span>. Incident luminance higher than the FWC saturates a pixel, leading to over-exposure.
</figcaption>
</figure>
</div>
<p>A larger FWC leads to a higher sensor <strong>dynamic range</strong>, which, informally, refers to the range of scene luminance that a sensor can capture. Formally, the dynamic range is defined as the ratio between the highest and the lowest luminance level that can be <em>faithfully</em> captured. The highest level is the FWC, but what about the lowest level? Wouldn’t that simply be 0 and, if so, wouldn’t the dynamic range of any image sensor be infinity?</p>
<p>The answer is that at very low light levels the charges collected by a pixel are dominated by noise. We call the charges collected when there is no incident light the “noise floor”, which can be measured by taking an image when the camera is in dark. The dynamic range is thus the ratio between the FWC and the noise floor <span class="citation" data-cites="nakamura2006image">(<a href="references.html#ref-nakamura2006image" role="doc-biblioref">Nakamura 2006</a>, Chpt. 3.4.2.1)</span>:</p>
<p><span class="math display">\[
    \text{DR} = \frac{\text{FWC}}{\text{Noise Floor}}
\]</span></p>
<p>We discuss noise in detail in <a href="imaging-noise.html" class="quarto-xref"><span>Chapter 18</span></a> and will not get into it too much here, but briefly, the noise floor is dominated by “dark noise”, which is caused by the thermally dislodged electrons, and the “read noise”, which is the noise introduced by the read-out circuitry.</p>
<p>Not only can saturation occur at a PD’s well, it can also occur when transferring the charges from the PD to the FD during read-out. As we have briefly alluded to when discussing the conversion gain (CG) in <a href="#sec-chpt-imaging-sensor-pixel-charge" class="quarto-xref"><span>Section 17.2.2</span></a>, when the CG is low, the SNR is high but we need to use a small FD, whose capacity could sometimes be smaller than that of the PD, in which case the charge transfer might saturate the FD. Alternatively, a large FD will not saturate (during charge transfer) but will lead to a low CG and, thus, lower SNR.</p>
<p>A technique that many image sensors use is called dual conversion gain (<strong>DCG</strong>), where a pixel’s charges can be read-out twice, once with a high conversion gain (HCG) and the second time with a low conversion gain (LCG) <span class="citation" data-cites="solhusvik20191280 willassen20151280 huggett2009dual miyauchi2020stacked takayanagi2018over">(<a href="references.html#ref-solhusvik20191280" role="doc-biblioref">Solhusvik et al. 2019</a>; <a href="references.html#ref-willassen20151280" role="doc-biblioref">Willassen et al. 2015</a>; <a href="references.html#ref-huggett2009dual" role="doc-biblioref">Huggett et al. 2009</a>; <a href="references.html#ref-miyauchi2020stacked" role="doc-biblioref">Miyauchi et al. 2020</a>; <a href="references.html#ref-takayanagi2018over" role="doc-biblioref">Takayanagi et al. 2018</a>)</span>. To support the LCG read-out, we need an (or sometimes multiple) extra capacitive node, e.g., an additional FD (let’s call that <span class="math inline">\(FD_2\)</span>), that is connected in parallel with the original FD (let’s call that <span class="math inline">\(FD_1\)</span>) so as to increase the effective <span class="math inline">\(C_{FD}\)</span> in <a href="#eq-charge_measure" class="quarto-xref">Equation&nbsp;<span>17.5</span></a>.</p>
<ul>
<li>In the first HCG read-out, we use only <span class="math inline">\(FD_1\)</span> but not <span class="math inline">\(FD_2\)</span>. This reading has a high HCG and high SNR, which is especially important for dark parts of the scene. For the bright areas, however, <span class="math inline">\(FD_1\)</span> saturates and the readings are useless. Importantly, however, the left-over charges are not discarded; they still stay in the PD.</li>
<li>Then in the subsequent LCG read-out, the extra <span class="math inline">\(FD_2\)</span> is switched in. Now all the charges, including the left-over ones in the PD and the charges in <span class="math inline">\(FD_1\)</span>, are then re-distributed to <span class="math inline">\(FD_1\)</span> <em>and</em> <span class="math inline">\(FD_2\)</span>, which collectively will not saturate, so highlights are captured at the cost of low SNR.</li>
</ul>
<!-- Fig. 7 in @iida20180 is a great example. -->
<section id="sec-chpt-imaging-sensor-pixel-dr-hdr" class="level4">
<h4 class="anchored" data-anchor-id="sec-chpt-imaging-sensor-pixel-dr-hdr">High Dynamic Range Imaging</h4>
<p>The goal of high-dynamic-range (<strong>HDR</strong>) imaging is to design imaging systems such that the scene luminance can be faithfully reconstructed from pixel values. Two things are in the way: noise at low-luminance regions in the scene and saturation at high-luminance regions in the scene. A common strategy for HDR is called <strong>exposure bracketing</strong>, which can be implemented in two ways, both involving taking multiple shots of the scene and then fuse them.</p>
<ul>
<li>Each shot has the same, short exposure time so no pixel is over-exposed, but pixels for low-luminance regions are noisy. We then average multiple shots; averaging is a form denoising (<a href="imaging-noise.html" class="quarto-xref"><span>Chapter 18</span></a>). This is the approach that Google’s HDR+ system takes <span class="citation" data-cites="hasinoff2016burst">(<a href="references.html#ref-hasinoff2016burst" role="doc-biblioref">Hasinoff et al. 2016</a>)</span>.</li>
<li>Each shot has a different exposure time. Long-exposure shots are used to capture details in low-luminance regions, and short-exposure shots capture details in high-luminance regions.</li>
</ul>
<p>Either way, the issue with exposure bracketing is the longer capturing time, which makes the resulting image more susceptible to motion blur. We ideally would like “single-shot” HDR. There are multiple methods, and they usually require co-designing the image sensor/pixel with the post-processing algorithms (aside from modern deep learning approaches that rely semantics information, which we will not discuss).</p>
<p>One strategy is to use <strong>split pixels</strong> or dual PDs, an emerging technology that sensor companies are exploring. The idea is to use split a pixel into two PDs, each with a different “sensitivity” to light <span class="citation" data-cites="iida20180 solhusvik20191280 willassen20151280 xu2022analysis">(<a href="references.html#ref-iida20180" role="doc-biblioref">Iida et al. 2018</a>; <a href="references.html#ref-solhusvik20191280" role="doc-biblioref">Solhusvik et al. 2019</a>; <a href="references.html#ref-willassen20151280" role="doc-biblioref">Willassen et al. 2015</a>; <a href="references.html#ref-xu2022analysis" role="doc-biblioref">J. Xu et al. 2022</a>)</span>. The sensitivity is usually controlled by PD size (and the corresponding microlens size): the larger PD (LPD) can collect more charges at the same light intensity (quantified by photons/area) than the small PD (SPD)—simply because of the large photon collection area—and, thus, saturate faster. The two groups of PDs are interleaved on the sensor plane, so they each perform a uniform sampling of the scene (preceded by a spatial integration over the pixel area of course).</p>
<div id="fig-sp_dcg_dr" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sp_dcg_dr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figs/sp_dcg_dr.svg" class="img-fluid figure-img" style="width:90.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sp_dcg_dr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.9: Illustration of how the split-pixel architecture, where a pixel has a large PD (LPD) and a small PD (SPD) and LOFIC extend the dynamic range (DR). Dual conversion gain (DCG), in this example, is applied to the LPD. Drawn based on <span class="citation" data-cites="xu2022analysis">J. Xu et al. (<a href="references.html#ref-xu2022analysis" role="doc-biblioref">2022</a>)</span>.
</figcaption>
</figure>
</div>
<p>The way that split pixels extend dynamic range is illustrated in <a href="#fig-sp_dcg_dr" class="quarto-xref">Figure&nbsp;<span>17.9</span></a>. The LPD, with a FWC of <span class="math inline">\(S_3\)</span>, saturates at a low luminance level <span class="math inline">\(L_1\)</span>, so only those (large) pixels that image low-luminance regions in the scene do not saturate; as a result, LPDs provide a good sampling of the low-luminance information. In contrast, the SPD, with a lower intrinsic FWC of <span class="math inline">\(S_2\)</span>, saturate at a high luminance level <span class="math inline">\(L_2\)</span>, so SPDs provide a good sampling for high-luminance information in the scene. Note that even though the SPD has a smaller intrinsic FWC than that of the LPD, the SPD’s sensitivity to light is even lower<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>, so the SPD still saturates at a higher intensity level.</p>
<p>If we increase the FWC of the small pixels, they take even longer/higher luminance to saturate. The way we increase the FWC is by adding a lateral overflow integration capacitor (<strong>LOFIC</strong>), which holds the overflow charges from the PD during exposure <span class="citation" data-cites="sugawa2005100 akahane2006sensitivity takayanagi2019120 ikeno20224">(<a href="references.html#ref-sugawa2005100" role="doc-biblioref">Sugawa et al. 2005</a>; <a href="references.html#ref-akahane2006sensitivity" role="doc-biblioref">Akahane et al. 2006</a>; <a href="references.html#ref-takayanagi2019120" role="doc-biblioref">Takayanagi et al. 2019</a>; <a href="references.html#ref-ikeno20224" role="doc-biblioref">Ikeno et al. 2022</a>)</span>. In almost all cases, the FD itself participates in collecting the overflow charges, too. In this way, the FWC of the small pixels, <span class="math inline">\(S_4\)</span>, is effectively the total capacity of the photodiode, the LOFIC, and the FD. This further extends the small pixel’s saturation level to <span class="math inline">\(L_3\)</span>, shown in <a href="#fig-sp_dcg_dr" class="quarto-xref">Figure&nbsp;<span>17.9</span></a>.</p>
<p>LOFIC can be used in conjunction with DCG. For instance, in the HCG read-out we would use only the FD as the measurement node, and in the LCG read-out we would use both the FD and LOFIC <span class="citation" data-cites="takayanagi2019120">(<a href="references.html#ref-takayanagi2019120" role="doc-biblioref">Takayanagi et al. 2019</a>)</span>. Of course we can also add additional FDs to lower the conversion gain even more <span class="citation" data-cites="iida20180">(<a href="references.html#ref-iida20180" role="doc-biblioref">Iida et al. 2018</a>)</span>. <!-- Fig. 2 in @takayanagi2019120 has a good timing diagram for how FD and LOFIC are used. --></p>
<p>We could also combine the split-pixel architecture with DCG <span class="citation" data-cites="iida20180 solhusvik20191280 willassen20151280">(<a href="references.html#ref-iida20180" role="doc-biblioref">Iida et al. 2018</a>; <a href="references.html#ref-solhusvik20191280" role="doc-biblioref">Solhusvik et al. 2019</a>; <a href="references.html#ref-willassen20151280" role="doc-biblioref">Willassen et al. 2015</a>)</span>, where usually the large pixels are read-out with twice with DCG and the small pixels are read-out with only LCG; this is because large pixels are meant to sample low-luminance information so they benefit more from HCG. This is shown in <a href="#fig-sp_dcg_dr" class="quarto-xref">Figure&nbsp;<span>17.9</span></a>, where <span class="math inline">\(S_1\)</span> is the capacity of the LPD’s FD node, which saturates at a lower intensity than <span class="math inline">\(L_1\)</span> and is the measurement node in the HCG read-out. The LCG read-out can read all the charges in the FWC (with the help of an additional FD) at the cost of a lower conversion gain.</p>
<p>Another approach is the <strong>time-to-saturation</strong> (TTS) technology <span class="citation" data-cites="stoppa2002novel">(<a href="references.html#ref-stoppa2002novel" role="doc-biblioref">Stoppa et al. 2002</a>)</span>, which uses a counter to measure the time it takes for each pixel to saturate and use that time to extrapolate the information given the actual exposure time:</p>
<p><span class="math display">\[
    Q_{\text{act}} = Q_{\text{sat}} \frac{T_{\text{exp}}}{T_{\text{sat}}},
\]</span></p>
<p>where <span class="math inline">\(Q_{\text{act}}\)</span> is the actual number of charges a pixel would have collected without saturation, <span class="math inline">\(Q_{\text{sat}}\)</span> is the FWC, <span class="math inline">\(T_{\text{exp}}\)</span> is the exposure time, and <span class="math inline">\(T_{\text{sat}}\)</span> is the saturation time. One could combine TTS with DCG and LOFIC <span class="citation" data-cites="ikeno20224 liu20204 liu2022augmented">(<a href="references.html#ref-ikeno20224" role="doc-biblioref">Ikeno et al. 2022</a>; <a href="references.html#ref-liu20204" role="doc-biblioref">Liu et al. 2020</a>, <a href="references.html#ref-liu2022augmented" role="doc-biblioref">2022</a>)</span>.</p>
</section>
</section>
</section>
<section id="sec-chpt-imaging-sensor-arch" class="level2" data-number="17.3">
<h2 data-number="17.3" class="anchored" data-anchor-id="sec-chpt-imaging-sensor-arch"><span class="header-section-number">17.3</span> Global Architecture</h2>
<p>We have discussed the individual building blocks that are needed for a pixel to turn lights into digital values, but how are they put together in an actual image sensor supporting tens of millions pixels? This chapter talks about the global architecture of an image sensor. We will start with a common architecture followed by other variants.</p>
<div id="fig-global_arch" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-global_arch-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figs/global_arch.svg" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-global_arch-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.10: (a): the block diagram of a typical rolling-shutter image sensor with column-level amplifiers and ADCs, where pixels in the same column share the same amplifier and ADC; pixels are exposed and read out row by row under the control of the <code>RST</code> signal (connecting to the <code>M-RST</code> switches) and the <code>SEL</code> signal (connecting to the <code>M-SEL</code> switches) (for simplicity, we omit the per-row <code>TX</code> signal, which connects to all the <code>M-TX</code> switches in the same row); (b): timing diagram operating the image sensor in (a) with a rolling shutter; technically the FD reset should be overlapped with the exposure time but is lumped into the readout box for simplicity. (c): comparison of column-level ADC used in (a) with pixel-level ADC and array/chip-level ADC. (d): timing diagram operating the image sensor in (a) with a global shutter.
</figcaption>
</figure>
</div>
<section id="sec-chpt-imaging-sensor-arch-col" class="level3" data-number="17.3.1">
<h3 data-number="17.3.1" class="anchored" data-anchor-id="sec-chpt-imaging-sensor-arch-col"><span class="header-section-number">17.3.1</span> Column-Parallel Readout</h3>
<p><a href="#fig-global_arch" class="quarto-xref">Figure&nbsp;<span>17.10</span></a> (a) shows a typical arrangement, where pixels are organized as a 2D array, just like a (DRAM/SRAM) memory array, and each column has an amplifier and ADC shared by all the pixels in that column. That is, the <code>Output</code> pin in <a href="#fig-4taps" class="quarto-xref">Figure&nbsp;<span>17.4</span></a> of all the pixels in the same column are connected to the same amplifier and ADC. The read-out circuit is then connected to digital processing circuitry, which could potentially perform simple image-space operations such as downsampling, scaling, rotation, etc. There is also an I/O unit that transfers the pixels to the host processor, usually through the MIPI-CSI interface, and transfers commands/configuration data from the host processor, usually through the I2C interface, which has a much lower bandwidth than MIPI (Kb/s vs.&nbsp;Gb/s).</p>
<p>The pixels in the pixel array are addressed row by row through a row scanner logic, shown on the left of <a href="#fig-global_arch" class="quarto-xref">Figure&nbsp;<span>17.10</span></a> (a). Pixels in the same row share three external signals: a reset signal <code>RST</code>, which is connected to all the <code>M-RST</code> transistors in the row, a row-select signal <code>SEL</code>, which is connected to all the <code>M-SEL</code> transistors of the same row, and a transfer signal <code>TX</code> (omitted in the figure) connected to all the <code>M-TX</code> switches in the same row.</p>
<p>The operating sequence of the pixel rows is shown in <a href="#fig-global_arch" class="quarto-xref">Figure&nbsp;<span>17.10</span></a> (b); the times are not drawn to scale. Each row of pixels goes through the PD reset, exposure, and readout phases under the control of the three external signals (<code>RST</code>, <code>SEL</code>, and <code>TX</code>). Importantly, the three phases are pipelined across rows. That is, while the first row is being exposed, we can start resetting the PDs for the subsequent rows and preparing them for exposure. For instance, in the concrete example of <a href="#fig-global_arch" class="quarto-xref">Figure&nbsp;<span>17.10</span></a> (a), the first row is starting the read-out sequence, the n<sup>th</sup> row is starting the exposure, while all other rows in-between are currently under exposure. While the exposure times of different rows can overlap, their readout sequences cannot — pixels in the same column but different rows share the same the read-out circuitry.</p>
<p>We can see that the way the pixel array is addressed and operated is similar to how a memory array (e.g., SRAM/DRAM) is, where the data in an entire row is accessed at once. However, since the pixel rows are operated strictly sequentially (unless random sampling is needed <span class="citation" data-cites="feng2024blisscam">(<a href="references.html#ref-feng2024blisscam" role="doc-biblioref">Feng et al. 2024</a>)</span>), the row scanner logic does not need a decoder, which supports random accesses that a typical memory array would need. Instead, one can usually use parallel shift registers to generate the three external signals row by row.</p>
</section>
<section id="sec-chpt-imaging-sensor-arch-shut" class="level3" data-number="17.3.2">
<h3 data-number="17.3.2" class="anchored" data-anchor-id="sec-chpt-imaging-sensor-arch-shut"><span class="header-section-number">17.3.2</span> Rolling vs.&nbsp;Global Shutter</h3>
<p>The timing diagram suggests that pixels in different rows technically have slightly shifted exposure times, inherently using a <strong>rolling shutter</strong>. The mechanical focal-plane shutter shown in <a href="#fig-rolling_shutter" class="quarto-xref">Figure&nbsp;<span>17.6</span></a> (a) is inherently a rolling shutter. Rolling shutters introduce noticeable artifacts; one such example is shown in <a href="#fig-rolling_shutter" class="quarto-xref">Figure&nbsp;<span>17.6</span></a> (b), where the photo was taken by a camera traveling in a car driving at about 50 mph. As a result, the fence and gate appear slanted because vertical parts of these objects are taken at different times. Such an artifact is much less visible for more distant objects, such as the cliff (can you reason about why?).</p>
<p><strong>Global shutters</strong> address the rolling shutter artifacts by exposing all pixels at the same time. <a href="#fig-rolling_shutter" class="quarto-xref">Figure&nbsp;<span>17.6</span></a> (d) shows the timing diagram of a global shutter sensor; compare that with that of the rolling shutter sensor in <a href="#fig-rolling_shutter" class="quarto-xref">Figure&nbsp;<span>17.6</span></a> (a). All the PDs are reset at the same time and have the same exposure duration.</p>
<p>The pixels are still read out row by row due to the column-level design of the read-out circuitry. This means the pixel values have to be temporarily held in some form of analog buffer after exposure and before they are read out. One could certainly use the FD for this analog buffer — with the caveat that the this prevents the PD from starting a new exposure cycle. This is because starting a new exposure requires resetting the PD, which would also reset the corresponding FD, as shown in <a href="#fig-4taps" class="quarto-xref">Figure&nbsp;<span>17.4</span></a> (a). For that reason, it is common to implement an additional analog buffer inside each pixel. The buffer can be implemented either in the charge domain before the FD <span class="citation" data-cites="yasutomi2011two sakakibara201283db tournier2018hdr kumagai2018back yokoyama2018high kobayashi20171">(<a href="references.html#ref-yasutomi2011two" role="doc-biblioref">Yasutomi, Itoh, and Kawahito 2011</a>; <a href="references.html#ref-sakakibara201283db" role="doc-biblioref">Sakakibara et al. 2012</a>; <a href="references.html#ref-tournier2018hdr" role="doc-biblioref">Tournier et al. 2018</a>; <a href="references.html#ref-kumagai2018back" role="doc-biblioref">Y. Kumagai et al. 2018</a>; <a href="references.html#ref-yokoyama2018high" role="doc-biblioref">Yokoyama et al. 2018</a>; <a href="references.html#ref-kobayashi20171" role="doc-biblioref">Kobayashi et al. 2017</a>)</span> or implemented in the voltage domain after the FD <span class="citation" data-cites="kondo20153d stark2018back miyauchi2020stacked">(<a href="references.html#ref-kondo20153d" role="doc-biblioref">Kondo et al. 2015</a>; <a href="references.html#ref-stark2018back" role="doc-biblioref">Stark et al. 2018</a>; <a href="references.html#ref-miyauchi2020stacked" role="doc-biblioref">Miyauchi et al. 2020</a>)</span>. <!-- %miyauchi2020stacked Table 1 has a comparison between the two. --></p>
</section>
<section id="sec-chpt-imaging-sensor-arch-ro" class="level3" data-number="17.3.3">
<h3 data-number="17.3.3" class="anchored" data-anchor-id="sec-chpt-imaging-sensor-arch-ro"><span class="header-section-number">17.3.3</span> Pixel-Parallel and Chip-Level Readout</h3>
<p>We can also arrange the read-out circuitry differently, as illustrated in <a href="#fig-global_arch" class="quarto-xref">Figure&nbsp;<span>17.10</span></a> (c). For instance, we could have a <em>per-pixel</em> (gain-controllable) amplifier and ADC and, consequently, a per-pixel digital memory. This essentially allows each pixel to directly output digital values, giving rise to the so called <strong>Digital Pixel Sensor</strong> (DPS) design, which was first reported in <span class="citation" data-cites="fowler1994cmos">Fowler, El Gamal, and Yang (<a href="references.html#ref-fowler1994cmos" role="doc-biblioref">1994</a>)</span> and is recently gaining tractions <span class="citation" data-cites="liu2019intelligent">(<a href="references.html#ref-liu2019intelligent" role="doc-biblioref">Liu et al. 2019</a>)</span>, where the in-pixel memory can is a 6T SRAM cell and the entire pixel array acts almost like an SRAM array. The bottom-right panel in <a href="#fig-pixel_design" class="quarto-xref">Figure&nbsp;<span>17.5</span></a> shows the pixel design diagram of a DPS, where the in-pixel memory can be, for instance, a 6T SRAM cell. In this case, the entire pixel array is indeed like an SRAM array.</p>
<p>DPS increases the pixel design complexity and pixel sizes, which, without microlenses, reduces the fill factor. This can, however, be alleviated with a stacked design, which we will get to in <a href="#sec-chpt-imaging-sensor-arch-comp" class="quarto-xref"><span>Section 17.3.5</span></a>. The main advantage of the DPS is that it massively increases the readout bandwidth due to pixel-parallel ADCs, which could shorten the frame latency when using a global shutter (see <a href="#fig-global_arch" class="quarto-xref">Figure&nbsp;<span>17.10</span></a> (d)), especially when short exposure time is desirable (e.g., high frame rate or “snap-shot” photography).</p>
<p>Yet another read-out arrangement is to have a single gain-controllable amplifier and ADC for the entire pixel array. This is shown in <a href="#fig-cmos_vs_ccd" class="quarto-xref">Figure&nbsp;<span>17.11</span></a> (b). In this case, we not only need logic to scan rows one by one but also to scan columns one by one (e.g., through shift registers). This arrangement is not common (thus omitted in <a href="#fig-global_arch" class="quarto-xref">Figure&nbsp;<span>17.10</span></a> (c)) due to its slow read-out speed but is the only option for sensors based on the Charge-coupled Devices (CCD), a design that is different from all the designs we have discussed so far and is our focus next.</p>
<div id="fig-cmos_vs_ccd" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cmos_vs_ccd-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figs/cmos_vs_ccd.svg" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cmos_vs_ccd-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.11: (a) charge-shifting read-out architecture for CCDs. (b) read-out architecture for CMOS image sensors with a global, array-level amplifier. Adapted from <span class="citation" data-cites="nakamura2006image">Nakamura (<a href="references.html#ref-nakamura2006image" role="doc-biblioref">2006, fig. 3.5</a>)</span>.
</figcaption>
</figure>
</div>
</section>
<section id="sec-chpt-imaging-sensor-arch-ccd" class="level3" data-number="17.3.4">
<h3 data-number="17.3.4" class="anchored" data-anchor-id="sec-chpt-imaging-sensor-arch-ccd"><span class="header-section-number">17.3.4</span> CMOS vs.&nbsp;CCD Sensor</h3>
<p>All the sensor designs we have covered so far are called Complementary Metal-Oxide-Semiconductor (CMOS) sensors, because they heavily rely on circuitries implemented using the CMOS techonlogies. CCD sensor is the other major category of sensor design, first reported in <span class="citation" data-cites="boyle1970charge">Boyle and Smith (<a href="references.html#ref-boyle1970charge" role="doc-biblioref">1970</a>)</span>. Both CCD and CMOS sensors use silicon to implement the PDs, although the specific implementations can differ <span class="citation" data-cites="nakamura2006image">(<a href="references.html#ref-nakamura2006image" role="doc-biblioref">Nakamura 2006</a>, Chpt. 3.1.2)</span>. The main difference lies in how the charges generated by the PDs are read out. See <span class="citation" data-cites="fossum1993active">Fossum (<a href="references.html#ref-fossum1993active" role="doc-biblioref">1993</a>)</span>, <span class="citation" data-cites="fossum1997cmos">Fossum (<a href="references.html#ref-fossum1997cmos" role="doc-biblioref">1997</a>)</span>, <span class="citation" data-cites="el2005cmos">El Gamal and Eltoukhy (<a href="references.html#ref-el2005cmos" role="doc-biblioref">2005</a>)</span>, and more recently, <span class="citation" data-cites="fossum2024digital">Fossum, Teranishi, and Theuwissen (<a href="references.html#ref-fossum2024digital" role="doc-biblioref">2024</a>)</span> for the historical background and comparisons.</p>
<p>A CCD sensor directly reads out charges from pixels by <em>shifting</em> the collected charges row by row. When a row reaches the bottom of the pixel array, we then shift the charges column by column to a single, array-level SF amplifier (and potentially a gain-controllable amplifier and ADC afterwards). This architecture is shown in <a href="#fig-cmos_vs_ccd" class="quarto-xref">Figure&nbsp;<span>17.11</span></a> (a). In CMOS sensors, in contrast, the charges are converted to voltages <em>within</em> the pixels, and it is the voltage potentials that are being read out from the pixel array by <em>addressing</em>, rather than shifting across, individual rows. The CMOS architecture is shown in <a href="#fig-cmos_vs_ccd" class="quarto-xref">Figure&nbsp;<span>17.11</span></a> (b).</p>
<p>The key to a CCD sensor is the charge-coupled devices themselves. A CCD is a set of connected MOS capacitors that store and transfer, between them, charges <span class="citation" data-cites="hu2009modern">(<a href="references.html#ref-hu2009modern" role="doc-biblioref">Hu 2009</a>, Chpt. 5)</span>, invented by Willard Boyle and George E. Smith <span class="citation" data-cites="boyle1970charge">(<a href="references.html#ref-boyle1970charge" role="doc-biblioref">Boyle and Smith 1970</a>)</span><a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>. In a CCD image sensor, the CCDs are connected to the PDs. After the exposure, all the PDs simultaneously transfer their charges to the corresponding vertical CCDs. The vertical CCDs in the same column then act as a shift register, transferring the charges downward to the horizontal CCD at the bottom of the chip. When a row of charges reaches the horizontal CCDs, the charges are then transferred horizontally (again, in a shift-register fashion) to the SF amplifier, which turns charges to voltage.</p>
<p>Given this signal read-out architecture, it is perhaps unsurprising to see that CCD sensors inherently support global shutters: the CCDs used for shifting charges naturally store the charges temporarily during the read-out.</p>
<p>CCDs are fabricated using process technologies that are optimized for charge transfer and that are incompatible with the CMOS technologies. In contrast, the read-out architecture of the CMOS sensors can be fabricated using CMOS technologies. This is a huge advantage because non-imaging logics such as control (e.g., clock generation) and analog/digital processing (e.g., ADC, image processing, computer vision tasks) are also based on CMOS technologies. Such logics, in CCD sensors, need to be implemented on a separate chip that interfaces with the CCD chip, rather than integrated with the pixel array on the same chip in a CMOS image sensor.</p>
<p>As modern CMOS technologies mature and gradually take over the semiconductor industry, CMOS image sensors have become more appealing. The main advantage of the CCD sensors is their high SNRs. CCD sensors do not have active devices during read-out and, thus, avoid/minimize many sources of noise that CMOS sensors are vulnerable to, a point we will return to when discussing noise modeling<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>. Because of that, while consumer cameras today mostly use CMOS sensors, CCD sensors are still use widely used in many scenarios where imaging quality is critical, e.g., scientific imaging. For instance, many telescopes for astrophysics (e.g., Sloan Digital Sky Survey) still use CCD sensors.</p>
</section>
<section id="sec-chpt-imaging-sensor-arch-comp" class="level3" data-number="17.3.5">
<h3 data-number="17.3.5" class="anchored" data-anchor-id="sec-chpt-imaging-sensor-arch-comp"><span class="header-section-number">17.3.5</span> Computational and Stacked CMOS Image Sensors</h3>
<p>Because the imaging circuitries and the logic processing circuitries both use the CMOS process technologies, a clear trend in CMOS Image Sensor (CIS) design is to move into the sensor computations that are traditionally carried out outside the sensor, which gives rise to the notion of <strong>Computational CIS</strong>.</p>
<section id="cis-scaling-trends" class="level4">
<h4 class="anchored" data-anchor-id="cis-scaling-trends">CIS Scaling Trends</h4>
<p><a href="#fig-cis_trends" class="quarto-xref">Figure&nbsp;<span>17.12</span></a> (a) shows the percentage of computational CIS papers in International Solid-State Circuits Conference (ISSCC) and International Electron Devices Meeting (IEDM), two premier venues for semiconductor circuits and devices, from Year 2000 and Year 2022 with respect to all the CIS papers during the same time range. The trend is clear: increasingly more CIS designs integrate compute capabilities.</p>
<div id="fig-cis_trends" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cis_trends-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figs/cis_trends.svg" class="img-fluid figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cis_trends-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.12: (a) Percentage of conventional CIS, computational CIS, and stacked computational CIS designs from surveying all ISSCC and IEDM papers published between the year 2000 and 2022. Increasingly more CIS designs are computational. (b) CIS process node always lags behind conventional CMOS process node. This is because CIS node scaling tracks the pixel size scaling, which does not shrink aggressively due to the fundamental need of maintaining photon sensitivity. From <span class="citation" data-cites="ma2023camj">Ma et al. (<a href="references.html#ref-ma2023camj" role="doc-biblioref">2023, figs. 1, 3</a>)</span>.
</figcaption>
</figure>
</div>
<p>A key reason why we could integrate processing/computational capabilities into the CIS chip is because of the advancements in the CMOS technologies that, for instance, have significantly shrunk the feature size, which is the smallest physical dimension that can be reliably fabricated on a semiconductor chip and is proportional to the transistor size. At the same time, however, the PD size itself has not shrunk proportionally, meaning adding CMOS logic to the sensor increases the total chip area minimally in the grand scheme of things.</p>
<p>This is shown in <a href="#fig-cis_trends" class="quarto-xref">Figure&nbsp;<span>17.12</span></a> (b), where triangle markers show the pixel sizes in CIS designs from all ISSCC papers appeared during Year 2000 and Year 2022, which include leading industry CIS designs at different times. We overlay a trend line regressed from these CIS designs to better illustrate the pixel size scaling trend. As a comparison, the blue line at the bottom represents the standard CMOS technology node scaling laid out by the International Roadmap for Devices and Systems (IRDS) <span class="citation" data-cites="irds">(<a href="references.html#ref-irds" role="doc-biblioref">IRDS 2024</a>)</span>. We can see that the gap between the pixel size and the standard CMOS feature size steadily increases. In fact, the pixel size scaling stagnates at around 5 <span class="math inline">\(\mu m\)</span>, which has long been seen as the practical pixel size limit <span class="citation" data-cites="fossum1997cmos">(<a href="references.html#ref-fossum1997cmos" role="doc-biblioref">Fossum 1997</a>)</span>. As semiconductor manufacturers keep pulling rabbits out of a hat, the CMOS feature size is still, miraculously, shrinking (TSMC/Samsung are shipping products with a 2 nm process node in 2025), so the gap would still exist, at least for quite a while.</p>
</section>
<section id="computational-cis-architectures" class="level4">
<h4 class="anchored" data-anchor-id="computational-cis-architectures">Computational CIS Architectures</h4>
<p>The computations inside a CIS could take place in both the analog and the digital domain. <a href="#fig-cis_arch" class="quarto-xref">Figure&nbsp;<span>17.13</span></a> (b) illustrates one example where analog computing is integrated into a CIS chip before the ADC. Analog operations usually implement primitives for feature extraction <span class="citation" data-cites="bong201714 bong2017low">(<a href="references.html#ref-bong201714" role="doc-biblioref">Bong, Choi, Kim, Kang, et al. 2017</a>; <a href="references.html#ref-bong2017low" role="doc-biblioref">Bong, Choi, Kim, Han, et al. 2017</a>)</span>, object detection <span class="citation" data-cites="young2019data">(<a href="references.html#ref-young2019data" role="doc-biblioref">Young et al. 2019</a>)</span>, and DNN inference <span class="citation" data-cites="hsu202005 xu2021senputing">(<a href="references.html#ref-hsu202005" role="doc-biblioref">Hsu et al. 2020</a>; <a href="references.html#ref-xu2021senputing" role="doc-biblioref">H. Xu et al. 2021</a>)</span>. <a href="#fig-cis_arch" class="quarto-xref">Figure&nbsp;<span>17.13</span></a> (c) illustrates another example that integrates digital processing, such as ISP <span class="citation" data-cites="murakami20224">(<a href="references.html#ref-murakami20224" role="doc-biblioref">Murakami et al. 2022</a>)</span>, image filtering <span class="citation" data-cites="kim2005200">(<a href="references.html#ref-kim2005200" role="doc-biblioref">Kim et al. 2005</a>)</span>, and DNN <span class="citation" data-cites="bong2017low">(<a href="references.html#ref-bong2017low" role="doc-biblioref">Bong, Choi, Kim, Han, et al. 2017</a>)</span>.</p>
<div id="fig-cis_arch" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cis_arch-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figs/cis_arch.svg" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cis_arch-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.13: (a) Traditional 2D imaging CIS with the PD array and the ADCs. (b) Computational CIS with analog processing capabilities (before the ADCs). (c) Computational CIS with digital processing. (d) Stacked computational CIS with digital processing in a separate layer. Adapted from <span class="citation" data-cites="ma2023camj">Ma et al. (<a href="references.html#ref-ma2023camj" role="doc-biblioref">2023, fig. 2</a>)</span>.
</figcaption>
</figure>
</div>
<p>As the processing capabilities become more complex, CIS design has embraced 3D stacking technologies, as is evident by the increasing number of stacked CIS in <a href="#fig-cis_trends" class="quarto-xref">Figure&nbsp;<span>17.12</span></a>. <a href="#fig-cis_arch" class="quarto-xref">Figure&nbsp;<span>17.13</span></a> (d) illustrates a typical stacked design, where the processing logic is separated from, and stacked with, the pixel array layer. The different layers communicate through the hybrid bond or the micro Through-Silicon Via (<span class="math inline">\(\mu\)</span>TSV) <span class="citation" data-cites="liu2022augmented tsugawa2017pixel">(<a href="references.html#ref-liu2022augmented" role="doc-biblioref">Liu et al. 2022</a>; <a href="references.html#ref-tsugawa2017pixel" role="doc-biblioref">Tsugawa et al. 2017</a>)</span>. The processing layer typically integrates digital processors, such as ISP <span class="citation" data-cites="kwon2020low">(<a href="references.html#ref-kwon2020low" role="doc-biblioref">Kwon et al. 2020</a>)</span>, image processing <span class="citation" data-cites="hirata20217 kumagai20181">(<a href="references.html#ref-hirata20217" role="doc-biblioref">Hirata et al. 2021</a>; <a href="references.html#ref-kumagai20181" role="doc-biblioref">O. Kumagai et al. 2018</a>)</span>, and DNN accelerators <span class="citation" data-cites="eki20219 liu2022augmented">(<a href="references.html#ref-eki20219" role="doc-biblioref">Eki et al. 2021</a>; <a href="references.html#ref-liu2022augmented" role="doc-biblioref">Liu et al. 2022</a>)</span>.</p>
<p>Three-layer stacked designs have also been proposed. Sony IMX 400 <span class="citation" data-cites="haruta20174">(<a href="references.html#ref-haruta20174" role="doc-biblioref">Haruta et al. 2017</a>)</span> is a 3-layer design that integrates a pixel layer, a DRAM layer (1 Gbit), and a logic layer with an Image Signal Processor (ISP). The DRAM layer buffers high-rate frames before streaming them out to the host. This enables super slow motion (960 FPS); otherwise, the bandwidth of the MIPI CSI-2 interface limits the capturing rate of the sensor. Meta conceptualizes a three-layer design <span class="citation" data-cites="liu2022augmented">(<a href="references.html#ref-liu2022augmented" role="doc-biblioref">Liu et al. 2022</a>)</span> with a pixel array layer, a per-pixel ADC layer, and a digital processing layer that integrates a DNN accelerator — using DPS. Stacking makes it easier to implement DPS: the main disadvantage of DPS is the complexity of the pixel design, but with stacking, the additional pixel processing circuitry (gain amplifier, ADC, etc.) can be “hidden” on a separate layer than the pixel array layer <span class="citation" data-cites="liu2022augmented liu20204">(<a href="references.html#ref-liu2022augmented" role="doc-biblioref">Liu et al. 2022</a>, <a href="references.html#ref-liu20204" role="doc-biblioref">2020</a>)</span>.</p>
</section>
<section id="challenges-of-cis" class="level4">
<h4 class="anchored" data-anchor-id="challenges-of-cis">Challenges of CIS</h4>
<p>Moving computation inside a CIS, however, is not without challenges. Most importantly, processing inside the sensor is far less efficient than that outside the sensor. This is because, while the CIS is implemented using the CMOS technologies, it uses significantly <em>older</em> process nodes than that of the conventional CMOS.</p>
<p>This is shown in <a href="#fig-cis_trends" class="quarto-xref">Figure&nbsp;<span>17.12</span></a> (b), where the square markers show the process node used in each CIS paper surveyed. As a reference, the IRDS standard CMOS process node scaling line is also shown. At around the year 2000, the CIS process node started lagging behind that of the conventional CMOS node, and the gap is increasing. CIS designs today commonly use 65 nm and older process nodes. This gap is not an artifact of the CIS designs we pick; it is fundamental: there is simply no need to aggressively scale down the process node because the pixel size does not, and can not, shrink much. In fact, from <a href="#fig-cis_trends" class="quarto-xref">Figure&nbsp;<span>17.12</span></a> (b) we can see that the slope of CIS process node scaling almost exactly follows that of the pixel size scaling. The reason that pixel size does not shrink much is to ensure light sensitivity: a small pixel reduces the number of photons it can collect, which directly reduces the dynamic range and the SNR<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>.</p>
<p>Inefficient in-sensor processing can be mitigated through 3D stacking technologies, which allow for heterogeneous integration: the pixel layer and the computing layer(s) can use their respective, optimal process node. Stacking, however, could increase power density, especially when future CIS integrates more processing capabilities. Therefore, harnessing the power of (stacked) computational CIS requires exploring a large design space and is still an active area of research <span class="citation" data-cites="ma2024efficient feng2024blisscam ma2023camj">(<a href="references.html#ref-ma2024efficient" role="doc-biblioref">Ma 2024</a>; <a href="references.html#ref-feng2024blisscam" role="doc-biblioref">Feng et al. 2024</a>; <a href="references.html#ref-ma2023camj" role="doc-biblioref">Ma et al. 2023</a>)</span>.</p>
</section>
</section>
</section>
<section id="sec-chpt-imaging-sensor-optics" class="level2" data-number="17.4">
<h2 data-number="17.4" class="anchored" data-anchor-id="sec-chpt-imaging-sensor-optics"><span class="header-section-number">17.4</span> In-Sensor Optics</h2>
<p>The on-chip optics serve a few purposes: blocking lights in the IR/UV ranges, boosting photon collection efficiency, anti-aliasing, and filtering for color reproduction.</p>
<section id="sec-chpt-imaging-sensor-optics-filters" class="level3" data-number="17.4.1">
<h3 data-number="17.4.1" class="anchored" data-anchor-id="sec-chpt-imaging-sensor-optics-filters"><span class="header-section-number">17.4.1</span> IR/UV Cut-Off Filters</h3>
<p>Many cameras have cut-off filters for infrared (IR) and ultraviolet (UV) lights. Their goals are to remove/block IR or UV lights, as much as possible, from the incident light. These filters are transparent in that they predominantly absorb light while scattering very little light. So their optical behaviors can be adequately captured by their transmittance spectra. <a href="#fig-cut_off_filter" class="quarto-xref">Figure&nbsp;<span>17.14</span></a> (left) shows the transmittance spectrum of the cut-off filter on the Nikon D200, where light below 400 nm and above 700 nm is essentially blocked from hitting the sensor.</p>
<div id="fig-cut_off_filter" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cut_off_filter-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figs/cut_off_filter.svg" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cut_off_filter-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.14: Left: transmittance spectrum of the on-chip cut-off optics on Nikon D200; from Kolarivision <span class="citation" data-cites="cut_off_filter">Melentijevic (<a href="references.html#ref-cut_off_filter" role="doc-biblioref">2015</a>)</span>. Right: IR thermal imaging uses light power in the IR range to estimate temperature; from <span class="citation" data-cites="ir_cam">Arno / Coen (<a href="references.html#ref-ir_cam" role="doc-biblioref">2006</a>)</span>.
</figcaption>
</figure>
</div>
<p>The reason most photographic cameras want to remove IR and UV lights is because the human visual system is not sensitive to IR and UV lights (recall our earlier discussions about the spectra of the cone fundamentals, which drop to 0 beyond roughly the 380 <span class="math inline">\(\text{nm}\)</span> and 780 <span class="math inline">\(\text{nm}\)</span> range). So for a camera to accurately reproduce the color of an object as if the object is directly viewed by the human eyes, the sensor’s sensitivity ideally needs to mimic that of the human eyes. Cutting IR and UV lights, to which our photoreceptors are not sensitive, is just the first step. We will discuss in detail in <a href="#sec-chpt-imaging-sensor-color" class="quarto-xref"><span>Section 17.7</span></a> what other mechanisms are in place for accurate color reproduction in image sensors.</p>
<p>Interestingly, thermographic cameras detect optical power in the IR range to estimate object temperature. Any object above absolute zero radiates, and this is call the <strong>blackbody radiation</strong>. Planck’s law governs the electromagnetic power emitted at a particular wavelength at a particular temperature. It turns out that at room temperature (about 300 K), most of the radiation power is in the IR range; very little radiation comes from the visible range. That is why thermal cameras use IR radiation for temperature estimation. <a href="#fig-cut_off_filter" class="quarto-xref">Figure&nbsp;<span>17.14</span></a> (right) shows an example of an IR image visualized as a heatmap, a real heatmap. <!-- %https://en.wikipedia.org/wiki/Black-body_radiation (a black body at room temperature (300 K) with one square meter of surface area will emit a photon in the visible range (390–750 nm) at an average rate of one photon every 41 seconds) --></p>
</section>
<section id="sec-chpt-imaging-sensor-optics-ml" class="level3" data-number="17.4.2">
<h3 data-number="17.4.2" class="anchored" data-anchor-id="sec-chpt-imaging-sensor-optics-ml"><span class="header-section-number">17.4.2</span> Microlenses</h3>
<p>An important figure of merit of image sensors is the <strong>fill factor</strong> (FF), which is defined as the ratio of the photosensitive area of a pixel to the actual pixel area. Usually the photosensitive area is much smaller than the pixel area. This is because in addition to the actual photodiode, a pixel contains many other electrical components (capacitors, transistors, and other complex logic gates) that take up the area. This is illustrated in <a href="#fig-microlenses" class="quarto-xref">Figure&nbsp;<span>17.15</span></a> (a), where many incident lights will not reach the PD, leading to a low FF. Given a fixed pixel area, a low FF means the pixel collects fewer photons during exposure, which translates to a higher signal-to-noise ratio, so it is almost always desirable to have a higher FF.</p>
<div id="fig-microlenses" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-microlenses-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figs/microlenses_new.svg" class="img-fluid figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-microlenses-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.15: (a): without a microlens, the photosensitive area of a pixel is the PD area; many incident lights will not hit the PD, leading to a low fill factor. (b): microlenses increase the effective fill factor of an image sensor.
</figcaption>
</figure>
</div>
<p>One common way to increase the FF that is prevalent in almost all image sensors is through microlenses. This is illustrated in <a href="#fig-microlenses" class="quarto-xref">Figure&nbsp;<span>17.15</span></a> (b). Every pixel has a convex lens, which we call a <strong>microlens</strong>, sitting on top of it. The job of the microlens is to, ideally, direct all the photons hitting the pixel to the photodiode, in which case the FF would effectively be 100%, which contemporary image sensors are very close to.</p>
</section>
<section id="sec-chpt-imaging-sensor-optics-aa" class="level3" data-number="17.4.3">
<h3 data-number="17.4.3" class="anchored" data-anchor-id="sec-chpt-imaging-sensor-optics-aa"><span class="header-section-number">17.4.3</span> Anti-Aliasing Filters</h3>
<p>Many image sensors also have anti-aliasing (AA) filters, especially photographic sensors. Recall that pixels perform spatial sampling of the optical image, which is continuous, thus introducing aliasing. The classic anti-aliasing method is to pre-filter the continuous signal using a low-pass filter, essentially blurring the signal and reducing its peak frequency. <span class="citation" data-cites="pharr2023physically">Pharr, Jakob, and Humphreys (<a href="references.html#ref-pharr2023physically" role="doc-biblioref">2023</a>, Chpt. 8)</span> and <span class="citation" data-cites="glassner1995principles">Glassner (<a href="references.html#ref-glassner1995principles" role="doc-biblioref">1995</a>, Unit II)</span> provide great technical discussions of signal sampling and reconstruction, which we will omit here.</p>
<p>In some sense, the photodiodes themselves and the microlenses act as pre-filters already: they inherently perform spatial 2D box convolutions over the continuous signal impinging upon them. Take the photodiode as an example: each photodiode integrates all the incident photons, as we have seen in <a href="#sec-chpt-imaging-sensor-pixel" class="quarto-xref"><span>Section 17.2</span></a>, and integration is equivalent to convolving/filtering the signal with a 2D box filter.</p>
<p>However, the support of the filter carried by the microlens and the photodiode is small: the microlens filter has a size of the pixel area, and the photodiode filter support is even more compact. To more aggressively pre-filter the signal, we need a filter with a wide support. To that end, AA filters use birefringent material, as shown in <a href="#fig-aa_psf" class="quarto-xref">Figure&nbsp;<span>17.16</span></a> (a), which essentially splits a ray into two rays, each with a different polarization and, thus, takes a slightly different path (recall that the refractive index depends on the polarization of light). If we cascade two such materials, a ray gets split into four rays; this is called a 4-dot beam splitting. This is done by, e.g., the Nikon D800e, as shown in <a href="#fig-aa_psf" class="quarto-xref">Figure&nbsp;<span>17.16</span></a> (b).</p>
<div id="fig-aa_psf" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-aa_psf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figs/aa_psf_new.svg" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-aa_psf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.16: (a): a birefringent material that, through double refraction, splits a ray into two; adapted from <span class="citation" data-cites="birefringent_material">APN MJM (<a href="references.html#ref-birefringent_material" role="doc-biblioref">2011</a>)</span>. (b): many anti-aliasing filters are made by cascading two birefringent materials that, collectively, split a ray into four; they are called 4-dot AA filters. (c): MTF of a 4-dot AA filter.
</figcaption>
</figure>
</div>
<p>The birefringent material acts as a low-pass filter. The intuition is that if an incident ray is spread over, say, 4 sensor-plane points, then each sensor-plane point, equivalently, integrates information from 4 incident rays, each coming from a distinct scene point (assuming a pinhole aperture). We know integration is essentially low-pass filtering.</p>
<p>The way to understand the effect of the AA filter is to analyze its Point Spread Function (PSF) and Modulation Transfer Function (MTF), which we have seen in <a href="imaging-optics.html#sec-chpt-imaging-optics-modeling-mtf" class="quarto-xref"><span>Section 16.5.3</span></a>. Assuming a pinhole aperture, a 4-dot beam-splitting AA filter essentially imposes a PSF where a scene point is spread over 4 sensor-plane points. The PSF is the sum of 4 Dirac Delta functions placed on a regular grid with an offset <span class="math inline">\(d\)</span> between adjacent grid points (which depends on the difference in refractive indices and the relative positions between the two splitting planes):</p>
<p><span class="math display">\[
    f(x, y) = \frac{1}{4}[\delta(x, y) + \delta(x-d, y) + \delta(x, y-d) + \delta(x-d, y-d)].
\]</span></p>
<p>With a little math, which we omit here, we can show that the MTF of this PSF is:</p>
<p><span class="math display">\[
    MTF(f_x, f_y) = |\cos(\pi d f_x)||\cos(\pi d f_y)|.
\]</span></p>
<p>An example of this MTF is shown in <a href="#fig-aa_psf" class="quarto-xref">Figure&nbsp;<span>17.16</span></a> (c), where the <span class="math inline">\(x\)</span>-axis and <span class="math inline">\(y\)</span>-axis are the two spatial frequencies <span class="math inline">\(f_x\)</span> and <span class="math inline">\(f_y\)</span>, and the <span class="math inline">\(a\)</span>-axis is the MTF. We can see that this particular MTF passes low frequencies and cuts off at a frequency of, in the case where <span class="math inline">\(d=1\)</span>, 0.5. Interestingly, the MTF also passes high frequencies, which is generally not a huge concern because power at high frequencies is usually already attenuated by the PSFs of other optical elements (e.g., the main imaging lens). Of course, in reality the aperture is not a pinhole, so the PSF is not simply a sum of four Delta functions but can nevertheless still be similarly analyzed.</p>
<div id="fig-aa_filter" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-aa_filter-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figs/aa_filter.svg" class="img-fluid figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-aa_filter-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.17: (a): a birefringent material that, through double refraction, splits a ray into two; adapted from <span class="citation" data-cites="birefringent_material">APN MJM (<a href="references.html#ref-birefringent_material" role="doc-biblioref">2011</a>)</span>. (b): many anti-aliasing filters are made by cascading two birefringent materials that, collectively, split a ray into four; they are called 4-dot AA filters. (c): MTF of a 4-dot AA filter.
</figcaption>
</figure>
</div>
<p><a href="#fig-aa_filter" class="quarto-xref">Figure&nbsp;<span>17.17</span></a> (a) and <a href="#fig-aa_filter" class="quarto-xref">Figure&nbsp;<span>17.17</span></a> (b) compare the images taken of the same scene by Nikon D800e, which lacks an AA filter, and Nikon D800, which has a 4-dot AA filter. Look at the AC’s condenser coil; the AA image is more blurred but has much less objectionable aliasing effect.</p>
</section>
</section>
<section id="sec-chpt-imaging-sensor-optics-monomodel" class="level2" data-number="17.5">
<h2 data-number="17.5" class="anchored" data-anchor-id="sec-chpt-imaging-sensor-optics-monomodel"><span class="header-section-number">17.5</span> Monochromatic, Noise-Free Sensor Model</h2>
<p>Each in-sensor optical element adds its own spectral transmittance, so the overall transmittance of the in-sensor optics is the product of them. We will simply use <span class="math inline">\(T(\lambda)\)</span> to represent the overall transmittance. Given what we have discussed so far, we can build an analytical model for a monochromatic, noise-free image sensor. The raw pixel value, also known as the Digital Number, <span class="math inline">\(n\)</span> of a pixel <span class="math inline">\(p\)</span> of size <span class="math inline">\(A_p\)</span> and is exposed for a duration of <span class="math inline">\(t_{exp}\)</span> is given by:</p>
<p><span id="eq-mono_model"><span class="math display">\[
\begin{aligned}
    N &amp;= \int_{\lambda} \int^{t_{exp}} \int^{A_p} Y(p, \lambda, t) T(\lambda) QE(\lambda) \text{d}p \text{d}t \text{d}\lambda, \\
    \Delta V &amp;= \frac{Nq}{C_{FD}} \times g, \\
    n &amp;= \lfloor \frac{\Delta V}{V_{max}} (2^{L} - 1) \rfloor,
\end{aligned}
\tag{17.7}\]</span></span></p>
<p>where <span class="math inline">\(Y(p, \lambda, t)\)</span> is the number of photons incident on position <span class="math inline">\(p\)</span> at a particular wavelength <span class="math inline">\(\lambda\)</span> at a particular time <span class="math inline">\(t\)</span>, so it is a quantal counterpart of the spectral irradiance; <span class="math inline">\(T(\lambda)\)</span> is the overall spectral transmittance of the in-sensor optics, <span class="math inline">\(QE(\lambda)\)</span> is the quantum efficiency, and <span class="math inline">\(q\)</span> is the elementary charge.</p>
<p>The first equation in <a href="#eq-mono_model" class="quarto-xref">Equation&nbsp;<span>17.7</span></a> models <span class="math inline">\(N\)</span>, the total amount of charges collected at the particular pixel, where we integrate spatially, temporally, and spectrally. The second equation in <a href="#eq-mono_model" class="quarto-xref">Equation&nbsp;<span>17.7</span></a> is essentially <a href="#eq-charge_measure" class="quarto-xref">Equation&nbsp;<span>17.5</span></a>, and models the voltage difference sensed before and after the exposure. The last equation in <a href="#eq-mono_model" class="quarto-xref">Equation&nbsp;<span>17.7</span></a> is a crude ADC model, assuming that the voltage range <span class="math inline">\([0, v_{max}]\)</span> is quantized into <span class="math inline">\(L\)</span> bits, and the output of the ADC model is the digital number, a.k.a., the raw pixel value.</p>
<p>How do we express <span class="math inline">\(Y(p, \lambda, t)\)</span>, the quantal counterpart of irradiance? The spectral irradiance at position <span class="math inline">\(p\)</span> and time <span class="math inline">\(t\)</span> is:</p>
<p><span id="eq-irra"><span class="math display">\[
    E(p, \lambda, t) = \int^{\Omega(p, V)} L(p, \omega, \lambda, t) \cos\theta~\text{d}\omega,
\tag{17.8}\]</span></span></p>
<p>where <span class="math inline">\(\Omega(p, V)\)</span> is the solid angle subtended by <span class="math inline">\(p\)</span> and the aperture <span class="math inline">\(V\)</span>; <span class="math inline">\(L(p, \omega, \lambda, t)\)</span> is the radiance with a wavelength <span class="math inline">\(\lambda\)</span> incident on <span class="math inline">\(p\)</span> from the direction <span class="math inline">\(\omega\)</span> at time <span class="math inline">\(t\)</span>, and <span class="math inline">\(\theta\)</span> is the polar angle subtended by <span class="math inline">\(\omega\)</span> and the pixel normal vector.</p>
<p>Given Planck’s equation (<a href="#eq-photon_energy" class="quarto-xref">Equation&nbsp;<span>17.1</span></a>), we can turn irradiance <span class="math inline">\(E\)</span> (energy per unit area per unit time) into the quantity <span class="math inline">\(Y\)</span> (photon quantity per unit area per unit time):</p>
<p><span id="eq-irra_to_quan"><span class="math display">\[
    Y(p, \lambda, t) = \frac{E(p, \lambda, t) \lambda}{hc}.
\tag{17.9}\]</span></span></p>
<p>Plugging <a href="#eq-irra" class="quarto-xref">Equation&nbsp;<span>17.8</span></a> and <a href="#eq-irra_to_quan" class="quarto-xref">Equation&nbsp;<span>17.9</span></a> into the <span class="math inline">\(N\)</span> expression in <a href="#eq-mono_model" class="quarto-xref">Equation&nbsp;<span>17.7</span></a>, we have:</p>
<p><span id="eq-cam_measurement_1"><span class="math display">\[
    N = \int_{\lambda} \int^{t_{exp}} \int^{A_p} \int^{\Omega(p, V)} \frac{L(p, \omega, \lambda, t) \cos\theta \text{d}\omega T(\lambda) QE(\lambda) \lambda}{hc} \text{d}p \text{d}t \text{d}\lambda.
\tag{17.10}\]</span></span></p>
<p>Rearranging the terms a bit we get:</p>
<p><span id="eq-cam_measurement_2"><span class="math display">\[
    N = \int_{\lambda} \Big( \int^{t_{exp}} \int^{A_p} \int^{\Omega(p, V)} L(p, \omega, \lambda, t) \cos\theta \text{d}\omega \text{d}p \text{d}t \Big) T(\lambda) QE(\lambda) \frac{\lambda}{hc} \text{d}\lambda.
\tag{17.11}\]</span></span></p>
<p>Recall from <a href="rendering-lightfield.html#sec-chpt-mat-basics-radiometry-cam" class="quarto-xref"><span>Section 10.1</span></a>, the inner four integrals in <a href="#eq-cam_measurement_2" class="quarto-xref">Equation&nbsp;<span>17.11</span></a> collectively form the so-called camera measurement equation, which calculates <span class="math inline">\(Q(\lambda)\)</span>, the energy at wavelength <span class="math inline">\(\lambda\)</span> collected by the pixel during the exposure<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>. Therefore, we get:</p>
<p><span id="eq-cam_measurement_3"><span class="math display">\[
    N = \int_{\lambda} Q(\lambda) T(\lambda) QE(\lambda) \frac{\lambda}{hc} \text{d}\lambda.
\tag{17.12}\]</span></span></p>
<p>We have implicitly assumed here that the effects of the in-sensor optics can simply be modeled by the spectral transmittance <span class="math inline">\(T(\lambda)\)</span>. This is largely reasonable because 1) in-sensor optics are mostly transparent and 2) they are very close to the pixels, so we can ignore rays that are incident on the edge of the optics and, after refractions, miss the pixels.</p>
<section id="sec-chpt-imaging-sensor-optics-monomodel-ssf" class="level3" data-number="17.5.1">
<h3 data-number="17.5.1" class="anchored" data-anchor-id="sec-chpt-imaging-sensor-optics-monomodel-ssf"><span class="header-section-number">17.5.1</span> Spectral Sensitivity Function</h3>
<p>We can make a few assumptions to simplify our discussion. First, we assume the ADC quantization error is negligible. Second, we assume that the irradiance within a pixel is spatially and temporally uniform during a short exposure time. The raw pixel value <span class="math inline">\(n\)</span> in <a href="#eq-mono_model" class="quarto-xref">Equation&nbsp;<span>17.7</span></a> is then simplified to:</p>
<p><span id="eq-mono_model_quan_1"><span class="math display">\[
    n \approx k \int_{\lambda} Y(p, \lambda, t) T(\lambda) QE(\lambda) \text{d}\lambda,
\tag{17.13}\]</span></span></p>
<p>where <span class="math inline">\(Y(p, \lambda, t)\)</span> is the (average) number of incident photons at wavelength <span class="math inline">\(\lambda\)</span> hitting position <span class="math inline">\(p\)</span> at time <span class="math inline">\(t\)</span>; <span class="math inline">\(k = uvt_{exp}\frac{qg}{C_{FD}}\frac{2^N-1}{V_{max}}\)</span> is a constant.</p>
<p>Let’s define a convenient term: <strong>Spectral Sensitivity Function</strong> (SSF), which is the product of <span class="math inline">\(T(\lambda)\)</span> and <span class="math inline">\(QE(\lambda)\)</span>. Therefore, we can rewrite <span class="math inline">\(n\)</span> as:</p>
<p><span id="eq-mono_model_quan_2"><span class="math display">\[
    n \approx k \int_{\lambda} Y(p, \lambda, t) SSF_{quantal}(\lambda) \text{d}\lambda.
\tag{17.14}\]</span></span></p>
<p>SSF is the only spectral (wavelength-dependent) term in <a href="#eq-mono_model_quan_2" class="quarto-xref">Equation&nbsp;<span>17.14</span></a> other than the incident light itself; it represents the phenomenological light sensitivity of the sensor over wavelength. SSF is sometimes also called the camera response function.</p>
<p>The SSF defined in <a href="#eq-mono_model_quan_2" class="quarto-xref">Equation&nbsp;<span>17.14</span></a> is an “equal-quantal” function because it tells us the relative responses between different wavelengths under the same amount of incident photons. We can turn it into an “equal-energy” or “equal-power” function that operates on energy or power. We first express the raw pixel value <span class="math inline">\(n\)</span> in terms of the spectral power distribution <span class="math inline">\(\Phi(\lambda)\)</span> rather than the spectral quantity distrubition <span class="math inline">\(Y(\lambda)\)</span> and rewrite <a href="#eq-mono_model_quan_2" class="quarto-xref">Equation&nbsp;<span>17.14</span></a> as:</p>
<p><span id="eq-mono_model_pow_2"><span class="math display">\[
    n \approx k \int_{\lambda} \frac{\Phi(p, \lambda, t)}{t_{exp}\frac{hc}{\lambda}} SSF_{quantal}(\lambda) \text{d}\lambda,
\tag{17.15}\]</span></span></p>
<p>where <span class="math inline">\(\Phi(p, \lambda, t)\)</span> denotes the spectral power distribution of the light hitting position <span class="math inline">\(p\)</span> at time <span class="math inline">\(t\)</span>. Now let’s absorb <span class="math inline">\(t_{exp}hc\)</span> into <span class="math inline">\(k\)</span> and define <span class="math inline">\(k' = uv\frac{qg}{C_{FD}}\frac{2^N-1}{V_{max}}\frac{1}{hc}\)</span> and <span class="math inline">\(SSF_{power}(\lambda) = SSF_{quantal}(\lambda)\lambda\)</span>, we get:</p>
<p><span id="eq-mono_model_pow_3"><span class="math display">\[
    n \approx k' \int_{\lambda} \Phi(p, \lambda, t)SSF_{power}(\lambda) \text{d}\lambda.
\tag{17.16}\]</span></span></p>
<p><span class="math inline">\(SSF_{power}(\lambda)\)</span> is the equal-power SSF. The subscript is usually omitted in the literature because it is usually clear what SSF is being used (e.g., from the quantity that is being multiplied with the SSF). Also note that in some literature, the SSF is used interchangeably with QE, so be very careful.</p>
</section>
</section>
<section id="sec-chpt-imaging-sensor-optics-pixel" class="level2" data-number="17.6">
<h2 data-number="17.6" class="anchored" data-anchor-id="sec-chpt-imaging-sensor-optics-pixel"><span class="header-section-number">17.6</span> What is a Pixel?</h2>
<p>Given the model of how a pixel value is generated, we can develop a more fundamental understanding of pixels. Perhaps the first question to ask is: what is a pixel? There are at least three forms of pixel that are relevant to us: a pixel on an image sensor, a pixel in a digital image, and a pixel on a display. They participate in the processes of filtering, sampling, and reconstructing the underlying light signal.</p>
<div id="fig-pixel_sampling" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pixel_sampling-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figs/pixel_sampling.svg" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pixel_sampling-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.18: (a): the continuous optical image impinging on the sensor plane; (b): each image sensor pixel integrates the photon energy incident on the pixel surface; (c): this is equivalent to filtering the optical image with a Box (average) filter and then sampling the filtered image only at the pixel centers; (d): the resulting array of samples is essentially a digital image (after some in-sensor processing); (e): a display takes that array of samples and reconstructs a continuous signal using a Box filter, equivalent to a nearest neighbor interpolation; (f): the chain of signal processing summarized.
</figcaption>
</figure>
</div>
<p>During an exposure period, the photons in the scene, after going through the various optics, rain down on the sensor plane. This is illustrated in <a href="#fig-pixel_sampling" class="quarto-xref">Figure&nbsp;<span>17.18</span></a> (a), where for illustration purpose shows arbitrarily only a handful of photons but in reality every point on the sensor receives photons from all directions. The spatial energy distribution on the sensor plane is what we call an optical image <span class="math inline">\(OI(p)\)</span>, a 2D continuous signal that tells us the energy at any point <span class="math inline">\(p\)</span> on the sensor plane. In radiometry, the energy of an infinitesimal point is called <strong>radiant exposure</strong> (whose unit is <span class="math inline">\(\text{J}/\text{m}^\text{2}\)</span>), equivalent to the irradiance of the point integrated over the exposure time.</p>
<p>As we have discussed above and seen in the measurement equation (<a href="rendering-lightfield.html#eq-cam_area" class="quarto-xref">Equation&nbsp;<span>10.2</span></a>), each sensor pixel spatially integrates the energy across its surface area, shown in <a href="#fig-pixel_sampling" class="quarto-xref">Figure&nbsp;<span>17.18</span></a> (b). This integration is equivalent to a cascade of two operations:</p>
<ol type="1">
<li>filtering the optical image using a 2D Box (average) filter <span class="math inline">\(B_i\)</span> with a support equivalent to the pixel size: <span class="math inline">\(FOI(p) = (OI \star B_i)(p)\)</span>, and</li>
<li>sampling the filtered signal at the center of each pixel: <span class="math inline">\(I(p) = FOI(p) \mathop{\mathrm{III}}(p)\)</span>, where <span class="math inline">\(\mathop{\mathrm{III}}(p)\)</span> is the 2D Dirac Comb function that is only non-zero at the pixel centers.</li>
</ol>
<p>Filtering with a Box filter is essentially integration, and filtering/convolution followed by sampling is equivalent to computing the convolution only at the sampled positions. The result is an array of samples, shown in <a href="#fig-pixel_sampling" class="quarto-xref">Figure&nbsp;<span>17.18</span></a> (c). Each sample then is processed inside the sensor (e.g., turned into charges by <a href="#eq-cam_measurement_3" class="quarto-xref">Equation&nbsp;<span>17.12</span></a>) and eventually read out as a digital number (<span class="math inline">\(n\)</span> in <a href="#eq-mono_model" class="quarto-xref">Equation&nbsp;<span>17.7</span></a>), i.e., a pixel value in the final digital image, shown in <a href="#fig-pixel_sampling" class="quarto-xref">Figure&nbsp;<span>17.18</span></a> (d).</p>
<p>That is, the digital image we get is nothing more than a (processed) array of samples of the filtered optical image <span class="math inline">\(FOI\)</span>. Ignoring the ADC quantization error and noise, the value of an image pixel is proportional to the corresponding sample in <span class="math inline">\(FOI\)</span>.</p>
<p>Now to display the image, we send that array to the display. For simplicity, let’s assume that we are dealing with monochromatic displays, in which case each image pixel drives a single display pixel. A display pixel, like a sensor pixel, is small but has a non-zero area. Each point on the display pixel also has a radiant exposure, which ideally is proportional to the image pixel value and is uniform across the entire pixel area.</p>
<p>Therefore, ignoring gaps between display pixels, ultimately what we get from the display is another 2D, continuous signal <span class="math inline">\(DOI\)</span>, shown in <a href="#fig-pixel_sampling" class="quarto-xref">Figure&nbsp;<span>17.18</span></a> (e). This is essentially reconstructing a continuous signal <span class="math inline">\(DOI\)</span> from the digital image (an array of samples) by applying, again, a Box filter <span class="math inline">\(B_d\)</span>: <span class="math inline">\(DOI(p) = (FOI \star B_d)(p)\)</span>. This filtering is equivalent to a nearest neighbor interpolation. The entire chain of signal processing from sensor pixels to digital image pixels and display pixels is summarized in <a href="#fig-pixel_sampling" class="quarto-xref">Figure&nbsp;<span>17.18</span></a> (f).</p>
<p>It is worth noting that our discussion above greatly simplifies what the display pixels actually do. Most importantly, the signal ultimately coming out of the display is not 2D but actually a light field: every point on the display emits lights across a range of directions, each of which has a spectral power distribution (that gives rise to color) that might change over time. How a display pixel turns a single image pixel value into a light field very much depends on the actual display design, which we will discuss in more detail in <a href="display-electronics.html#sec-disp-sp" class="quarto-xref"><span>Section 21.2</span></a>.</p>
</section>
<section id="sec-chpt-imaging-sensor-color" class="level2" data-number="17.7">
<h2 data-number="17.7" class="anchored" data-anchor-id="sec-chpt-imaging-sensor-color"><span class="header-section-number">17.7</span> Color Sensing</h2>
<p>There is one main piece of the on-chip optics we have not discussed: the color filters, which are critical for color sensing and deserve their own section.</p>
<section id="sec-chpt-imaging-sensor-color-goal" class="level3" data-number="17.7.1">
<h3 data-number="17.7.1" class="anchored" data-anchor-id="sec-chpt-imaging-sensor-color-goal"><span class="header-section-number">17.7.1</span> Goal of Color Sensing</h3>
<p>What does it mean for an image sensor to capture color? We know that colors are subjective sensations caused by cone photoreceptor responses to light; a color can be expressed as a point in a 3D space formed by the L, M, and S cone responses, i.e., the LMS cone space. Ideally, if we can build an image sensor in such a way that it also possesses three kinds of pixels, each of which has a spectral sensitivity matching exactly that of a cone class (i.e., cone fundamental), the sensor would be able to accurately capture and reproduce the color information.</p>
<p>In fact, it is even sufficient for the sensor responses to be just a (linear) transformation away from the cone responses, as long as we can pre-calibrate the transformation matrix offline. This idea is illustrated in <a href="#fig-color_sensing_goal" class="quarto-xref">Figure&nbsp;<span>17.19</span></a>. We emphasize linear transformation here simply because it is computationally cheaper; nothing prevents you from designing a sensor sensitivity profile that requires a sophisticated transformation from the cone space.</p>
<div id="fig-color_sensing_goal" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-color_sensing_goal-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figs/color_sensing_goal.svg" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-color_sensing_goal-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.19: The goal of color sensing is to form a color space from the raw pixel values and for there to exist a (preferably linear) transformation between the sensor color space and a standard color space, typically the CIE XYZ space. Adapted from <span class="citation" data-cites="retina_photoreceptors">Blume and Garbazza and Spitschan (<a href="references.html#ref-retina_photoreceptors" role="doc-biblioref">2019</a>)</span>, <span class="citation" data-cites="spd_incan">Thorseth (<a href="references.html#ref-spd_incan" role="doc-biblioref">2015</a>)</span>, and <span class="citation" data-cites="iphone12_cam">ajay_suresh (<a href="references.html#ref-iphone12_cam" role="doc-biblioref">2021</a>)</span>}.
</figcaption>
</figure>
</div>
<p>Where do the three classes of spectral sensitivities come? Examine our monochromatic sensing model in <a href="#eq-mono_model_quan_2" class="quarto-xref">Equation&nbsp;<span>17.14</span></a>; it appears that all the pixels share the same response function and, thus, have the same spectral sensitivity: every pixel has the same quantum efficiency and the same optical elements sitting above them (so the same spectral transmittance of the optics).</p>
<p>There are a variety of ways to introduce sensitivity differences across pixels, which we will discuss shortly in <a href="#sec-chpt-imaging-sensor-color-impl" class="quarto-xref"><span>Section 17.7.2</span></a>. Assuming, for now, that we have somehow introduced the three classes of SSFs, denoted <span class="math inline">\(SSF_R(\lambda)\)</span>, <span class="math inline">\(SSF_G(\lambda)\)</span>, and <span class="math inline">\(SSF_B(\lambda)\)</span>. Given an incident light with an SPD <span class="math inline">\(\Phi(\lambda)\)</span>, the camera responses are:</p>
<p><span class="math display">\[
    [\int_{\lambda} \Phi(\lambda)SSF_{R}(\lambda) \text{d}\lambda, \int_{\lambda} \Phi(\lambda)SSF_{G}(\lambda) \text{d}\lambda, \int_{\lambda} \Phi(\lambda)SSF_{B}(\lambda) \text{d}\lambda].
\]</span></p>
<p>This is a direct invocation of <a href="#eq-mono_model_pow_3" class="quarto-xref">Equation&nbsp;<span>17.16</span></a> with the constant omitted. The color of the light expressed in the LMS cone space is:</p>
<p><span class="math display">\[
    [\int_{\lambda} \Phi(\lambda)L(\lambda) \text{d}\lambda, \int_{\lambda} \Phi(\lambda)M(\lambda) \text{d}\lambda, \int_{\lambda} \Phi(\lambda)S(\lambda) \text{d}\lambda].
\]</span></p>
<p>If the cone responses form a 3D cone space, the camera raw responses also form a color space, which is sometimes called the camera’s native color space. We provide an interactive tutorial that allows you to interactively explore and compare the native color spaces of various cameras and the LMS cone space. <a href="#fig-camera_ssf" class="quarto-xref">Figure&nbsp;<span>17.20</span></a> (left) shows the SSFs of iPhone 11 (solid lines) and the cone fundamentals. The SSFs are normalized so that <span class="math inline">\(SSF_G\)</span> is peaked at unity, and the cone fundamentals are each normalized to peak at unity, so you could compare the relative sensitivity between the three SSFs in iPhone 11 but could not between the cone classes. Usually the SSF of a camera depends on a variety of factors such as the materials of the optical elements and the photodiodes as well as the pixel design, so it is almost impossible for the three SSFs to match exactly the cone fundamentals. <a href="#fig-camera_ssf" class="quarto-xref">Figure&nbsp;<span>17.20</span></a> (right) shows the spectral locus in iPhone 11’s native color space and in the cone space; they evidently do not overlap.</p>
<div id="fig-camera_ssf" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-camera_ssf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figs/camera_ssf.svg" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-camera_ssf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.20: Left: Spectral sensitivity functions of iPhone 11 (the RGB filters; solid lines) in comparison with the LMS cone fundamentals (dashed lines). Right: the spectral locus in the LMS space and in the camera’s native color space. Adapted from <span class="citation" data-cites="zhu2022cam">Zhu (<a href="references.html#ref-zhu2022cam" role="doc-biblioref">2022</a>)</span>.
</figcaption>
</figure>
</div>
<p>A major task in sensor calibration is to identify a transformation matrix <span class="math inline">\(M\)</span> such that the following (approximately) holds:</p>
<p><span class="math display">\[
\begin{aligned}
\begin{bmatrix}
    \int_{\lambda} \Phi(\lambda)SSF_{R}(\lambda) \text{d}\lambda\\
    \int_{\lambda} \Phi(\lambda)SSF_{G}(\lambda) \text{d}\lambda\\
    \int_{\lambda} \Phi(\lambda)SSF_{B}(\lambda) \text{d}\lambda
\end{bmatrix}
\times M =
\begin{bmatrix}
    \int_{\lambda} \Phi(\lambda)L(\lambda) \text{d}\lambda\\
    \int_{\lambda} \Phi(\lambda)M(\lambda) \text{d}\lambda\\
    \int_{\lambda} \Phi(\lambda)S(\lambda) \text{d}\lambda
\end{bmatrix}
\end{aligned}
\]</span></p>
<p>The transformation matrix is then applied in the post-processing pipeline of the raw pixels to turn raw pixel responses into a color value. We will discuss the calibration and the post-processing pipeline in greater details in <a href="imaging-isp.html" class="quarto-xref"><span>Chapter 19</span></a>.</p>
</section>
<section id="sec-chpt-imaging-sensor-color-impl" class="level3" data-number="17.7.2">
<h3 data-number="17.7.2" class="anchored" data-anchor-id="sec-chpt-imaging-sensor-color-impl"><span class="header-section-number">17.7.2</span> Implementing Three “Classes of Pixels”</h3>
<p>Perhaps the most straightforward method to introduce varying SSF is to apply a spectral filter to different pixels. A spectral filter is just a transparent optical element with a wavelength-selective transmittance. We need only three filters to emulate the three cone classes, but ideally each pixel should get all three simultaneously, which is difficult if you think about it, since at any given time you can physically have only one filter sitting on a pixel.</p>
<section id="three-shot-and-three-chip-methods" class="level4">
<h4 class="anchored" data-anchor-id="three-shot-and-three-chip-methods">Three-Shot and Three-Chip Methods</h4>
<p>There are two ways to go about addressing this issue. We can take three images of the same scene, each with a different filter, and then combine the together. This approach is believed to be pioneered by Sergey Prokudin-Gorsky, who conducted a breathtaking “photographic survey” of the early 20th-century Russia using this method <span class="citation" data-cites="gorskycollection">(<a href="references.html#ref-gorskycollection" role="doc-biblioref">Prokudin-Gorsky 1948</a>)</span>. This is called the “three-shot” approach. Alternatively, one could split the incident lights and send each of them to a different sensor, each with a different filter. This approach would obviously increase the form factor of the camera but avoids having to register and align the three separate shots, which is subjective to object motion. These camera are called “three-chip” or “three-CCD/COMS” cameras, which are still very widely used today in broadcasting, film studios, etc.</p>
</section>
<section id="color-filter-array-cfa" class="level4">
<h4 class="anchored" data-anchor-id="color-filter-array-cfa">Color Filter Array (CFA)</h4>
<p>Both the three-shot and the three-chip approach allow each incident light to be transformed to three responses needed for color reproduction — at the cost of capturing overhead or bulky system design. A much simpler approach, and the most commonly used approach today, is called Color Filter Array (CFA), which assigns each pixel only <em>one</em> filter.</p>
<div id="fig-bayer_cfa" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bayer_cfa-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figs/bayer_cfa.svg" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bayer_cfa-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.21: Left: the Bayer color filter array; from <span class="citation" data-cites="bayer_cfa">Cburnett (<a href="references.html#ref-bayer_cfa" role="doc-biblioref">2006</a>)</span>. Middle and Right: a Bayer-domain image where each pixel generates only one response and a full-color image assuming each pixel generates three responses; adapted from <span class="citation" data-cites="bayer_ex">Cmglee (<a href="references.html#ref-bayer_ex" role="doc-biblioref">2018</a>)</span>.
</figcaption>
</figure>
</div>
<p><a href="#fig-bayer_cfa" class="quarto-xref">Figure&nbsp;<span>17.21</span></a> shows the most commonly used CFA, where the three classes of filters are tiled in what is called the Bayer filter mosaic, named after Bryce Bayer, who invented this pattern while working for Eastman Kodak in Rochester, NY <span class="citation" data-cites="bayer1976color">(<a href="references.html#ref-bayer1976color" role="doc-biblioref">Bayer 1976</a>)</span>. Each of the three filters has a transmittance spectrum that peaks at, roughly, red-ish, green-ish, and blue-ish wavelengths, similar to the spectra shown in <a href="#fig-camera_ssf" class="quarto-xref">Figure&nbsp;<span>17.20</span></a> (left).</p>
<p>The three filter classes are organized in <span class="math inline">\(2\times 2\)</span> tiles, where each tile has two green filters. Bayer did so because he wanted to mimic human vision, where the photopic Luminance Efficiency Function (LEF) is most sensitive to green-ish lights <span class="citation" data-cites="sharpe2005luminous sharpe2011luminous">(<a href="references.html#ref-sharpe2005luminous" role="doc-biblioref">Sharpe et al. 2005</a>, <a href="references.html#ref-sharpe2011luminous" role="doc-biblioref">2011</a>)</span> (see <a href="hvs-color.html#fig-lef" class="quarto-xref">Figure&nbsp;<span>4.9</span></a>). We can see that the CFA approach is actually more similar to human color vision than the three-shot or three-chip approach. In human vision, each cone photoreceptor has a particular sensitivity spectrum, and generates one of the three responses needed to form color vision.</p>
<p>A necessary consequence of using the CFA is that each pixel gets only one color channel information. <a href="#fig-bayer_cfa" class="quarto-xref">Figure&nbsp;<span>17.21</span></a> (middle) shows a raw image captured using a CFA, where each pixel evidently has only one color channel. The overall image looks overwhelmingly green because of the sheer amount of green filters. An important step in the post-processing pipeline is to reconstruct the two other missing channels, a process called <strong>demosaicing</strong>, i.e., removing the Bayer mosaic artifacts. An example of the reconstructed image is shown in <a href="#fig-bayer_cfa" class="quarto-xref">Figure&nbsp;<span>17.21</span></a> (right).</p>
<p>We will have more to say about the demosaicing process when we get to <a href="imaging-isp.html" class="quarto-xref"><span>Chapter 19</span></a>, but for now, let’s just observe that demosaicing is nothing more than a signal sampling and reconstruction problem. The CFA allows each pixel to sample only one channel of the three channels of response. So the green-filter response, for instance, is sampled by half of the pixels<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>, and the other two responses are sampled by one quarter of the pixels each. The job of demosaicing is then to reconstruct the full signal responses from the samples — a well-established problem in signal processing.</p>
</section>
<section id="foveon-approach" class="level4">
<h4 class="anchored" data-anchor-id="foveon-approach">Foveon Approach</h4>
<p>The final approach does away with optical color filters altogether. Instead, we will use three photodiodes vertically stacked for each pixel. <a href="#fig-foveon_x3_pixel" class="quarto-xref">Figure&nbsp;<span>17.22</span></a> illustrates a pixel in the Foveon X3 sensor, which is perhaps the most famous sensor that uses this architecture.</p>
<div id="fig-foveon_x3_pixel" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-foveon_x3_pixel-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figs/foveon_x3_pixel.svg" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-foveon_x3_pixel-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.22: Illustration of the Foveon X3 pixel, which has three PDs made of the same material (silicon) vertically stacked; adapted from <span class="citation" data-cites="foveon_x3">Anoneditor (<a href="references.html#ref-foveon_x3" role="doc-biblioref">2007</a>)</span>. Each PD receives a different light spectrum (due to the depth-varying absorption), effectively creating three different responses of the same light incident on the pixel surface.
</figcaption>
</figure>
</div>
<p>The idea is that the silicon absorption spectrum is wavelength sensitive, as shown in the right panel of <a href="#fig-silicon_abs_qe" class="quarto-xref">Figure&nbsp;<span>17.3</span></a>. Blue-ish lights have a much shorter mean free length than do green-ish lights, which have a shorter mean free length than do red-ish lights. This means most short-wavelength lights will be absorbed after the first photodiode, leaving mostly medium- to long-wavelength lights. Those lights will go through the second photodiode, which absorbs mostly the medium-wavelength lights, leaving mostly long-wavelength lights to the third photodiode. As a result, each PD actually receives a different light spectrum, effectively creating three different responses for the same light incident on the pixel.</p>
<p>Let’s assume that the three PDs have a depth of <span class="math inline">\(d_B\)</span>, <span class="math inline">\(d_G\)</span>, and <span class="math inline">\(d_R\)</span>, respectively. The incident light impinging on the pixel (i.e., the first PD surface) has a SPD <span class="math inline">\(\Phi(\lambda)\)</span>. The light impinging on the second PD then has a spectrum <span class="math inline">\(\Phi(\lambda)e^{-\sigma(\lambda)d_B}\)</span>, where <span class="math inline">\(\sigma(\lambda)\)</span> is the silicon’s absorption coefficient spectrum. This is easily derived from the fact that pure absorption (no scattering and emission) leads to an exponential decay of the input signal (<a href="rendering-sss.html#eq-abs_4" class="quarto-xref">Equation&nbsp;<span>13.4</span></a>). Similarly, the light impinging on the third PD then has a spectrum <span class="math inline">\(\Phi(\lambda)e^{-\sigma(\lambda)(d_B+d_G)}\)</span>. The responses produced by the three PDs are thus (in the order of R, G, and G):</p>
<p><span class="math display">\[
    [\int_{\lambda}\Phi(\lambda)\eta_R(\lambda)e^{-\sigma(\lambda)(d_B+d_G)}, \int_{\lambda}\Phi(\lambda)\eta_G(\lambda)e^{-\sigma(\lambda)(d_B)}, \int_{\lambda}\Phi(\lambda)\eta_B(\lambda)],
\]</span></p>
<p>where <span class="math inline">\(\eta_R(\lambda)\)</span>, <span class="math inline">\(\eta_G(\lambda)\)</span>, and <span class="math inline">\(\eta_B(\lambda)\)</span> are QE spectra of the three PDs (where we consider only photons that reach a PD as the denominator in <a href="#eq-qe" class="quarto-xref">Equation&nbsp;<span>17.2</span></a> while ignoring photons that are reflected/absorbed before the photons hit the PD), respectively, and <span class="math inline">\(\Phi(\lambda)\)</span> is the SPD of the light incident on the pixel surface. The three PDs use identical material (so they share the same silicon absorption spectrum) but can still have different <span class="math inline">\(\eta(\lambda)\)</span>s because of the thickness differences — due to the differences in the lengths of the depletion and neutral regions in the PD p-n junctions. Can you guess why the thickness tends to increase for deeper PDs in <a href="#fig-foveon_x3_pixel" class="quarto-xref">Figure&nbsp;<span>17.22</span></a> (right)?</p>
<p>Compared to using the CFA, the vertical PD stacking approach is much more complicated to fabricate and more costly, so it is much less commonly used. It avoids color sampling (and the resulting aliasing) and the need for demosaicing, and in theory could also have a higher overall quantum efficiency (and signal-to-noise ratio) since there are no color filters, so it might find uses in scientific imaging <span class="citation" data-cites="chen2023bioinspired">(<a href="references.html#ref-chen2023bioinspired" role="doc-biblioref">Chen et al. 2023</a>)</span>.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-iphone12_cam" class="csl-entry" role="listitem">
ajay_suresh. 2021. <span>“<span class="nocase">iPhone 12 cameras; CC BY-SA 2.0 license</span>.”</span> <a href="https://commons.wikimedia.org/wiki/File:Apple_iPhone_12_Pro_-_Cameras_(50535314721).jpg" class="uri">https://commons.wikimedia.org/wiki/File:Apple_iPhone_12_Pro_-_Cameras_(50535314721).jpg</a>.
</div>
<div id="ref-akahane2006sensitivity" class="csl-entry" role="listitem">
Akahane, Nana, Shigetoshi Sugawa, Satoru Adachi, Kazuya Mori, Toshiyuki Ishiuchi, and Koichi Mizobuchi. 2006. <span>“A Sensitivity and Linearity Improvement of a 100-dB Dynamic Range CMOS Image Sensor Using a Lateral Overflow Integration Capacitor.”</span> <em>IEEE Journal of Solid-State Circuits</em> 41 (4): 851–58.
</div>
<div id="ref-foveon_x3" class="csl-entry" role="listitem">
Anoneditor. 2007. <span>“<span class="nocase">Illustration of the Foveon X3 sensor; CC BY-SA 3.0</span>.”</span> <a href="https://commons.wikimedia.org/wiki/File:Absorption-X3.png" class="uri">https://commons.wikimedia.org/wiki/File:Absorption-X3.png</a>.
</div>
<div id="ref-aoki19822" class="csl-entry" role="listitem">
Aoki, Masakazu, Haruhisa Ando, Shinya Ohba, Iwao Takemoto, Shusaku Nagahara, Toshio Nakano, Masaharu Kubo, and Tsutomu Fujita. 1982. <span>“2/3-Inch Format MOS Single-Chip Color Imager.”</span> <em>IEEE Transactions on Electron Devices</em> 29 (4): 745–50.
</div>
<div id="ref-birefringent_material" class="csl-entry" role="listitem">
APN MJM. 2011. <span>“<span class="nocase">A calcite crystal displays the double refractive properties while sitting on a sheet of graph paper; CC BY-SA 3.0 license</span>.”</span> <a href="https://commons.wikimedia.org/wiki/File:Crystal_on_graph_paper.jpg" class="uri">https://commons.wikimedia.org/wiki/File:Crystal_on_graph_paper.jpg</a>.
</div>
<div id="ref-ir_cam" class="csl-entry" role="listitem">
Arno / Coen. 2006. <span>“<span class="nocase">Thermogram of a snake wrapped around a human arm; CC BY-SA 3.0 license</span>.”</span> <a href="https://commons.wikimedia.org/wiki/File:Wiki_stranglesnake.jpg" class="uri">https://commons.wikimedia.org/wiki/File:Wiki_stranglesnake.jpg</a>.
</div>
<div id="ref-overexposure2" class="csl-entry" role="listitem">
Axel Jacobs. 2006. <span>“<span class="nocase">Open window with armchair and manequin. Sample scene for HDRI (standard LDR, single image from a set of bracketed exposures); CC BY-SA 2.0 license</span>.”</span> <a href="https://commons.wikimedia.org/wiki/File:HDRI_Sample_Scene_Window_-_08.jpg" class="uri">https://commons.wikimedia.org/wiki/File:HDRI_Sample_Scene_Window_-_08.jpg</a>.
</div>
<div id="ref-bayer1976color" class="csl-entry" role="listitem">
Bayer, Bryce E. 1976. <span>“Color Imaging Array.”</span>
</div>
<div id="ref-mcmaster2008wide" class="csl-entry" role="listitem">
Biretta, John A, and Matt McMaster. 2008. <em>Wide Field and Planetary Camera 2 Instrument Handbook v. 10.0</em>. Space Telescope Science Institute.
</div>
<div id="ref-retina_photoreceptors" class="csl-entry" role="listitem">
Blume and Garbazza and Spitschan. 2019. <span>“<span class="nocase">Schematic overview of photorecetors; CC BY-SA 4.0 license</span>.”</span> <a href="https://commons.wikimedia.org/wiki/File:Overview_of_the_retina_photoreceptors_(a).png" class="uri">https://commons.wikimedia.org/wiki/File:Overview_of_the_retina_photoreceptors_(a).png</a>.
</div>
<div id="ref-bong2017low" class="csl-entry" role="listitem">
Bong, Kyeongryeol, Sungpill Choi, Changhyeon Kim, Donghyeon Han, and Hoi-Jun Yoo. 2017. <span>“A Low-Power Convolutional Neural Network Face Recognition Processor and a CIS Integrated with Always-on Face Detector.”</span> <em>IEEE Journal of Solid-State Circuits</em> 53 (1): 115–23.
</div>
<div id="ref-bong201714" class="csl-entry" role="listitem">
Bong, Kyeongryeol, Sungpill Choi, Changhyeon Kim, Sanghoon Kang, Youchang Kim, and Hoi-Jun Yoo. 2017. <span>“14.6 a 0.62 mW Ultra-Low-Power Convolutional-Neural-Network Face-Recognition Processor and a CIS Integrated with Always-on Haar-Like Face Detector.”</span> In <em>2017 IEEE International Solid-State Circuits Conference (ISSCC)</em>, 248–49. IEEE.
</div>
<div id="ref-boyle1970charge" class="csl-entry" role="listitem">
Boyle, Willard S, and George E Smith. 1970. <span>“Charge Coupled Semiconductor Devices.”</span> <em>Bell System Technical Journal</em> 49 (4): 587–93.
</div>
<div id="ref-rs_artifacts" class="csl-entry" role="listitem">
BrayLockBoy. 2018. <span>“<span class="nocase">An example of the Rolling shutter effect in action at Afton Down, Isle of Wight, taken by a camera on a car travelling at approximately 50 miles per hour. CC BY-SA 4.0 license</span>.”</span> <a href="https://commons.wikimedia.org/wiki/File:Rolling_Shutter_Effect_at_Afton_Down,_21_August_2018.jpg" class="uri">https://commons.wikimedia.org/wiki/File:Rolling_Shutter_Effect_at_Afton_Down,_21_August_2018.jpg</a>.
</div>
<div id="ref-bayer_cfa" class="csl-entry" role="listitem">
Cburnett. 2006. <span>“<span class="nocase">A Bayer pattern on a sensor; CC BY-SA 3.0</span>.”</span> <a href="https://commons.wikimedia.org/wiki/File:Bayer_pattern_on_sensor.svg" class="uri">https://commons.wikimedia.org/wiki/File:Bayer_pattern_on_sensor.svg</a>.
</div>
<div id="ref-chen2023bioinspired" class="csl-entry" role="listitem">
Chen, Cheng, Ziwen Wang, Jiajing Wu, Zhengtao Deng, Tao Zhang, Zhongmin Zhu, Yifei Jin, et al. 2023. <span>“Bioinspired, Vertically Stacked, and Perovskite Nanocrystal–Enhanced CMOS Imaging Sensors for Resolving UV Spectral Signatures.”</span> <em>Science Advances</em> 9 (44): eadk3860.
</div>
<div id="ref-bayer_ex" class="csl-entry" role="listitem">
Cmglee. 2018. <span>“<span class="nocase">Images of a garden with some tulips and narcissus; CC BY-SA 3.0</span>.”</span> <a href="https://commons.wikimedia.org/wiki/File:Colorful_spring_garden_Bayer_%2B_RGB.png" class="uri">https://commons.wikimedia.org/wiki/File:Colorful_spring_garden_Bayer_%2B_RGB.png</a>.
</div>
<div id="ref-cis" class="csl-entry" role="listitem">
———. 2019. <span>“<span class="nocase">Comparison of front- vs. back-illuminated sensors; CC BY-SA 4.0 license</span>.”</span> <a href="https://commons.wikimedia.org/wiki/File:Comparison_backside_illumination.svg" class="uri">https://commons.wikimedia.org/wiki/File:Comparison_backside_illumination.svg</a>.
</div>
<div id="ref-dyck1968integrated" class="csl-entry" role="listitem">
Dyck, Rudolph H, and Gene P Weckler. 1968. <span>“Integrated Arrays of Silicon Photodetectors for Image Sensing.”</span> <em>IEEE Transactions on Electron Devices</em> 15 (4): 196–201.
</div>
<div id="ref-einstein1905heuristic" class="csl-entry" role="listitem">
Einstein, Albert. 1905a. <span>“On a Heuristic Point of View about the Creation and Conversion of Light.”</span> <em>Annalen Der Physik</em> 17 (6): 132–48.
</div>
<div id="ref-einstein1905erzeugung" class="csl-entry" role="listitem">
———. 1905b. <span>“<span>Ü</span>ber Einen Die Erzeugung Und Verwandlung Des Lichtes Betreffenden Heuristischen Gesichtspunkt.”</span> Albert Einstein-Gesellschaft.
</div>
<div id="ref-eki20219" class="csl-entry" role="listitem">
Eki, Ryoji, Satoshi Yamada, Hiroyuki Ozawa, Hitoshi Kai, Kazuyuki Okuike, Hareesh Gowtham, Hidetomo Nakanishi, et al. 2021. <span>“9.6 a 1/2.3 Inch 12.3 Mpixel with on-Chip 4.97 TOPS/w CNN Processor Back-Illuminated Stacked CMOS Image Sensor.”</span> In <em>2021 IEEE International Solid-State Circuits Conference (ISSCC)</em>, 64:154–56. IEEE.
</div>
<div id="ref-el2005cmos" class="csl-entry" role="listitem">
El Gamal, Abbas, and Helmy Eltoukhy. 2005. <span>“CMOS Image Sensors.”</span> <em>IEEE Circuits and Devices Magazine</em> 21 (3): 6–20.
</div>
<div id="ref-emva1288" class="csl-entry" role="listitem">
EMVA. 2021. <span>“<span class="nocase">EMVA Standard 1288 Standard for Characterization of Image Sensors and Cameras</span>.”</span> <a href="https://www.emva.org/wp-content/uploads/EMVA1288General_4.0Release.pdf" class="uri">https://www.emva.org/wp-content/uploads/EMVA1288General_4.0Release.pdf</a>.
</div>
<div id="ref-ccd_qe" class="csl-entry" role="listitem">
Eric Bajart. 2010. <span>“<span class="nocase">Quantum efficiency of the CCD sensor <span>‘PC1’</span> in the Hubble Space Telescope’s Wide Field and Planetary Camera WFPC2; CC BY-SA 3.0</span>.”</span> <a href="https://commons.wikimedia.org/wiki/File:Quantum_efficiency_graph_for_WFPC2-en.svg" class="uri">https://commons.wikimedia.org/wiki/File:Quantum_efficiency_graph_for_WFPC2-en.svg</a>.
</div>
<div id="ref-feng2024blisscam" class="csl-entry" role="listitem">
Feng, Yu, Tianrui Ma, Yuhao Zhu, and Xuan Zhang. 2024. <span>“Blisscam: Boosting Eye Tracking Efficiency with Learned in-Sensor Sparse Sampling.”</span> In <em>2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA)</em>, 1262–77. IEEE.
</div>
<div id="ref-fossum1993active" class="csl-entry" role="listitem">
Fossum, Eric R. 1993. <span>“Active Pixel Sensors: Are CCDs Dinosaurs?”</span> In <em>Charge-Coupled Devices and Solid State Optical Sensors III</em>, 1900:2–14. SPIE.
</div>
<div id="ref-fossum1997cmos" class="csl-entry" role="listitem">
———. 1997. <span>“CMOS Image Sensors: Electronic Camera-on-a-Chip.”</span> <em>IEEE Transactions on Electron Devices</em> 44 (10): 1689–98.
</div>
<div id="ref-fossum2014review" class="csl-entry" role="listitem">
Fossum, Eric R, and Donald B Hondongwa. 2014. <span>“A Review of the Pinned Photodiode for CCD and CMOS Image Sensors.”</span> <em>IEEE Journal of the Electron Devices Society</em>.
</div>
<div id="ref-fossum2024digital" class="csl-entry" role="listitem">
Fossum, Eric R, Nobukazu Teranishi, and Albert JP Theuwissen. 2024. <span>“Digital Image Sensor Evolution and New Frontiers.”</span> <em>Annual Review of Vision Science</em> 10 (1): 171–98.
</div>
<div id="ref-fowler1994cmos" class="csl-entry" role="listitem">
Fowler, Boyd, Abbas El Gamal, and David XD Yang. 1994. <span>“A CMOS Area Image Sensor with Pixel-Level a/d Conversion.”</span> In <em>Proceedings of IEEE International Solid-State Circuits Conference-ISSCC’94</em>, 226–27. IEEE.
</div>
<div id="ref-glassner1995principles" class="csl-entry" role="listitem">
Glassner, Andrew S. 1995. <em>Principles of Digital Image Synthesis</em>. Elsevier.
</div>
<div id="ref-green1995optical" class="csl-entry" role="listitem">
Green, Martin A, and Mark J Keevers. 1995. <span>“Optical Properties of Intrinsic Silicon at 300 k.”</span> <em>Progress in Photovoltaics: Research and Applications</em> 3 (3): 189–92.
</div>
<div id="ref-haruta20174" class="csl-entry" role="listitem">
Haruta, Tsutomu, Tsutomu Nakajima, Jun Hashizume, Taku Umebayashi, Hiroshi Takahashi, Kazuo Taniguchi, Masami Kuroda, et al. 2017. <span>“4.6 a 1/2.3 Inch 20Mpixel 3-Layer Stacked CMOS Image Sensor with DRAM.”</span> In <em>2017 IEEE International Solid-State Circuits Conference (ISSCC)</em>, 76–77. IEEE.
</div>
<div id="ref-hasinoff2016burst" class="csl-entry" role="listitem">
Hasinoff, Samuel W, Dillon Sharlet, Ryan Geiss, Andrew Adams, Jonathan T Barron, Florian Kainz, Jiawen Chen, and Marc Levoy. 2016. <span>“Burst Photography for High Dynamic Range and Low-Light Imaging on Mobile Cameras.”</span> <em>ACM Transactions on Graphics (ToG)</em> 35 (6): 1–12.
</div>
<div id="ref-hirata20217" class="csl-entry" role="listitem">
Hirata, Tomoki, Hironobu Murata, Hideaki Matsuda, Yojiro Tezuka, and Shiro Tsunai. 2021. <span>“7.8 a 1-Inch 17Mpixel 1000fps Block-Controlled Coded-Exposure Back-Illuminated Stacked CMOS Image Sensor for Computational Imaging and Adaptive Dynamic Range Control.”</span> In <em>2021 IEEE International Solid-State Circuits Conference (ISSCC)</em>, 64:120–22. IEEE.
</div>
<div id="ref-hsu202005" class="csl-entry" role="listitem">
Hsu, Tzu-Hsiang, Yi-Ren Chen, Ren-Shuo Liu, Chung-Chuan Lo, Kea-Tiong Tang, Meng-Fan Chang, and Chih-Cheng Hsieh. 2020. <span>“A 0.5-v Real-Time Computational CMOS Image Sensor with Programmable Kernel for Feature Extraction.”</span> <em>IEEE Journal of Solid-State Circuits</em> 56 (5): 1588–96.
</div>
<div id="ref-hu2009modern" class="csl-entry" role="listitem">
Hu, Chenming. 2009. <em>Modern Semiconductor Devices for Integrated Circuits</em>. Prentice Hall.
</div>
<div id="ref-huggett2009dual" class="csl-entry" role="listitem">
Huggett, Anthony, Chris Silsby, Sergi Cami, and Jeff Beck. 2009. <span>“A Dual-Conversion-Gain Video Sensor with Dewarping and Overlay on a Single Chip.”</span> In <em>2009 IEEE International Solid-State Circuits Conference-Digest of Technical Papers</em>, 52–53. IEEE.
</div>
<div id="ref-iida20180" class="csl-entry" role="listitem">
Iida, S, Y Sakano, T Asatsuma, M Takami, I Yoshiba, N Ohba, H Mizuno, et al. 2018. <span>“A 0.68 e-Rms Random-Noise 121dB Dynamic-Range Sub-Pixel Architecture CMOS Image Sensor with LED Flicker Mitigation.”</span> In <em>2018 IEEE International Electron Devices Meeting (IEDM)</em>, 10–12. IEEE.
</div>
<div id="ref-ikeno20224" class="csl-entry" role="listitem">
Ikeno, Rimon, Kazuya Mori, Masayuki Uno, Ken Miyauchi, Toshiyuki Isozaki, Isao Takayanagi, Junichi Nakamura, et al. 2022. <span>“A 4.6-<span class="math inline">\(\mu\)</span>m, 127-dB Dynamic Range, Ultra-Low Power Stacked Digital Pixel Sensor with Overlapped Triple Quantization.”</span> <em>IEEE Transactions on Electron Devices</em> 69 (6): 2943–50.
</div>
<div id="ref-irds" class="csl-entry" role="listitem">
IRDS. 2024. <span>“International Roadmap for Devices and Systems.”</span> <a href="https://irds.ieee.org/" class="uri">https://irds.ieee.org/</a>.
</div>
<div id="ref-kim2005200" class="csl-entry" role="listitem">
Kim, Seong-Jin, Kwang-Hyun Lee, Sang-Wook Han, and Euisik Yoon. 2005. <span>“A 200/Spl Times/160 Pixel CMOS Fingerprint Recognition SoC with Adaptable Column-Parallel Processors.”</span> In <em>ISSCC. 2005 IEEE International Digest of Technical Papers. Solid-State Circuits Conference, 2005.</em>, 250–596. IEEE.
</div>
<div id="ref-kobayashi20171" class="csl-entry" role="listitem">
Kobayashi, Masahiro, Yusuke Onuki, Kazunari Kawabata, Hiroshi Sekine, Toshiki Tsuboi, Takashi Muto, Takeshi Akiyama, et al. 2017. <span>“4.5A 1.8e-Rms Temporal Noise over 110 dB Dynamic Range <span class="math inline">\(3.4\mu\mathrm {m}\)</span> Pixel Pitch Global-Shutter CMOS Image Sensor with Dual-Gain Amplifiers SS-ADC, Light Guide Structure, and Multiple-Accumulation Shutter.”</span> <em>IEEE Journal of Solid-State Circuits</em> 53 (1): 219–28.
</div>
<div id="ref-kondo20153d" class="csl-entry" role="listitem">
Kondo, Toru, Yoshiaki Takemoto, Kenji Kobayashi, Mitsuhiro Tsukimura, Naohiro Takazawa, Hideki Kato, Shunsuke Suzuki, et al. 2015. <span>“A 3D Stacked CMOS Image Sensor with 16Mpixel Global-Shutter Mode and 2Mpixel 10000fps Mode Using 4 Million Interconnections.”</span> In <em>2015 Symposium on VLSI Circuits (VLSI Circuits)</em>, C90–91. IEEE.
</div>
<div id="ref-kozlowski1998comparison" class="csl-entry" role="listitem">
Kozlowski, Lester J, J Luo, WE Kleinhans, and T Liu. 1998. <span>“Comparison of Passive and Active Pixel Schemes for CMOS Visible Imagers.”</span> In <em>Infrared Readout Electronics IV</em>, 3360:101–10. SPIE.
</div>
<div id="ref-kumagai20181" class="csl-entry" role="listitem">
Kumagai, Oichi, Atsumi Niwa, Katsuhiko Hanzawa, Hidetaka Kato, Shinichiro Futami, Toshio Ohyama, Tsutomu Imoto, et al. 2018. <span>“A 1/4-Inch 3.9 Mpixel Low-Power Event-Driven Back-Illuminated Stacked CMOS Image Sensor.”</span> In <em>2018 IEEE International Solid-State Circuits Conference-(ISSCC)</em>, 86–88. IEEE.
</div>
<div id="ref-kumagai2018back" class="csl-entry" role="listitem">
Kumagai, Y, R Yoshita, N Osawa, H Ikeda, K Yamashita, T Abe, S Kudo, et al. 2018. <span>“Back-Illuminated <span class="math inline">\(2.74\mu\mathrm {m}\)</span>-Pixel-Pitch Global Shutter CMOS Image Sensor with Charge-Domain Memory Achieving 10k e-Saturation Signal.”</span> In <em>2018 IEEE International Electron Devices Meeting (IEDM)</em>, 10–16. IEEE.
</div>
<div id="ref-kwon2020low" class="csl-entry" role="listitem">
Kwon, Minho, Seunghyun Lim, Hyeokjong Lee, Il-Seon Ha, Moo-Young Kim, Il-Jin Seo, Suho Lee, et al. 2020. <span>“A Low-Power 65/14nm Stacked CMOS Image Sensor.”</span> In <em>2020 IEEE International Symposium on Circuits and Systems (ISCAS)</em>, 1–4. IEEE.
</div>
<div id="ref-liu20204" class="csl-entry" role="listitem">
Liu, Chiao, Lyle Bainbridge, Andrew Berkovich, Song Chen, Wei Gao, Tsung-Hsun Tsai, Kazuya Mori, et al. 2020. <span>“A 4.6 <span class="math inline">\(\mu\)</span>m, 512<span class="math inline">\(\times\)</span> 512, Ultra-Low Power Stacked Digital Pixel Sensor with Triple Quantization and 127dB Dynamic Range.”</span> In <em>2020 IEEE International Electron Devices Meeting (IEDM)</em>, 16–11. IEEE.
</div>
<div id="ref-liu2019intelligent" class="csl-entry" role="listitem">
Liu, Chiao, Andrew Berkovich, Song Chen, Hans Reyserhove, Syed Shakib Sarwar, and Tsung-Hsun Tsai. 2019. <span>“Intelligent Vision Systems–Bringing Human-Machine Interface to AR/VR.”</span> In <em>2019 IEEE International Electron Devices Meeting (IEDM)</em>, 10–15. IEEE.
</div>
<div id="ref-liu2022augmented" class="csl-entry" role="listitem">
Liu, Chiao, Song Chen, Tsung-Hsun Tsai, Barbara De Salvo, and Jorge Gomez. 2022. <span>“Augmented Reality-the Next Frontier of Image Sensors and Compute Systems.”</span> In <em>2022 IEEE International Solid-State Circuits Conference (ISSCC)</em>, 65:426–28. IEEE.
</div>
<div id="ref-ma2024efficient" class="csl-entry" role="listitem">
Ma, Tianrui. 2024. <span>“Efficient Data-Driven Machine Vision: A Co-Design of Circuit, Algorithm, and Architecture for Edge Vision Sensors.”</span> PhD thesis, Washington University in St. Louis.
</div>
<div id="ref-ma2023camj" class="csl-entry" role="listitem">
Ma, Tianrui, Yu Feng, Xuan Zhang, and Yuhao Zhu. 2023. <span>“Camj: Enabling System-Level Energy Modeling and Architectural Exploration for in-Sensor Visual Computing.”</span> In <em>Proceedings of the 50th Annual International Symposium on Computer Architecture</em>, 1–14.
</div>
<div id="ref-cut_off_filter" class="csl-entry" role="listitem">
Melentijevic. 2015. <span>“<span>DSLR Internal Cut Filter / Lowpass Filter / Hot Mirror Transmission Curves</span>.”</span> <a href="https://kolarivision.com/articles/internal-cut-filter-transmission/" class="uri">https://kolarivision.com/articles/internal-cut-filter-transmission/</a>.
</div>
<div id="ref-miyauchi2020stacked" class="csl-entry" role="listitem">
Miyauchi, Ken, Kazuya Mori, Toshinori Otaka, Toshiyuki Isozaki, Naoto Yasuda, Alex Tsai, Yusuke Sawai, Hideki Owada, Isao Takayanagi, and Junichi Nakamura. 2020. <span>“A Stacked Back Side-Illuminated Voltage Domain Global Shutter CMOS Image Sensor with a 4.0 <span class="math inline">\(\mu\)</span>m Multiple Gain Readout Pixel.”</span> <em>Sensors</em> 20 (2): 486.
</div>
<div id="ref-murakami20224" class="csl-entry" role="listitem">
Murakami, Hirotaka, Eric Bohannon, John Childs, Grace Gui, Eric Moule, Katsuhiko Hanzawa, Tomofumi Koda, et al. 2022. <span>“A 4.9 Mpixel Programmable-Resolution Multi-Purpose CMOS Image Sensor for Computer Vision.”</span> In <em>2022 IEEE International Solid-State Circuits Conference (ISSCC)</em>, 65:104–6. IEEE.
</div>
<div id="ref-adc_survey" class="csl-entry" role="listitem">
Murmann, Boris. 2014. <span>“<span>ADC Performance Survey 1997-2024</span>.”</span> <a href="https://github.com/bmurmann/ADC-survey" class="uri">https://github.com/bmurmann/ADC-survey</a>.
</div>
<div id="ref-nakamura2006image" class="csl-entry" role="listitem">
Nakamura, Junichi. 2006. <em>Image Sensors and Signal Processing for Digital Still Cameras</em>. CRC press.
</div>
<div id="ref-nitta2006high" class="csl-entry" role="listitem">
Nitta, Yoshikazu, Yoshinori Muramatsu, Kiyotaka Amano, Takayuki Toyama, K Mishina, Atsushi Suzuki, Tadayuki Taura, et al. 2006. <span>“High-Speed Digital Double Sampling with Analog CDS on Column Parallel ADC Architecture for Low-Noise Active Pixel Sensor.”</span> In <em>2006 IEEE International Solid State Circuits Conference-Digest of Technical Papers</em>, 2024–31. IEEE.
</div>
<div id="ref-noble1968self" class="csl-entry" role="listitem">
Noble, Peter JW. 1968. <span>“Self-Scanned Silicon Image Detector Arrays.”</span> <em>IEEE Transactions on Electron Devices</em> 15 (4): 202–9.
</div>
<div id="ref-ohta2020smart" class="csl-entry" role="listitem">
Ohta, Jun. 2020. <em>Smart CMOS Image Sensors and Applications</em>. CRC press.
</div>
<div id="ref-focal_plane_shutter" class="csl-entry" role="listitem">
Ommnomnomgulp. 2008. <span>“<span class="nocase">A focal plane shutter firing at 1/500 of a second with the <span>‘gap’</span> clearly visible. This shutter is on a Nikon film SLR. CC BY-SA 3.0 license</span>.”</span> <a href="https://commons.wikimedia.org/wiki/File:1_500_Sec_Focal_P_Shut.jpg" class="uri">https://commons.wikimedia.org/wiki/File:1_500_Sec_Focal_P_Shut.jpg</a>.
</div>
<div id="ref-pharr2023physically" class="csl-entry" role="listitem">
Pharr, Matt, Wenzel Jakob, and Greg Humphreys. 2023. <em>Physically Based Rendering: From Theory to Implementation</em>. 4th ed. MIT Press.
</div>
<div id="ref-gorskycollection" class="csl-entry" role="listitem">
Prokudin-Gorsky, Sergey. 1948. <span>“<span class="nocase">Library of Congress Prokudin-Gorskii Collection</span>.”</span> <a href="https://www.loc.gov/collections/prokudin-gorskii/about-this-collection/" class="uri">https://www.loc.gov/collections/prokudin-gorskii/about-this-collection/</a>.
</div>
<div id="ref-sakakibara201283db" class="csl-entry" role="listitem">
Sakakibara, Masaki, Yusuke Oike, Takafumi Takatsuka, Akihiko Kato, Katsumi Honda, Tadayuki Taura, Takashi Machida, et al. 2012. <span>“An 83dB-Dynamic-Range Single-Exposure Global-Shutter CMOS Image Sensor with in-Pixel Dual Storage.”</span> In <em>2012 IEEE International Solid-State Circuits Conference</em>, 380–82. IEEE.
</div>
<div id="ref-sharpe2005luminous" class="csl-entry" role="listitem">
Sharpe, Lindsay T, Andrew Stockman, Wolfgang Jagla, and Herbert Jägle. 2005. <span>“A Luminous Efficiency Function, v*(<span class="math inline">\(\lambda\)</span>), for Daylight Adaptation.”</span> <em>Journal of Vision</em> 5 (11): 3–3.
</div>
<div id="ref-sharpe2011luminous" class="csl-entry" role="listitem">
———. 2011. <span>“A Luminous Efficiency Function, VD65*(<span class="math inline">\(\lambda\)</span>), for Daylight Adaptation: A Correction.”</span> <em>Color Research &amp; Application</em> 36 (1): 42–46.
</div>
<div id="ref-solhusvik20191280" class="csl-entry" role="listitem">
Solhusvik, Johannes, T Willassent, Sindre Mikkelsen, Mathias Wilhelmsen, Sohei Manabe, Duli Mao, Zhaoyu He, Keiji Mabuchi, and TA Hasegawa. 2019. <span>“1280<span class="math inline">\(\times\)</span> 960 2.8 <span class="math inline">\(\mu\)</span>m HDR CIS with DCG and Split-Pixel Combined.”</span> In <em>Proceedings of the International Image Sensor Workshop (IISW), Snowbird, UT, USA</em>, 23–27.
</div>
<div id="ref-stark2018back" class="csl-entry" role="listitem">
Stark, Laurence, Jeffrey M Raynor, Frederic Lalanne, and Robert K Henderson. 2018. <span>“A Back-Illuminated Voltage-Domain Global Shutter Pixel with Dual in-Pixel Storage.”</span> <em>IEEE Transactions on Electron Devices</em> 65 (10): 4394–4400.
</div>
<div id="ref-stoppa2002novel" class="csl-entry" role="listitem">
Stoppa, David, Andrea Simoni, Lorenzo Gonzo, Massimo Gottardi, and G-F Dalla Betta. 2002. <span>“Novel CMOS Image Sensor with a 132-dB Dynamic Range.”</span> <em>IEEE Journal of Solid-State Circuits</em> 37 (12): 1846–52.
</div>
<div id="ref-sugawa2005100" class="csl-entry" role="listitem">
Sugawa, Shigetoshi, Nana Akahane, Satoru Adachi, Kazuya Mori, Toshiyuki Ishiuchi, and Koichi Mizobuchi. 2005. <span>“A 100 dB Dynamic Range CMOS Image Sensor Using a Lateral Overflow Integration Capacitor.”</span> In <em>ISSCC. 2005 IEEE International Digest of Technical Papers. Solid-State Circuits Conference, 2005.</em>, 352–603. IEEE.
</div>
<div id="ref-swain2008back" class="csl-entry" role="listitem">
Swain, PK, and David Cheskis. 2008. <span>“Back-Illuminated Image Sensors Come to the Forefront.”</span> <em>Photonics Spectra</em> 42 (8): 46.
</div>
<div id="ref-takayanagi2019120" class="csl-entry" role="listitem">
Takayanagi, Isao, Ken Miyauchi, Shunsuke Okura, Kazuya Mori, Junichi Nakamura, and Shigetoshi Sugawa. 2019. <span>“A 120-Ke- Full-Well Capacity 160-<span class="math inline">\(\mu\)</span>v/e- Conversion Gain 2.8-<span class="math inline">\(\mu\)</span>m Backside-Illuminated Pixel with a Lateral Overflow Integration Capacitor.”</span> <em>Sensors</em> 19 (24): 5572.
</div>
<div id="ref-takayanagi2018over" class="csl-entry" role="listitem">
Takayanagi, Isao, Norio Yoshimura, Kazuya Mori, Shinichiro Matsuo, Shunsuke Tanaka, Hirofumi Abe, Naoto Yasuda, et al. 2018. <span>“An over 90 dB Intra-Scene Single-Exposure Dynamic Range CMOS Image Sensor Using a 3.0 <span class="math inline">\(\mu\)</span>m Triple-Gain Pixel Fabricated in a Standard BSI Process.”</span> <em>Sensors</em> 18 (1): 203.
</div>
<div id="ref-teranishi2015effect" class="csl-entry" role="listitem">
Teranishi, Nobukazu. 2015. <span>“Effect and Limitation of Pinned Photodiode.”</span> <em>IEEE Transactions on Electron Devices</em> 63 (1): 10–15.
</div>
<div id="ref-teranishi1982no" class="csl-entry" role="listitem">
Teranishi, Nobukazu, Akiyoshi Kohono, Yasuo Ishihara, Eiji Oda, and Kouichi Arai. 1982. <span>“No Image Lag Photodiode Structure in the Interline CCD Image Sensor.”</span> In <em>1982 International Electron Devices Meeting</em>, 324–27. IEEE.
</div>
<div id="ref-spd_incan" class="csl-entry" role="listitem">
Thorseth. 2015. <span>“<span class="nocase">Spectral power distribution of a 25 W incandescent light bulb; CC BY-SA 4.0 license</span>.”</span> <a href="https://commons.wikimedia.org/wiki/File:Spectral_power_distribution_of_a_25_W_incandescent_light_bulb.png" class="uri">https://commons.wikimedia.org/wiki/File:Spectral_power_distribution_of_a_25_W_incandescent_light_bulb.png</a>.
</div>
<div id="ref-tournier2018hdr" class="csl-entry" role="listitem">
Tournier, Arnaud, F Roy, Y Cazaux, F Lalanne, P Malinge, M Mcdonald, G Monnot, and N Roux. 2018. <span>“A HDR 98dB <span class="math inline">\(3.2\mu\mathrm {m}\)</span> Charge Domain Global Shutter CMOS Image Sensor.”</span> In <em>2018 IEEE International Electron Devices Meeting (IEDM)</em>, 10–14. IEEE.
</div>
<div id="ref-tsugawa2017pixel" class="csl-entry" role="listitem">
Tsugawa, H, H Takahashi, R Nakamura, T Umebayashi, T Ogita, H Okano, K Iwase, et al. 2017. <span>“Pixel/DRAM/Logic 3-Layer Stacked CMOS Image Sensor Technology.”</span> In <em>2017 IEEE International Electron Devices Meeting (IEDM)</em>, 3–2. IEEE.
</div>
<div id="ref-weckler1967operation" class="csl-entry" role="listitem">
Weckler, Gene P. 1967. <span>“Operation of Pn Junction Photodetectors in a Photon Flux Integrating Mode.”</span> <em>IEEE Journal of Solid-State Circuits</em> 2 (3): 65–73.
</div>
<div id="ref-willassen20151280" class="csl-entry" role="listitem">
Willassen, Trygve, Johannes Solhusvik, Robert Johansson, Sohrab Yaghmai, Howard Rhodes, Sohei Manabe, Duli Mao, et al. 2015. <span>“A 1280<span class="math inline">\(\times\)</span> 1080 4.2 <span class="math inline">\(\mu\)</span>m Split-Diode Pixel Hdr Sensor in 110 Nm Bsi Cmos Process.”</span> In <em>Proceedings of the International Image Sensor Workshop, Vaals, the Netherlands</em>, 8–11.
</div>
<div id="ref-xu2021senputing" class="csl-entry" role="listitem">
Xu, Han, Ningchao Lin, Li Luo, Qi Wei, Runsheng Wang, Cheng Zhuo, Xunzhao Yin, Fei Qiao, and Huazhong Yang. 2021. <span>“Senputing: An Ultra-Low-Power Always-on Vision Perception Chip Featuring the Deep Fusion of Sensing and Computing.”</span> <em>IEEE Transactions on Circuits and Systems I: Regular Papers</em> 69 (1): 232–43.
</div>
<div id="ref-xu2022analysis" class="csl-entry" role="listitem">
Xu, Jiangtao, Liuqin Shu, Zhiyuan Gao, Quanmin Chen, and Kaiming Nie. 2022. <span>“Analysis and Parameter Optimization of High Dynamic Range Pixels for Split Photodiode in CMOS Image Sensors.”</span> <em>IEEE Sensors Journal</em> 22 (7): 6748–54.
</div>
<div id="ref-yasutomi2011two" class="csl-entry" role="listitem">
Yasutomi, Keita, Shinya Itoh, and Shoji Kawahito. 2011. <span>“A Two-Stage Charge Transfer Active Pixel CMOS Image Sensor with Low-Noise Global Shuttering and a Dual-Shuttering Mode.”</span> <em>IEEE Transactions on Electron Devices</em> 58 (3): 740–47.
</div>
<div id="ref-yokoyama2018high" class="csl-entry" role="listitem">
Yokoyama, Toshifumi, Masafumi Tsutsui, Yoshiaki Nishi, Ikuo Mizuno, Veinger Dmitry, and Assaf Lahav. 2018. <span>“High Performance <span class="math inline">\(2.5\mu\mathrm {m}\)</span> Global Shutter Pixel with New Designed Light-Pipe Structure.”</span> In <em>2018 IEEE International Electron Devices Meeting (IEDM)</em>, 10–15. IEEE.
</div>
<div id="ref-young2019data" class="csl-entry" role="listitem">
Young, Christopher, Alex Omid-Zohoor, Pedram Lajevardi, and Boris Murmann. 2019. <span>“A Data-Compressive 1.5/2.75-Bit Log-Gradient QVGA Image Sensor with Multi-Scale Readout for Always-on Object Detection.”</span> <em>IEEE Journal of Solid-State Circuits</em> 54 (11): 2932–46.
</div>
<div id="ref-zhu2022cam" class="csl-entry" role="listitem">
Zhu, Yuhao. 2022. <span>“<span class="nocase">Exploring Camera Color Space and Color Correction</span>.”</span> <a href="https://horizon-lab.org/colorvis/camcolor.html" class="uri">https://horizon-lab.org/colorvis/camcolor.html</a>.
</div>
</div>
</section>
</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>For the charges collected in PD to be transferable to the FD, the photodiode needs to be “pinned”, which means there is another layer of p+ implant above the p-n junction pinned to the ground (0 V). Such a PD is also called the Pinned Photodiode, or PPD <span class="citation" data-cites="teranishi1982no teranishi2015effect fossum2014review">(<a href="references.html#ref-teranishi1982no" role="doc-biblioref">Teranishi et al. 1982</a>; <a href="references.html#ref-teranishi2015effect" role="doc-biblioref">Teranishi 2015</a>; <a href="references.html#ref-fossum2014review" role="doc-biblioref">Fossum and Hondongwa 2014</a>)</span>.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p><span class="math inline">\(V_1\)</span> and <span class="math inline">\(V_{rst}\)</span> technically are ever so slightly different because the charges might be leaking between resetting and read out.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>For instance in <span class="citation" data-cites="solhusvik20191280">Solhusvik et al. (<a href="references.html#ref-solhusvik20191280" role="doc-biblioref">2019</a>)</span>, the sensitivity ratio between the LPD and SPD is over 100<span class="math inline">\(\times\)</span>, but the FWC of the SPD is less than three times smaller than that of the LPD.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>They shared the Nobel Prize in Physics in 2009.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>It is worth noting, however, that it is difficult for the CCD sensor to perform CDS because of its read-out architecture (shifting charges to a single SF amplifier).<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>It is interesting to note the fact that there is a fundamental pixel size limit negates one advantage of the CCD sensors, where the pixel design is simpler so one can theoretically make the pixel size smaller, but that is countered by the limit to which the PDs can shrink <span class="citation" data-cites="fossum1997cmos">(<a href="references.html#ref-fossum1997cmos" role="doc-biblioref">Fossum 1997</a>)</span>.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>Don’t be confused by the two similar notations that represent different quantities: <span class="math inline">\(N\)</span> for the number of charges at a pixel and <span class="math inline">\(Q\)</span> for the energy at a pixel.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>If we want to be pedantic, each green pixel has a small, but non-infinitesimal, area, so it first performs a low-pass filtering using a box filter whose extent is the pixel area, followed by sampling at the center of the pixel.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./imaging-optics.html" class="pagination-link" aria-label="Imaging Optics">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Imaging Optics</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./imaging-noise.html" class="pagination-link" aria-label="Noise">
        <span class="nav-page-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Noise</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/learnvisualcomputing/learnvisualcomputing.github.io/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></div></div></footer></body></html>