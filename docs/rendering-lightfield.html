<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>9&nbsp; Light Field – Foundations of Visual Computing</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./rendering-re.html" rel="next">
<link href="./rendering-radiometry.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-485d01fc63b59abcd3ee1bf1e8e2748d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://hypothes.is/embed.js"></script>
<script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./rendering.html">Rendering</a></li><li class="breadcrumb-item"><a href="./rendering-lightfield.html"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Light Field</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Foundations of Visual Computing</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/learnvisualcomputing/learnvisualcomputing.github.io" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./Foundations-of-Visual-Computing.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.linkedin.com/sharing/share-offsite/?url=|url|">
              <i class="bi bi-linkedin pe-1"></i>
            LinkedIn
            </a>
          </li>
      </ul>
    </div>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">An Invitation to Visual Computing</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./hvs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Human Visual System</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./hvs-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">From Light to Vision</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./hvs-receptor.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Photoreceptors</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./hvs-color.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Color Vision</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./hvs-colorimetry.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Colorimetry</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./hvs-adaptation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Visual Adaptations and Constancy</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./rendering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Rendering</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./rendering-overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Overview</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./rendering-radiometry.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Radiometry and Photometry</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./rendering-lightfield.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Light Field</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./rendering-re.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Rendering Surface Scattering</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./rendering-surface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Modeling Material Surface</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./rendering-sss.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Volume and Subsurface Scattering Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./rendering-rte.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Rendering Volume and Subsurface Scattering</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./rendering-nflux.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">The N-Flux Theory</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./imaging.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Imaging</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./imaging-optics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Imaging Optics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./imaging-sensor.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Image Sensor Architecture</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./imaging-noise.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Noise</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./imaging-isp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Camera Signal Processing</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./display.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Display</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./display-optics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Optical Mechanisms</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./display-electronics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Driving Circuits</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./display-signal-processing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Display Signal Processing</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-chpt-mat-basics-radiometry-cam" id="toc-sec-chpt-mat-basics-radiometry-cam" class="nav-link active" data-scroll-target="#sec-chpt-mat-basics-radiometry-cam"><span class="header-section-number">9.1</span> The Measurement Equation</a></li>
  <li><a href="#sec-chpt-mat-basics-radiometry-lf" id="toc-sec-chpt-mat-basics-radiometry-lf" class="nav-link" data-scroll-target="#sec-chpt-mat-basics-radiometry-lf"><span class="header-section-number">9.2</span> Light Field</a>
  <ul class="collapse">
  <li><a href="#sec-chpt-mat-basics-radiometry-lf-im" id="toc-sec-chpt-mat-basics-radiometry-lf-im" class="nav-link" data-scroll-target="#sec-chpt-mat-basics-radiometry-lf-im"><span class="header-section-number">9.2.1</span> Light-Field Imaging</a></li>
  <li><a href="#sec-chpt-mat-basics-radiometry-lf-ren" id="toc-sec-chpt-mat-basics-radiometry-lf-ren" class="nav-link" data-scroll-target="#sec-chpt-mat-basics-radiometry-lf-ren"><span class="header-section-number">9.2.2</span> Light-Field Rendering</a></li>
  <li><a href="#radiance-field-rendering" id="toc-radiance-field-rendering" class="nav-link" data-scroll-target="#radiance-field-rendering"><span class="header-section-number">9.2.3</span> Radiance-Field Rendering</a></li>
  <li><a href="#light-field-display" id="toc-light-field-display" class="nav-link" data-scroll-target="#light-field-display"><span class="header-section-number">9.2.4</span> Light-Field Display</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/learnvisualcomputing/learnvisualcomputing.github.io/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./rendering.html">Rendering</a></li><li class="breadcrumb-item"><a href="./rendering-lightfield.html"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Light Field</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-chpt-mat-basics-lf" class="quarto-section-identifier"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Light Field</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Given that we have the basic understanding of radiometry, now seems like a good time to show how radiometry is of fundamental importance to computer graphics and imaging. We do so by building two important concepts using radiometry: the camera measurement equation (<a href="#sec-chpt-mat-basics-radiometry-cam" class="quarto-xref"><span>Section 9.1</span></a>) and the light field (<a href="#sec-chpt-mat-basics-radiometry-lf" class="quarto-xref"><span>Section 9.2</span></a>).</p>
<section id="sec-chpt-mat-basics-radiometry-cam" class="level2" data-number="9.1">
<h2 data-number="9.1" class="anchored" data-anchor-id="sec-chpt-mat-basics-radiometry-cam"><span class="header-section-number">9.1</span> The Measurement Equation</h2>
<p>For simplicity, let’s just consider one single pixel with a setup illustrated in <a href="#fig-cam_measurement_setup" class="quarto-xref">Figure&nbsp;<span>9.1</span></a>.</p>
<div id="fig-cam_measurement_setup" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cam_measurement_setup-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figs/cam_measurement_setup.svg" class="img-fluid figure-img" style="width:30.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cam_measurement_setup-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.1: Geometric setting for the camera measurement equation. Calculating the pixel value requires integrating the energy of all the rays hitting the pixel area, which requires knowing the light field inside the camera, which, in turn, requires knowing the light field in the scene and how the camera optics transfer the external light field to the internal light field.
</figcaption>
</figure>
</div>
<p>Each pixel is very small, but it has a finite area, say <span class="math inline">\(A_p\)</span>. Each pixel is constantly being bombarded by lights that enter the aperture, which has an area <span class="math inline">\(V\)</span>. The raw pixel value is roughly proportional to the energy it receives during the exposure time<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. So using the basic radiometry, we can write the total energy received by a pixel during the exposure time <span class="math inline">\(T\)</span> as:</p>
<p><span id="eq-cam"><span class="math display">\[
    Q = \int^{T} \int^{A_p} \int^{\Omega(p, V)} L(p, \omega) \cos\theta~\text{d}\omega~\text{d}p~\text{d}t,
\tag{9.1}\]</span></span></p>
<p>where <span class="math inline">\(\Omega(p, V)\)</span> explicitly expresses that a solid angle is determined by the aperture <span class="math inline">\(V\)</span> and a point <span class="math inline">\(p\)</span> on the pixel surface. Of course this quantity changes with <span class="math inline">\(p\)</span>. We sometimes omit <span class="math inline">\(p\)</span> and <span class="math inline">\(V\)</span> when it is clear what <span class="math inline">\(\Omega\)</span> refers to, but here, since the solid angle changes with the dummy variable <span class="math inline">\(p\)</span> in the integral equation, we express it explicitly. <!-- %similar to Equ 6.65b in CIFA. --> In graphics literature, this equation is sometimes called the <strong>measurement equation</strong> of an image sensor <span class="citation" data-cites="kolb1995realistic reinhard2008color pharr2023physically">(<a href="references.html#ref-kolb1995realistic" role="doc-biblioref">Kolb, Mitchell, and Hanrahan 1995</a>; <a href="references.html#ref-reinhard2008color" role="doc-biblioref">Reinhard et al. 2008, chap. 6.8.1</a>; <a href="references.html#ref-pharr2023physically" role="doc-biblioref">Pharr, Jakob, and Humphreys 2023, chap. 5.4</a>)</span>.</p>
<p>The inner integral in <a href="#eq-cam" class="quarto-xref">Equation&nbsp;<span>9.1</span></a> is expressed over the solid angle, which varies with <span class="math inline">\(p\)</span>. A more common, but equivalent, formulation of the measurement equation is to re-express the inner integral over the aperture area <span class="math inline">\(V\)</span>:</p>
<p><span id="eq-cam_area"><span class="math display">\[
    Q = \frac{1}{d^2} \int^{T} \int^{A_p} \int^{V} L(p, \omega) |\cos^4\theta|~\text{d}p'~\text{d}p~\text{d}t,
\tag{9.2}\]</span></span></p>
<p>where <span class="math inline">\(d\)</span> is the distance between the aperture plane and the sensor plane, and <span class="math inline">\(p'\)</span> is a point on the aperture plane. The derivation is available in standard texts <span class="citation" data-cites="pharr2023physically">(<a href="references.html#ref-pharr2023physically" role="doc-biblioref">Pharr, Jakob, and Humphreys 2023, chap. 5.4.1</a>)</span> and is omitted here.</p>
<p>The measurement equation is concerned with the radiance distribution inside a camera, but the only reason there is a radiance distribution inside the camera is because there is an external radiance distribution in the scene impinging upon the camera optics, which act as a transfer function that turns external radiance into internal radiance. The transfer function is determined by the material properties of the camera optics (e.g., lenses, filters, etc.), whose effects are nothing more than surface scattering and volume scattering, topics of the next two chapters.</p>
<p>Using <a href="#fig-cam_measurement_setup" class="quarto-xref">Figure&nbsp;<span>9.1</span></a> as a concrete example, to know the radiance <span class="math inline">\(L(p, \omega)\)</span> inside the camera, we need to know <span class="math inline">\(L(p', \omega')\)</span>, the corresponding radiance in the scene and how the latter is transferred to the former. If the camera is an ideal pinhole, we have <span class="math inline">\(\omega = \omega'\)</span> and <span class="math inline">\(L(p, \omega) = L(p', \omega')\)</span> (ignoring diffraction). If the camera uses an ideal convex lens, the relationship between the two rays is governed by the Gauss lens equation (<a href="imaging-optics.html#sec-chpt-imaging-optics-lens-gauss" class="quarto-xref"><span>Section 15.3.1</span></a>) and, with some simplifications, <span class="math inline">\(L(p, \omega) = L(p', \omega')\)</span> still holds (<a href="imaging-optics.html#sec-chpt-imaging-optics-lens-rad" class="quarto-xref"><span>Section 15.3.5</span></a>). The transfer function is more complicated when as the camera optics become more complicated. Imaging we replace the lenses with a duck tape — how would the radiance be transferred?</p>
<p>The measurement equation is important because it fundamentally allows us to, in theory, synthesize/render any image taken by any camera at any viewpoint — given that we know the radiance distribution of the scene. Using <a href="#fig-lf_setup" class="quarto-xref">Figure&nbsp;<span>9.2</span></a> as an example, let us simulate a new camera where the sensor is moved closer to the lens. To calculate the pixel value <span class="math inline">\(p_c\)</span> of this new camera imaging the scene, it requires nothing more than invoking the measurement equation <a href="#eq-cam" class="quarto-xref">Equation&nbsp;<span>9.1</span></a> at <span class="math inline">\(p_c\)</span>, integrating over all the incident rays, which is a portion of the overall radiance distribution. This is why having access to the underlying radiance field allows us to synthesize new images.</p>
<div id="fig-lf_setup" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lf_setup-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figs/lf_setup.svg" class="img-fluid figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lf_setup-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.2: The light field is described by the plenoptic function, which describes the radiance of any ray, i.e., the energy at any position, along any ray direction, at any wavelength, and at any time. In free space, the plenoptic function is invariant to traversal along the ray propagation direction (<span class="math inline">\(P_1\)</span> and <span class="math inline">\(P_2\)</span> share the same radiance but not with <span class="math inline">\(P_3\)</span>). Having access to the entire light field allows us to synthesize any image taken by any camera (e.g., moving the sensor closer to the lens). A lens-based camera, however, is a poor device to capture the light field, since each pixel necessarily integrates many rays.
</figcaption>
</figure>
</div>
<p>Critically, observe that two of the rays that <span class="math inline">\(p_c\)</span> needs are already captured by <span class="math inline">\(p_a\)</span> and <span class="math inline">\(p_b\)</span> in the current camera. So it is only natural to ask: can we synthesize new images from images taken from the same scene? How do we systematically reason about this? Read on.</p>
</section>
<section id="sec-chpt-mat-basics-radiometry-lf" class="level2" data-number="9.2">
<h2 data-number="9.2" class="anchored" data-anchor-id="sec-chpt-mat-basics-radiometry-lf"><span class="header-section-number">9.2</span> Light Field</h2>
<p>There is a name for the distribution of the radiance in the space — it is called the <strong>light field</strong>, which refers to the complete set of all the possible radiances flowing through every possible direction. The light field is thus a function <span class="math inline">\(L(p, \omega, \lambda, t)\)</span>, describing the energy of a ray passing the position <span class="math inline">\(p\)</span>, along the direction <span class="math inline">\(\omega\)</span>, at time <span class="math inline">\(t\)</span> and wavelength <span class="math inline">\(\lambda\)</span>. This function is also called the <strong>plenoptic function</strong> <span class="citation" data-cites="bergen1991plenoptic gortler1996lumigraph levoy1996light">(<a href="references.html#ref-bergen1991plenoptic" role="doc-biblioref">Bergen and Adelson 1991</a>; <a href="references.html#ref-gortler1996lumigraph" role="doc-biblioref">Gortler et al. 1996</a>; <a href="references.html#ref-levoy1996light" role="doc-biblioref">Levoy and Hanrahan 1996</a>)</span>. <a href="#fig-lf_setup" class="quarto-xref">Figure&nbsp;<span>9.2</span></a> shows a tiny portion of the light field — six rays in fact; three inside the camera and three outside the camera.</p>
<section id="sec-chpt-mat-basics-radiometry-lf-im" class="level3" data-number="9.2.1">
<h3 data-number="9.2.1" class="anchored" data-anchor-id="sec-chpt-mat-basics-radiometry-lf-im"><span class="header-section-number">9.2.1</span> Light-Field Imaging</h3>
<p>The field of <strong>light-field imaging</strong> is concerned with measuring the light field of a scene, which is a task impossible — we cannot possibly measure the radiance of every single ray. There are some simplifications we can make. For instance, we can assume that a ray’s energy does not change in free space during propagation, so the plenoptic function is invariant along the ray traversal direction; we can also assume that the light field is time-invariant during the period of interest. But still, the task of measuring the entire field is a daunting one.</p>
<p>The next best thing is to sample the light field. A lens-based camera does a poor job of sampling the light field. The pixel <span class="math inline">\(p_a\)</span> integrates a bundle of rays, two of which are shown. Even assuming that the ray’s radiance remains unchanged as it passes through the lens, the inherent integration by the pixel (i.e., the measurement equation in <a href="#eq-cam" class="quarto-xref">Equation&nbsp;<span>9.1</span></a>) still means from the pixel value itself we could not decouple the radiance of the incident rays. Therefore, the ray that <span class="math inline">\(p_c\)</span> wants cannot be easily extracted from <span class="math inline">\(p_a\)</span>. Using an ideal pinhole helps, but pinhole imaging comes with its own limitations that make it infeasible in practice (<a href="imaging-optics.html#sec-chpt-imaging-optics-pinhole" class="quarto-xref"><span>Section 15.2</span></a>).</p>
<div id="fig-lf_microlens_array" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lf_microlens_array-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figs/lf_microlens_array.svg" class="img-fluid figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lf_microlens_array-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.3: A pixel in a conventional camera (e.g., <span class="math inline">\(q\)</span>) integrates over a large portion of the light field. By placing a microlens array (here at where the sensor plane would have been in lieu of the microlens array), each pixel (e.g., <span class="math inline">\(p\)</span>) now integrates over a small portion of the light field.
</figcaption>
</figure>
</div>
<p>A vast literature exists in effective light-field sampling <span class="citation" data-cites="lam2015computational">(<a href="references.html#ref-lam2015computational" role="doc-biblioref">Lam 2015, sec. 3</a>)</span>. A good trade-off in practice is to insert a lenticular array or a microlens array between the main imaging lens and the sensor plane <span class="citation" data-cites="ng2006digital adelson1992single">(<a href="references.html#ref-ng2006digital" role="doc-biblioref">Ng 2006</a>; <a href="references.html#ref-adelson1992single" role="doc-biblioref">Adelson and Wang 1992</a>)</span>. The idea was first conceptualized by Gabriel Lippmann <span class="citation" data-cites="lippmann1908epreuves">(<a href="references.html#ref-lippmann1908epreuves" role="doc-biblioref">Lippmann 1908</a>)</span><a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> <a href="#fig-lf_microlens_array" class="quarto-xref">Figure&nbsp;<span>9.3</span></a> shows one such example. Without the microlens array, a pixel (e.g., <span class="math inline">\(q\)</span>) would integrate over all the rays that are subtended by the main lens, which is relatively large. Now we insert a microlens array and move the sensor plane a little farther back; each pixel (e.g., <span class="math inline">\(q\)</span>) now integrates over a much smaller portion of the light field (rays subtended by a microlens), providing a higher angular resolution in light-field measurement.</p>
</section>
<section id="sec-chpt-mat-basics-radiometry-lf-ren" class="level3" data-number="9.2.2">
<h3 data-number="9.2.2" class="anchored" data-anchor-id="sec-chpt-mat-basics-radiometry-lf-ren"><span class="header-section-number">9.2.2</span> Light-Field Rendering</h3>
<p>The main reason we want to measure the light field is so that we can render new images. <strong>Light-field rendering</strong> is concerned with rendering a new image at a novel perspective (or by a novel camera configuration) given a set of images from other perspectives/configurations. It is a form of <strong>image-based rendering</strong>. In this sense, many familiar tasks such as interpolating between video frames, panoramic photography, and (stereoscopic) 360<span class="math inline">\(^\circ\)</span> video rendering are all light-field rendering in disguise.</p>
<p>Given that each image is a sample of a portion of the light field followed by a low-pass filter (i.e., the integration in <a href="#eq-cam" class="quarto-xref">Equation&nbsp;<span>9.1</span></a>), rendering an image at a new perspective is nothing more than estimating another sample of the light field. As with any signal re-sampling task, the ideal solution to light-field rendering is to first reconstruct the underlying light field from a set of samples and then re-sample the light field given the new perspective. Signal filtering is necessary for both signal reconstruction and anti-aliasing, and the name of the game is to design good filters that are practically useful and computationally tractable <span class="citation" data-cites="pharr2023physically">(<a href="references.html#ref-pharr2023physically" role="doc-biblioref">Pharr, Jakob, and Humphreys 2023, chap. 8.8</a>)</span>.</p>
<p>Of course, modern image-based rendering, known under the name (neural) radiance-field rendering <span class="citation" data-cites="mildenhall2021nerf kerbl20233d">(<a href="references.html#ref-mildenhall2021nerf" role="doc-biblioref">Mildenhall et al. 2021</a>; <a href="references.html#ref-kerbl20233d" role="doc-biblioref">Kerbl et al. 2023</a>)</span>, approaches the whole problem through machine learning and learns to reconstruct from massive amounts of data. To be precise, these methods do not reconstruct the light field; they reconstruct the radiance field.</p>
</section>
<section id="radiance-field-rendering" class="level3" data-number="9.2.3">
<h3 data-number="9.2.3" class="anchored" data-anchor-id="radiance-field-rendering"><span class="header-section-number">9.2.3</span> Radiance-Field Rendering</h3>
<p><strong>Radiance field</strong>, popularized by <span class="citation" data-cites="mildenhall2021nerf">Mildenhall et al. (<a href="references.html#ref-mildenhall2021nerf" role="doc-biblioref">2021</a>)</span>, applies a simplification and an addition to a light field. A radiance field is described by a function <span class="math inline">\(R(p, \omega, r, g, b, \sigma)\)</span>, describing the <span class="math inline">\((r, g, b)\)</span> color and the density <span class="math inline">\(\sigma\)</span> of a ray passing through a position <span class="math inline">\(p\)</span> along the direction <span class="math inline">\(\omega\)</span>. Compare that with the plenoptic function, we can see that the radiance field function simplifies the the energy spectrum into just the tristimulus color values and assumes that the energy is time-invariant.</p>
<p>Importantly, the radiance field incorporates a new quantity, density, that is absent in the light field. Density has nothing to do with the energy of a ray; rather, it is/models an intrinsic property of the material (at position <span class="math inline">\(p\)</span> along ray direction <span class="math inline">\(\omega\)</span>). Materials are important for imaging and rendering, because they change the light field of the scene — through surface scattering and volume scattering. After all, rendering is a process of simulating the light-matter interactions.</p>
<p>In essence, a radiance field combines both a (simplified) light field, a property of the light, and a density field, a property of the materials. This simple extension from light to materials allows radiance-field methods to model (in fact, learn) material properties, which in turn enables more effective light-field rendering. Conventional light-field rendering, in contrast, does not attempt to decouple the light field from the material properties.</p>
<p>Radiance-field methods learn, from offline captured images (hence image-based rendering), to predict the tristimulus color values and density of a given point along a given direction:</p>
<p><span class="math display">\[
f: (p, \omega) \mapsto r, g, b, \sigma.
\]</span></p>
<p>The function <span class="math inline">\(f\)</span> can be parameterized in many ways. Two of most popular parameterizations are to use either a neural network <span class="citation" data-cites="mildenhall2021nerf">(<a href="references.html#ref-mildenhall2021nerf" role="doc-biblioref">Mildenhall et al. 2021</a>)</span> or a mixture of Gaussians <span class="citation" data-cites="kerbl20233d">(<a href="references.html#ref-kerbl20233d" role="doc-biblioref">Kerbl et al. 2023</a>)</span>. With <span class="math inline">\(f\)</span>, we can then synthesize/rendering any image — by co-opting the classic volume rendering. We will study density and radiance field in much greater detail in <a href="rendering-rte.html" class="quarto-xref"><span>Chapter 13</span></a>.</p>
</section>
<section id="light-field-display" class="level3" data-number="9.2.4">
<h3 data-number="9.2.4" class="anchored" data-anchor-id="light-field-display"><span class="header-section-number">9.2.4</span> Light-Field Display</h3>
<p><strong>Light-field display</strong> is a 3D display technology that attempts to reproduce the light field of a scene <span class="citation" data-cites="jones2007rendering wetzstein2012tensor lanman2013near">(<a href="references.html#ref-jones2007rendering" role="doc-biblioref">Jones et al. 2007</a>; <a href="references.html#ref-wetzstein2012tensor" role="doc-biblioref">Wetzstein et al. 2012</a>; <a href="references.html#ref-lanman2013near" role="doc-biblioref">Lanman and Luebke 2013</a>)</span>. Reproducing the light field provides the depth information of a scene that is missing in conventional 2D displays and can, thus, accurately drive the accommodation of eye lens in immersive (AR/VR) environment <span class="citation" data-cites="wann1995natural hoffman2008vergence">(<a href="references.html#ref-wann1995natural" role="doc-biblioref">Wann, Rushton, and Mon-Williams 1995</a>; <a href="references.html#ref-hoffman2008vergence" role="doc-biblioref">Hoffman et al. 2008</a>)</span>. Other technologies include varifocal displays, multi-focal displays, and holographic displays.</p>
<div id="fig-lf_disp" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lf_disp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figs/lf_disp.svg" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lf_disp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.4: We first capture the light field (here using a pinhole array) and then reproduce the light field (by placing the displays on the other side of the pinhole array), offering depth cues.
</figcaption>
</figure>
</div>
<p><a href="#fig-lf_disp" class="quarto-xref">Figure&nbsp;<span>9.4</span></a> shows a usual two-stage process of displaying a light-field. The first step is to capture the light field using some form of light-field imaging technique discussed in <a href="#sec-chpt-mat-basics-radiometry-lf-im" class="quarto-xref"><span>Section 9.2.1</span></a>; here we use a pinhole array placed in front of the sensor plane. Each pinhole covers a small group of the pixels on the sensor; the image captured by the group of pixels under each pinhole is called an <em>elemental image</em>. Once we have recorded the light field, we can then reproduce it. This is done by displaying the elemental images, each with a display placed on the other side of the pinhole array. Note that the relative positions of the display pixels are reversed from that of the the image pixels during light-field recording.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-adelson1992single" class="csl-entry" role="listitem">
Adelson, Edward H, and John YA Wang. 1992. <span>“Single Lens Stereo with a Plenoptic Camera.”</span> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> 14 (2): 99–106.
</div>
<div id="ref-bergen1991plenoptic" class="csl-entry" role="listitem">
Bergen, James R, and Edward H Adelson. 1991. <span>“The Plenoptic Function and the Elements of Early Vision.”</span> <em>Computational Models of Visual Processing</em> 1 (8): 3.
</div>
<div id="ref-gortler1996lumigraph" class="csl-entry" role="listitem">
Gortler, Steven J, Radek Grzeszczuk, Richard Szeliski, and Michael F Cohen. 1996. <span>“The Lumigraph.”</span> In <em>ACM Transactions on Graphics (ToG)</em>, 43–54. ACM New York, NY, USA.
</div>
<div id="ref-hoffman2008vergence" class="csl-entry" role="listitem">
Hoffman, David M, Ahna R Girshick, Kurt Akeley, and Martin S Banks. 2008. <span>“Vergence–Accommodation Conflicts Hinder Visual Performance and Cause Visual Fatigue.”</span> <em>Journal of Vision</em> 8 (3): 33–33.
</div>
<div id="ref-jones2007rendering" class="csl-entry" role="listitem">
Jones, Andrew, Ian McDowall, Hideshi Yamada, Mark Bolas, and Paul Debevec. 2007. <span>“Rendering for an Interactive 360 Light Field Display.”</span> In <em>ACM SIGGRAPH 2007 Papers</em>, 40–es.
</div>
<div id="ref-kerbl20233d" class="csl-entry" role="listitem">
Kerbl, Bernhard, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 2023. <span>“3d Gaussian Splatting for Real-Time Radiance Field Rendering.”</span> <em>ACM Trans. Graph.</em> 42 (4): 139–31.
</div>
<div id="ref-kolb1995realistic" class="csl-entry" role="listitem">
Kolb, Craig, Don Mitchell, and Pat Hanrahan. 1995. <span>“A Realistic Camera Model for Computer Graphics.”</span> In <em>Proceedings of the 22nd Annual Conference on Computer Graphics and Interactive Techniques</em>, 317–24.
</div>
<div id="ref-lam2015computational" class="csl-entry" role="listitem">
Lam, Edmund Y. 2015. <span>“Computational Photography with Plenoptic Camera and Light Field Capture: Tutorial.”</span> <em>Journal of the Optical Society of America A</em> 32 (11): 2021–32.
</div>
<div id="ref-lanman2013near" class="csl-entry" role="listitem">
Lanman, Douglas, and David Luebke. 2013. <span>“Near-Eye Light Field Displays.”</span> <em>ACM Transactions on Graphics (TOG)</em> 32 (6): 1–10.
</div>
<div id="ref-levoy1996light" class="csl-entry" role="listitem">
Levoy, Marc, and Pat Hanrahan. 1996. <span>“Light Field Rendering.”</span> In <em>ACM Transactions on Graphics (ToG)</em>, 31–42. ACM New York, NY, USA.
</div>
<div id="ref-lippmann1908epreuves" class="csl-entry" role="listitem">
Lippmann, Gabriel. 1908. <span>“Epreuves Reversibles Donnant La Sensation Du Relief.”</span> <em>J. Phys. Theor. Appl.</em> 7 (1): 821–25.
</div>
<div id="ref-mildenhall2021nerf" class="csl-entry" role="listitem">
Mildenhall, Ben, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. 2021. <span>“Nerf: Representing Scenes as Neural Radiance Fields for View Synthesis.”</span> <em>Communications of the ACM</em> 65 (1): 99–106.
</div>
<div id="ref-ng2006digital" class="csl-entry" role="listitem">
Ng, Ren. 2006. <span>“Digital Light Field Photography.”</span> PhD thesis, Stanford University.
</div>
<div id="ref-pharr2023physically" class="csl-entry" role="listitem">
Pharr, Matt, Wenzel Jakob, and Greg Humphreys. 2023. <em>Physically Based Rendering: From Theory to Implementation</em>. 4th ed. MIT Press.
</div>
<div id="ref-reinhard2008color" class="csl-entry" role="listitem">
Reinhard, Erik, Erum Arif Khan, Ahmet Oguz Akyuz, and Garrett Johnson. 2008. <em>Color Imaging: Fundamentals and Applications</em>. CRC Press.
</div>
<div id="ref-wann1995natural" class="csl-entry" role="listitem">
Wann, John P, Simon Rushton, and Mark Mon-Williams. 1995. <span>“Natural Problems for Stereoscopic Depth Perception in Virtual Environments.”</span> <em>Vision Research</em> 35 (19): 2731–36.
</div>
<div id="ref-wetzstein2012tensor" class="csl-entry" role="listitem">
Wetzstein, Gordon, Douglas R Lanman, Matthew Waggener Hirsch, and Ramesh Raskar. 2012. <span>“Tensor Displays: Compressive Light Field Synthesis Using Multilayer Displays with Directional Backlighting.”</span>
</div>
</div>
</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>Assuming there is no noise and there is no quantization error in converting analog signals to digital signals.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Lippmann did not get to implement the idea. He won the Nobel Prize in Physics in 1908 for inventing, for the first time, a method for color photography.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./rendering-radiometry.html" class="pagination-link" aria-label="Radiometry and Photometry">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Radiometry and Photometry</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./rendering-re.html" class="pagination-link" aria-label="Rendering Surface Scattering">
        <span class="nav-page-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Rendering Surface Scattering</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/learnvisualcomputing/learnvisualcomputing.github.io/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></div></div></footer></body></html>