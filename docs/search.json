[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Foundations of Visual Computing",
    "section": "",
    "text": "Preface\nThis book attempts to present a unified view of visual computing, looking at it as a series of signal transductions across different domains — optical, analog, digital, and semantic — along with the processing that happens within each. Any sufficiently complex visual computing system worth studying will likely involve both transductions and processing in all of these domains.\nTake Augmented Reality glasses as an example. The input signals — light — are in the optical domain. These first need to be converted into electrical signals by an image sensor so that a computer system can process them to extract semantic information–say, the orientation of a table in a room. The system then simulates light transport to generate photorealistic, context-appropriate virtual objects–perhaps a mug correctly oriented on that very table. Finally, these virtual objects must be transformed back from electrical signals, in the form of pixels, into optical signals by the display.\nBut wait, we are still not done! The light emitted by the display enters our eyes, where photoreceptors in the retina convert the optical signals back into electrical ones. The retina and, further downstream, the brain, process these signals, eventually giving rise to our perception and cognition: we see a virtual mug sitting naturally on a real table.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#why-this-book",
    "href": "index.html#why-this-book",
    "title": "Foundations of Visual Computing",
    "section": "Why This Book",
    "text": "Why This Book\nVisual computing is wonderfully broad, touching everything from the sciences of human vision to the engineering of sensors, optics, displays, and rendering systems. The main motivation for writing the book grew out of a simple observation: while each of these areas is well covered in excellent texts, they are rarely explored together in a single, coherent story.\nWhy bring them together? If you are an engineer, the ability to move comfortably “across the stack” let you see the whole system at once, finding connections and optimizations that are otherwise not obvious when working in isolation. For the scientifically curious, there is joy in understanding the fundamentals even without immediate application. As Edwin H. Land once put it, the true application of science is that “we finally know what we are doing.”\nEvery so often I ask myself: why write this book at all, when none of the material is new and so many fine resources already exist? Then I remind myself that most textbooks contain no new results; by the time knowledge reaches a textbook, it is already established. What books do is to offer perspectives, and that always reveals the idiosyncrasies of the author.\nThe perspective I most want to convey is this: every conclusion is drawn based on a set of precisely defined terms and assumptions, which, unfortunately, are not always announced. By taking a first-principles approach, I hope to make those definitions and assumptions explicit, so it is always clear when each conclusion applies and when it does not.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-do-we-not-cover",
    "href": "index.html#what-do-we-not-cover",
    "title": "Foundations of Visual Computing",
    "section": "What Do We Not Cover",
    "text": "What Do We Not Cover\nWe will not attempt to be comprehensive — no one understands everything, and no one needs to understand everything to get started. Our aim is to take a first-principles approach, giving you a solid foundation and the confidence to learn new concepts as they arise.\nWe will not cover computer vision — a field central to, and is arguably the most well-understood part of, visual computing. There are simply too many excellent texts.\nIn human vision, our focus will be on the early visual system, particularly the retina. This is where the first steps of seeing occur, setting the limits of our vision and representing the most thoroughly studied part of the visual system.\nIn imaging and display, we review basic principles (optical-electrical signal transductions) while being intentionally light on implementation details — these evolve quickly and are often proprietary.\nIn rendering, we focus on the classical computational models of light-matter interactions while completely ignoring the implementation issues such as the modern graphics pipelines (e.g., programming Vulkan) and GPU hardware. We will not cover the emerging (neural) rendering paradigm in detail, but will explore its connections to the classical theories.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#book-structure",
    "href": "index.html#book-structure",
    "title": "Foundations of Visual Computing",
    "section": "Book Structure",
    "text": "Book Structure\nThe book starts with an overview of or, rather, an invitation to visual computing, putting different topics we will discuss into a unified framework. The rest of the book is then divided into four major parts.\nPart I is concerned with the human visual system. The output of an imaging system, a display, or a rendering system is ultimately consumed by humans. Even computer vision algorithms that analyze visual inputs take inspirations from human vision.\nPart II discusses rendering, focusing on modeling light-matter interactions. We will mostly be taking a geometrical optics perspective, using radiometry and radiative transfer (or light transport) as our main theoretical tool.\nPart III discusses imaging, where we will cover how a modern digital camera forms an image from a real-world scene. We will discuss the imaging optics, the image sensor architecture, and the post-sensing image signal processing. Along the way, we focus on computational modeling of the image formation process, taking a linear system perspective and accounting for the presence of noise.\nPart IV discusses displays. We will discuss modern display architectures, including its optical components that turn electrical signals to lights, the electrical components that drive the optics, and the computational components that turn image pixels to drive signals.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#other-books",
    "href": "index.html#other-books",
    "title": "Foundations of Visual Computing",
    "section": "Other Books",
    "text": "Other Books\nThis is necessarily an incomplete list, because I can mention only books that I find myself reviewing from time to time.\nFor human vision, the monumental book by Wandell (1995) is a must read; it is freely available, and a second version is in the works. Cornsweet (1970), while slightly dated, is an all-time classic that has inspired a whole generation of vision scientists. Rodieck (1998) is a breathtaking walk-through of the early stages in vision (eye optics and retinal processing), where every step is described operationally rather than conceptually. In the end, you get the feeling that there is no magic. I also like Nelson (2017), covering light, imaging, and vision — written by a physicist and from a physical perspective. What’s most powerful about these books is that none of them relies on complicated equations — they focus on building intuitions and basic principles first. Bass et al. (2009) provides a comprehensive and authoritative coverage of various topics in vision and visual optics. Goldstein and Brockmole (2017) and Yantis and Abrams (2017) are popular introductory texts to human perception (vision and beyond).\nFor rendering, Pharr, Jakob, and Humphreys (2023) is a golden reference, and Dorsey, Rushmeier, and Sillion (2010) provides a comprehensive coverage of light-matter interactions in the graphics context. Glassner (1995) is bit dated but a classic. For a gentle discussion of light-matter interactions, I would highly recommend Johnsen (2012), which is written in the biological context, but the intuitions built are general. For a technical treatment in the realm of classical electromagnetic theory, read Bohren and Clothiaux (2006). It is written for atmospheric scientists but, again, the principles apply broadly. Of course, for a quantum treatment of light-matter interactions, I’d be remiss not to mention Feynman (1985). Thompson et al. (2011) provides a good survey of visual perception and how explicitly accounting for it can help design better rendering systems.\nThere are comparatively fewer texts on imaging. The one I particularly like is Rowlands (2020), which covers almost every single component in imaging, with rigor, from optics to image sensors and image signal processing. Trussell and Vrhel (2008) and Nakamura (2006) are two excellent references; the former focuses more on the principles (of both imaging and display) and the latter is heavier on the hardware implementations (of image sensors).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#a-live-document",
    "href": "index.html#a-live-document",
    "title": "Foundations of Visual Computing",
    "section": "A Live Document",
    "text": "A Live Document\nOne nice thing about publishing a book online is that it can be a live document, which means two things. First, it allows me to keep working on the book. Here is a list of topics that are being contemplated, and there is a plan to embed the code with the book. Second, you can contribute too! If you see a typo, a technical error, or just an exposition that could use clarification, feel free to open an issue. As mentioned above, the book is necessarily incompletely, so if you think of a topic that warrants coverage, send me an email so that we can expand the book together.\n\n\n\n\nBass, Michael, Casimer DeCusatis, Jay Enoch, Vasudevan Lakshminarayanan, Guifang Li, Carolyn Macdonald, Virendra Mahajan, and Eric Van Stryland. 2009. Handbook of Optics, Volume III: Vision and Vision Optics (Set). McGraw-Hill, Inc.\n\n\nBohren, Craig F, and Eugene E Clothiaux. 2006. Fundamentals of Atmospheric Radiation: An Introduction with 400 Problems. John Wiley & Sons.\n\n\nCornsweet, Tom. 1970. Visual Perception. Academic press.\n\n\nDorsey, Julie, Holly Rushmeier, and François Sillion. 2010. Digital Modeling of Material Appearance. Elsevier.\n\n\nFeynman, R. 1985. QED: The Strange Theory of Light and Matter by Richard Feynman. Princeton University Press.\n\n\nGlassner, Andrew S. 1995. Principles of Digital Image Synthesis. Elsevier.\n\n\nGoldstein, E. Bruce, and R. James Brockmole. 2017. Sensation and Perception. 10th ed. Cengage Learning.\n\n\nJohnsen, Sönke. 2012. The Optics of Life: A Biologist’s Guide to Light in Nature. Princeton University Press.\n\n\nNakamura, Junichi. 2006. Image Sensors and Signal Processing for Digital Still Cameras. CRC press.\n\n\nNelson, Philip. 2017. “From Photon to Neuron: Light, Imaging, Vision.”\n\n\nPharr, Matt, Wenzel Jakob, and Greg Humphreys. 2023. Physically Based Rendering: From Theory to Implementation. 4th ed. MIT Press.\n\n\nRodieck, Robert W. 1998. The First Steps in Seeing. Sinauer Associates.\n\n\nRowlands, D Andrew. 2020. Physics of Digital Photography. 2nd ed. IOP Publishing.\n\n\nThompson, William, Roland Fleming, Sarah Creem-Regehr, and Jeanine Kelly Stefanucci. 2011. Visual Perception from a Computer Graphics Perspective. CRC press.\n\n\nTrussell, H Joel, and Michael J Vrhel. 2008. Fundamentals of Digital Imaging. Cambridge University Press.\n\n\nWandell, Brian A. 1995. Foundations of Vision. Sinauer Associates.\n\n\nYantis, Steven, and Richard A Abrams. 2017. Sensation and Perception. 2nd ed. Worth Publishers.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  An Invitation to Visual Computing",
    "section": "",
    "text": "1.1 What is Visual Computing?\nWe present a framework in which we can reason about the connections between different fields of visual computing. The framework is centered around a series of signal transductions across fundamental signal domains: optical, analog, digital, and semantic, along with the processing that happens within each. We then use the human visual system as a concrete example to examine the different signal transductions and processing, and conclude with a simple abstraction that allows us to reason about opportunities to optimize visual computing systems.\nWe can think of many things when it comes to visual computing. Cameras turn the world into visually pleasing images. Computer graphics algorithms simulate how visually pleasing images are captured as if there was a camera placed in the scene. Computer vision interprets visual information (i.e., images) to infer semantic information of the world (e.g., object categories). Displays generate visual information (i.e., lights) to represent an intended scene. What about Augmented Reality (AR) and Virtual Reality (VR)? Of course; in fact, AR/VR requires all the above to work seamlessly together.\nBut what are the fundamental connections of the multitude of things that we can all loosely associate with visual computing? Figure 1.1 shows the key concepts that unify the different fields of visual computing: 1) representing the physical world in three fundamental signal domains, i.e., the optical, electrical, and semantic domains; 2) processing signals within these domains; and 3) transforming signals across these domains.\nWe will use the Human Visual System (HVS) as an example to walk through some of the key concepts (Section 1.2). We will then expand to three more visual computing domains (computer imaging, computer graphics and rendering, and machine vision), comparing and contrasting how the signal representations, processing, and transformations are exercised in different systems (Section 1.3). We will introduce a power abstraction that governs any visual computing system. This abstraction allows us to reason about the limits of a system and design ways to improve a system (Section 1.4).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An Invitation to Visual Computing</span>"
    ]
  },
  {
    "objectID": "intro.html#sec-chpt-intro-what",
    "href": "intro.html#sec-chpt-intro-what",
    "title": "1  An Invitation to Visual Computing",
    "section": "",
    "text": "Figure 1.1: A framework unifying visual computing. The fundamental building blocks are the signals represented in three fundamental information domains: optical, electrical, and semantic. Visual computing systems transform signals across, and process them within, these domains.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An Invitation to Visual Computing</span>"
    ]
  },
  {
    "objectID": "intro.html#sec-chpt-intro-hvs",
    "href": "intro.html#sec-chpt-intro-hvs",
    "title": "1  An Invitation to Visual Computing",
    "section": "1.2 Human Visual System as a Visual Computing Platform",
    "text": "1.2 Human Visual System as a Visual Computing Platform\nImagine taking a walk through the woods and seeing a butterfly. How does your visual system allow you to notice the butterfly and that it is flying? The inputs to an HVS are lights from the butterfly and the trees in the physical world; they are information represented in the optical domain. The output of the HVS is semantic information, e.g., the color and motion of the butterfly. The HVS extracts semantic information from optical signals through a sequence of signal transformations illustrated as ① \\(\\rightarrow\\) ④ \\(\\rightarrow\\) ⑦ in Figure 1.1.\n\n1.2.1 Signal Representations, Processing, and Transformations in HVS\n\nOptical Signal Processing\nFirst, lights enter your eyes by traveling through the ocular media in your eyes, such as the cornea, pupil, and lenses, and eventually reach the retina. Just before the lights get processed by the retina, the optical signal is already being processed as lights propagate through the eye. This is illustrated by ① in Figure 1.1. For instance, the ocular media absorb photons of certain wavelengths and transmit photons that are unabsorbed. The pupil controls how many photons are allowed in at any given time, and the lens bends and focuses lights on the retina — the chief goal of the eye.\nThe optical information after eye optics and right before being processed by the retina is usually called the optical image. An optical image is a lossy and aberrated version of the optical information in the scene, because the optical signal processing in the eye is lossy. For instance, by focusing on the butterfly, which is at a particular depth, objects at other depths, such as the trees in the background, are blurred. The ocular media also absorb photons selectively across wavelengths, so the true light spectra in the scene are lost.\n\n\nOptical to Electrical Signal Transduction\nThe optical image gets transformed into an electrical representation by the photoreceptors on the retina. This is step ④ in Figure 1.1. Photoreceptors absorb incident photons; once a photon is absorbed, it could, through the phototransduction cascade (Wald 1968)1, generate electrical responses in the form of photocurrents (or, equivalently, photovoltages) across the cell membrane of the photoreceptor. The responses of all the photoreceptors form the electrical representation of the optical image. The rest of the visual system is “just” a hugely complicated circuit that processes the electrical signals from the photoreceptors. In this sense, the optical to electrical transformation is the first step in seeing.\nThis optical to electrical signal transduction is once again lossy. Photoreceptors sample and integrate signals spatially, temporally, and spectrally. As a result, much of the optical information of the incident light, such as the incident angle of the rays, the wavelengths of the photons, and the polarization of the light, is all lost. The main information that is retained, light intensity, is fundamentally limited by sampling and integration, which establish the limits of vision.\n\n\nElectrical to Semantic Signal Transduction\nThe electrical signals produced by the photoreceptors are first processed by the rest of the neurons on the retina and then transmitted in the nervous system to the rest of the visual system, first to the Lateral Geniculate Nucleus (LGN) and then to the visual cortex, where the electrical signals undergo further processing and eventually the semantic meanings of the scene arise. You might now realize that the object is, in fact, a red lacewing butterfly (object recognition), that the color of the butterfly is an astonishing bright red and pale brown interlaced by black and white (color perception), and that the butterfly is flapping and flying (motion perception). We lump all the processing stages after the photoreceptor and call them “post-receptoral” processing, which is denoted by ⑦ in Figure 1.1.\nThe post-receptoral processing progressively extracts richer and higher-level information as the signal progresses through the retina-LGN-cortex pathway. The retina encodes information such as the spatial/temporal frequency, contrast, and, to a large extent, color. This set of information is generally regarded as “low-level” information, which does not, in any way, suggest that the information is somehow inferior; rather, they are the building blocks for higher-order visual processing.\nIt is no small feat that our retina can extract such information: the retina must reliably do so across a very wide range of illumination conditions. For instance, the retina adapts to different illumination levels spanning several orders of magnitude. Perhaps somewhat surprisingly, much of the adaptation takes place within the photoreceptors, whose sensitivity changes based on the incident light intensity. This suggests that photoreceptors are not merely signal transduction devices.\nThe LGN and early areas in the visual cortex extract information such as edge and orientation, and other higher-order areas further refine the signals to extract information such as motion, depth, and object category. Eventually, all these individual bits and pieces are knit together in our brain to give us perception and cognition, i.e., the semantic signals. Information processing in the visual system is not purely feed-forward. There are many feedback paths between cortical areas and between the cortex and the LGN (Gilbert and Li 2013; Briggs 2020). We hasten to add that while we know a lot about the correlation between the electrical responses and the semantic signals, we cannot yet say much about the causation.\n\n\n\n1.2.2 The Transformations are Born of Necessity\nThis complex sequence of transformations that turns the physical realities into one’s subjective percept is born of necessity. A comparison is another sequence of transformations that computer architects are perhaps more familiar with. To have a computer solve a problem for us, we first describe the algorithm in a program written in a high-level language and then transform the program to a low-level, machine-understandable language (i.e., the Instruction Set Architecture), which is then executed on the microarchitecture implemented using circuits and, eventually, moving electrons. If we could directly talk to the electrons and instruct them to move to solve our problem, this sequence of transformations would not be strictly necessary. Similarly, if we could crack open one’s head and manipulate the neuron spikes at will, we could perhaps directly impose certain percepts on humans. But since we cannot (easily), the sequence of signal transduction is necessary. Of course, the sequence of transformation in the computer systems is purposefully engineered to be that way, whereas the one in the HVS is naturally evolved.\n\n\n\n\n\n\nFigure 1.2: Just like in a computer system, HVS also involves a sequence of transformations and can be studied at different levels of abstraction.\n\n\n\nThe fact that there is a sequence of transformations involved suggests that we can study the HVS at different levels of abstraction. This idea is illustrated in Figure 1.2, where, again, we compare the HVS with a computer system. The goal of a computer system is to solve a problem for us, and we can study how a computer system solves the problem at different levels of abstraction. Similarly, the HVS reacts to physical stimuli that are presented to it, and we can study, at different levels, how an HVS reacts to the physical stimuli.\nFirst, we can study it at the psychological level to understand how human psychology, i.e., different forms of perception, cognition, and action, varies under physical stimuli (e.g., lights). This is the field of psychophysics. The psychological experiences one has are results of the collective behaviors of the neurons in the HVS. Naturally, the second way to study the HVS is to relate the behaviors of the neurons and the neural networks to the physical stimuli. This is the field of systems neuroscience. Finally, the behavior of a neuron is fundamentally a result of how cells and molecules function inside and between neurons, so one can study the underlying cellular and molecular processes given physical stimuli. This is the field of cellular and molecular physiology.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An Invitation to Visual Computing</span>"
    ]
  },
  {
    "objectID": "intro.html#sec-chpt-intro-others",
    "href": "intro.html#sec-chpt-intro-others",
    "title": "1  An Invitation to Visual Computing",
    "section": "1.3 Engineered Visual Computing Systems",
    "text": "1.3 Engineered Visual Computing Systems\nWhile the example above is drawn from a biological system, engineered visual computing systems such as smartphones are fundamentally no different in that they all involve visual information represented in and transformed between different domains. We will consider three examples of engineered systems and compare and contrast them with those in the human visual system.\n\n1.3.1 Computer Imaging and Digital Photography\nImaging refers to the task of capturing images of the physical world. Photography is sometimes used interchangeably with imaging. Just to be pedantic, however, photography is a special case of imaging where the goal is to capture visually pleasing images for the human visual system. Scientific imaging is another branch of imaging, where the goal is to capture physically accurate information for scientific inquiry. Examples of scientific imaging include astrophotography, microscopy, and Computed Tomography (CT). We will focus on photography here. Conventional photography is purely analog; think of dark rooms and film development. Modern imaging is computer-assisted, hence the name “computer imaging”, not to be confused with “computational imaging” or “computational photography”, which we will see later.\nAn end-to-end photography system is a complicated sequence of signal transductions involving ② \\(\\rightarrow\\) ⑥ \\(\\rightarrow\\) ⑤ \\(\\rightarrow\\) ① \\(\\rightarrow\\) ④ \\(\\rightarrow\\) ⑦ in Figure 1.1. Lights enter the camera and are first processed by the optics in the camera with the main goal of focusing lights (②), similar to the eye optics. Camera optics are designed completely by humans and we can, therefore, specifically engineer them to achieve a particular performance, whereas eye optics do not enjoy such flexibilities. An example is compound lenses, where a combination of lenses of different kinds are cascaded together to correct various aberrations that a single (spherical) lens introduces.\nAfter the lenses, lights hit the image sensor, whose main job is to transform optical signals to electrical signals (⑥). This is achieved by an array of light-sensitive photodiodes, or pixels, that convert photons to electric charges — using the photoelectric effect (Einstein 1905b, 1905a)2 — which are then converted to digital values, i.e., image pixels. From the signal transduction perspective, the pixels in an image sensor are “just” like photoreceptors on the retina. Vision scientists might take offense at this comparison, because the photoreceptors, as alluded to earlier, are much “smarter” and do a lot more than the pixels, e.g., visual adaptation. In fact, an active area of research is to design pixels so that they adapt like photoreceptors (Liao et al. 2022).\nEventually, an image needs to be displayed for the human visual system to see. The display performs an electrical to optical signal transduction, turning digital pixels to lights (⑤). The photons from the display then enter human eyes, and what we have discussed before about the HVS applies.\n\n\n1.3.2 Computer Graphics and Rendering\nComputer graphics and rendering systems generate images, where photorealism is the main goal (although not the exclusive goal). What does it take to render photorealistic images? A rendered image is photorealistic if it almost looks like a photo taken by a camera, so to render something photorealistic, we want to simulate how a photo is taken! To that end, we must simulate two things: 1) how light transports in space before entering the camera and 2) how lights are turned into pixels by a camera, which follows the signal chain in an imaging system. %Photorealistic rendering is nothing more than simulation.\nComparatively speaking, the second simulation is easier; it amounts to simulating the image formation process in a camera (i.e., ② \\(\\rightarrow\\) ⑥). Since cameras are built by humans, we know exactly how they work, at least in principle. The first simulation is much harder, because it requires simulating the nature: modeling the complicated light-matter interactions (③).\nThis is why most compelling rendering systems are physically based. The kind of physical models used for rendering are phenomenological in nature; they describe the empirical rules governing the light-matter interactions but are not always derived from first principles. An example is that we sometimes model light scattering within a volume using the Bidirectional Scattering Surface Reflectance Distribution Function (BSSRDF), which maps energy from incident rays to exiting rays while abstracting away the details of how photons interact with particles in the material, for which one has to turn to the theory of radiative/energy transfer (Chandrasekhar 1960).\nUsing phenomenological models is sometimes the only option when the actual underlying physics elude us. More importantly, however, simulating physics at the lowest level is simply unnecessary for rendering (which cares about photorealism rather than physical realism) and is computationally too costly for real-time rendering. A recent trend in graphics is neural rendering (Mildenhall et al. 2021), which parameterizes the phenomenological models using deep neural networks and learns such models from actual images, which, by definition, are precisely simulated — by nature.\nSimilar to photography, the rendered images will also go through an electrical-to-optical signal transformation by the display, whose output is then consumed by the HVS.\n\n\n1.3.3 Machine Vision Systems\nFor better or worse, machine vision systems are prevalent in modern society. Autonomous vehicles use machine vision to navigate the environment, drones are used in agriculture to monitor crop health, and facial recognition technologies are increasingly used for security authentications. A machine vision system has two main components: 1) an imaging sub-system that, as discussed above, transforms the optical information in the scene to the electrical information encoded by the image pixels (② \\(\\rightarrow\\) ⑥) and 2) a computing sub-system, which uses computer vision algorithms to interpret the images and extract meanings from the scene (⑧).\nAt the risk of once again downplaying the capabilities and complexities of the HVS, one can argue that a machine vision system largely emulates the HVS — from a signal transduction perspective. Both aim to extract semantic information from the physical world, and both do so by first turning the optical information in the world to its electrical representation. One can even go as far as saying that today’s dominant paradigm toward computer vision, i.e., deep learning, is heavily inspired by the HVS. The field of neuromorphic computing explicitly aims to mimic the structure and operation of the human brain. %The human brain remains the holy grail that engineered systems fall far behind both functionality and efficiency wise. minor league.\nA key difference between imaging in machine vision and imaging for photography is their respective consumer: the output of a photograph is meant to be consumed by an HVS, so visual quality is the main consideration, whereas images captured by, for instance, a robot are meant to be consumed by the downstream computer vision algorithms, which do not care about the visual appearance as long as the semantic information can be decoded from the images. This difference influences the design of the imaging system used in photography and for machine vision.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An Invitation to Visual Computing</span>"
    ]
  },
  {
    "objectID": "intro.html#sec-chpt-intro-research",
    "href": "intro.html#sec-chpt-intro-research",
    "title": "1  An Invitation to Visual Computing",
    "section": "1.4 A Powerful Abstraction",
    "text": "1.4 A Powerful Abstraction\nA visual computing system enlists the work of multiple stages of signal transformation. At every stage in an application’s pipeline, we have decisions to make. These decisions should not be made locally to optimize for a specific stage. Much of the exciting research in visual computing focuses on jointly designing and optimizing all stages of an end-to-end system. This section provides two concrete examples. But before we can entertain them, we first introduce a power abstraction that will allow us to reason about these research ideas.\n\n1.4.1 The Encoding-Decoding Abstraction\nWe can take an information-theoretical perspective and abstract virtually any end-to-end visual computing pipeline as an encoding-decoding process. Decoding is the ultimate goal, but encoding is necessary, because it transforms signals to a domain that can be processed by the decoder. Take, for instance, human vision and machine vision systems; while the ultimate goal is to generate percepts of the physical world, information in the world must be first encoded as electrical signals (through imaging), which are what the brain and computer vision algorithms can process. Imaging itself can also be regarded as an encoding-decoding pair, where the optical information of the scene is first encoded in the electrical domain and the computational algorithms, acting as a decoder, reconstruct an electrical representation that faithfully captures the information in the original scene.\nA more complicated example is visual display devices such as a VR headset. When developing a VR application, we usually have a scene in mind, e.g., a red lacewing butterfly flying in the woods. We hope that users will perceive the object (butterfly), color (the astonishing bright red and pale brown interlaced by black and white), and motion (flapping and flying), but we cannot simply impose these percepts on humans. Instead, we generate visual stimuli on the display to encode the desired percepts. This encoding is done through a combination of rendering (generating electrical signals) and display (converting electrical signals to optical signals). The entire HVS then acts as the decoder, which ideally would provide the intended percepts to users.\n\nEncoding Capabilities Set the Limits on Decoding\nOnce we take this encoding-decoding abstraction, we can start reasoning about limits of a visual computing system. The decoder consumes information generated by the encoder, so its utility is fundamentally limited by the encoding capabilities. Ideally, the encoder should faithfully capture all the information in the world. But in practice, encoding is almost always lossy — for a number of reasons.\nFirst, the actual encoding device used in a system, be it biological or engineered, usually uses fundamentally lossy mechanisms such as sampling and low-pass filtering (e.g., integration). Take HVS as an example, where the optical information of the scene is encoded as photoreceptor responses. The photoreceptors sample the continuous optical image impinging on the retina. The sampling rate dictates, according to the Nyquist–Shannon sampling theorem (Shannon 1949), how well the original optical image can be reconstructed, which in turn limits our ability to see fine details. Even before the photoreceptor sampling, the eye lens blurs signals in the scene not currently in focus, and the pupil, when very small, further blurs even in-focus objects through diffraction, setting the first limit of vision. Blurring is a form of low-pass filtering and is one of the many optical aberrations introduced during the optical signal processing in the HVS.\nSecond, an encoding device might completely disregard certain information in the incident signal. For instance, the polarization information in the incident light is simply ignored by the photoreceptors, whose responses are, thus, invariant to the polarization states. As a result, humans cannot “see” polarization. Some animals, such as butterflies, have polarization-sensitive photoreceptors. So it is not surprising that monarch butterflies make use of the light polarization for navigation (Reppert, Zhu, and White 2004).\n\n\nJointly Design Encoding and Decoding\nThe encoder-decoder abstraction also allows us to design strategies to enhance a visual computing system, both augmenting its capabilities and improving its execution efficiency. For instance, when certain information is not needed for an accurate decoding, it need not be encoded in the first place and, of course, will not participate in decoding, reducing both encoding and decoding costs. Alternatively, if we know what information is crucial for decoding, we can design the encoding system to specifically capture such information. We can also “over-design” the encoder to encode signals in a higher dimensional space than the space to which the information is to be decoded; this essentially introduces redundant samples to improve the robustness to noise.\nUltimately, exploiting these ideas amounts to modeling and, often times, jointly designing the encoder and decoder, considering the end-to-end requirements of task performance, efficiency, and quality — of both the humans and the machine. We will discuss two concrete examples.\n\n\n\n1.4.2 Encoding-Decoding Co-Design: Two Examples\n\nComputational Photography\nThe optical to electrical signal transduction in the image sensor is lossy due to various forms of signal sampling and integration. The signal transduction process itself is also not perfect due to fundamental physical limitations (e.g., quantal fluctuation in photon arrivals) and practical engineering considerations (e.g., sensor size). As a result, the sensor output, which is usually called raw pixels, is noisy (especially in low-light conditions) and does not accurately represent the luminance (especially under bright illuminations) and color information in the scene; certain information such as light-field and polarization is completely lost.\nTo overcome these limitations, modern smartphones and advanced imaging systems use computational algorithms to correct those imperfections, reconstruct the lost information, and sometimes can even add an artistic touch to the photo. Critically, such computational algorithms are usually jointly designed with the imaging system, i.e., the optics and image sensor. This is computational photography, co-designing camera optics, image sensors, and the computational algorithms to overcome fundamental limitations that conventional imaging systems face.\nA classic example of computational photography has to do with a practical problem in photography. As a contemporary reader, you most likely have had the experience where you want to use your smartphone camera to capture a scene that has both a very bright region (e.g., the sunny sky) and a relatively dark region (e.g., a street corner). In technical terms, such a scene has a very high dynamic range (HDR), in that the ratio between the highest and lowest luminance in the scene is huge. The challenge is that image sensors on smartphones cannot capture a wide dynamic range: information at low-luminance regions is noisy, and high-luminance regions saturate pixels. So how do we capture the full luminance range in the scene? This is the task of HDR imaging.\nPeople over the years have come up with a variety of clever ideas for HDR imaging. The most well-known idea is perhaps exposure bracketing, where we take multiple captures of the scene, each with a different exposure time, and then computationally combine the captures to synthesize the full dynamic range in the scene. Another approach is Google’s HDR+ algorithm (Hasinoff et al. 2016), which takes multiple exposures using the same (low) exposure time to ensure high luminance regions are accurately captured, which is then followed by denoising algorithms (e.g., frame averaging) to recover low luminance information. Yet another approach is the time-to-saturation (TTS) image sensors (Stoppa et al. 2002), which measure the time it takes for each pixel to saturate and use that time to extrapolate the luminance information.\nThese HDR imaging techniques are all examples where the imaging system, i.e., the encoder, is intentionally designed to capture critical information (luminance) that is otherwise lost (either due to noise or due to saturation).\n\n\nPerceptual Rendering\nPerceptual rendering is a classic example that leverages the characteristics of HVS (decoder) to inform the design of visual display systems (encoder) such as AR/VR or even just smartphones. We illustrate the basic idea in Figure 1.3, where imaging, rendering, and computing systems encode information that is then decoded by the HVS. The output of the decoder, i.e., the perception, cognition, and action of a human user, is what we care to influence, but what we actually have influence over, for the most part, is the encoding system (for imaging, rendering, and computing). If we have a good understanding of the HVS, we can then invert it to solve for the optimal stimuli, and from there we can then figure out how to optimally engineer the encoding system to deliver the desired stimuli while maximizing the system efficiency.\n\n\n\n\n\n\nFigure 1.3: In visual displaying devices such as AR/VR and smartphones, the engineered systems (imaging/rendering/computer systems) act as an encoder and the HVS acts as a decoder. By understanding the the decoding process, we can then better engineer the imaging, rendering, and computer systems to maximize end-to-end performance both for humans and for machines. Brain implants and gene therapy can directly influence the decoding process and must be designed with the encoder in mind, too.\n\n\n\nGaze-contingent rendering is a well-known technique in AR/VR that exploits this opportunity. Our peripheral visual acuity is extremely bad: we could not tell the details of an object in our peripheral vision. This is mainly a result of: 1) a higher degree of low-pass filtering due to neural convergence in the periphery, and 2) a lower rate of sampling in the periphery due to drastically fewer photoreceptors. When immersed in a virtual environment with a VR headset, the majority of the pixels rendered and displayed fall in the periphery of the retina. Therefore, one could improve the rendering speed by generating low-quality visual stimuli for the periphery with impunity (Patney et al. 2016; Guenter et al. 2012). We could also alter pixel colors in the periphery to reduce display power without introducing artifacts (Duinkharjav et al. 2022).\nModern science and engineering have also empowered us to directly influence the decoder itself through techniques like brain implants and gene therapy — just imagine how powerful it would be to directly control a function whose outputs we care about. An example is the artificial retina (Gogliettino et al. 2023; Muratore and Chichilnisky 2020), an electronic retinal implants that converts optical signals to electrical signals that mimic actual retinal responses or codes; this holds the potential to restore vision to blind individuals. Similarly, these mechanisms must be designed with the encoder in mind in order to deliver the desired output.\n\n\n\n\nBriggs, Farran. 2020. “Role of Feedback Connections in Central Visual Processing.” Annual Review of Vision Science 6 (1): 313–34.\n\n\nChandrasekhar, Subrahmanyan. 1960. Radiative Transfer. Courier Corporation.\n\n\nDuinkharjav, Budmonde, Kenneth Chen, Abhishek Tyagi, Jiayi He, Yuhao Zhu, and Qi Sun. 2022. “Color-Perception-Guided Display Power Reduction for Virtual Reality.” ACM Transactions on Graphics (TOG) 41 (6): 1–16.\n\n\nEinstein, Albert. 1905a. “On a Heuristic Point of View about the Creation and Conversion of Light.” Annalen Der Physik 17 (6): 132–48.\n\n\n———. 1905b. “Über Einen Die Erzeugung Und Verwandlung Des Lichtes Betreffenden Heuristischen Gesichtspunkt.” Albert Einstein-Gesellschaft.\n\n\nGilbert, Charles D, and Wu Li. 2013. “Top-down Influences on Visual Processing.” Nature Reviews Neuroscience 14 (5): 350–63.\n\n\nGogliettino, Alex R, Sasidhar S Madugula, Lauren E Grosberg, Ramandeep S Vilkhu, Jeff Brown, Huy Nguyen, Alexandra Kling, et al. 2023. “High-Fidelity Reproduction of Visual Signals by Electrical Stimulation in the Central Primate Retina.” Journal of Neuroscience 43 (25): 4625–41.\n\n\nGuenter, Brian, Mark Finch, Steven Drucker, Desney Tan, and John Snyder. 2012. “Foveated 3D Graphics.” ACM Transactions on Graphics (TOG) 31 (6): 1–10.\n\n\nHasinoff, Samuel W, Dillon Sharlet, Ryan Geiss, Andrew Adams, Jonathan T Barron, Florian Kainz, Jiawen Chen, and Marc Levoy. 2016. “Burst Photography for High Dynamic Range and Low-Light Imaging on Mobile Cameras.” ACM Transactions on Graphics (ToG) 35 (6): 1–12.\n\n\nLiao, Fuyou, Zheng Zhou, Beom Jin Kim, Jiewei Chen, Jingli Wang, Tianqing Wan, Yue Zhou, et al. 2022. “Bioinspired in-Sensor Visual Adaptation for Accurate Perception.” Nature Electronics 5 (2): 84–91.\n\n\nMildenhall, Ben, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. 2021. “Nerf: Representing Scenes as Neural Radiance Fields for View Synthesis.” Communications of the ACM 65 (1): 99–106.\n\n\nMuratore, Dante G, and EJ Chichilnisky. 2020. “Artificial Retina: A Future Cellular-Resolution Brain-Machine Interface.” In NANO-CHIPS 2030: On-Chip AI for an Efficient Data-Driven World, 443–65. Springer.\n\n\nPatney, Anjul, Marco Salvi, Joohwan Kim, Anton Kaplanyan, Chris Wyman, Nir Benty, David Luebke, and Aaron Lefohn. 2016. “Towards Foveated Rendering for Gaze-Tracked Virtual Reality.” ACM Transactions on Graphics (TOG) 35 (6): 1–12.\n\n\nReppert, Steven M, Haisun Zhu, and Richard H White. 2004. “Polarized Light Helps Monarch Butterflies Navigate.” Current Biology 14 (2): 155–58.\n\n\nShannon, Claude Elwood. 1949. “Communication in the Presence of Noise.” Proceedings of the IRE 37 (1): 10–21.\n\n\nStoppa, David, Andrea Simoni, Lorenzo Gonzo, Massimo Gottardi, and G-F Dalla Betta. 2002. “Novel CMOS Image Sensor with a 132-dB Dynamic Range.” IEEE Journal of Solid-State Circuits 37 (12): 1846–52.\n\n\nWald, George. 1968. “Molecular Basis of Visual Excitation.” Science 162 (3850): 230–39.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An Invitation to Visual Computing</span>"
    ]
  },
  {
    "objectID": "intro.html#footnotes",
    "href": "intro.html#footnotes",
    "title": "1  An Invitation to Visual Computing",
    "section": "",
    "text": "the discovery of which won George Wald his Nobel Prize.↩︎\nthe discovery of which won Albert Einstein his Nobel Prize.↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An Invitation to Visual Computing</span>"
    ]
  },
  {
    "objectID": "hvs.html",
    "href": "hvs.html",
    "title": "Human Visual System",
    "section": "",
    "text": "At the center of visual computing is human vision, which is important for two key reasons. First, many visual computing systems (such as cameras and AR/VR headsets) generate image signals that are ultimately consumed by humans. It is therefore essential to understand how the human visual system processes and interprets these signals. Second, although not the focus of this book, machine vision systems are still, in many ways, striving to match the capabilities of human vision. A deeper understanding of human vision may help inform and inspire better machine vision designs.\nOur discussion will primarily focus on the eye’s optics and retinal processing while touching only lightly on cortical processing — for three reasons. First, eye optics and retinal processing are relatively better understood in the scientific community compared to cortical processing. Second, many visual behavioral characteristics—such as spatial and temporal resolution, contrast sensitivity, and visual adaptation—can be largely explained by the eye’s optics and retinal function. These characteristics are frequently exploited in engineering optimizations aimed at improving efficiency. Finally, the computer science and engineering community is already relatively attuned to cortical processing, thanks to the substantial body of work on spiking neural networks. For further exploration of cortical mechanisms, we refer readers to those specialized texts.",
    "crumbs": [
      "Human Visual System"
    ]
  },
  {
    "objectID": "hvs-intro.html",
    "href": "hvs-intro.html",
    "title": "2  From Light to Vision",
    "section": "",
    "text": "2.1 The Big Picture\nThis chapter gives an overview of the human visual system. We start from eye optics, which direct light to the retina, where the optical-to-electrical signal transduction takes place. We describe a few basic facts about the retina, focusing on the structure and functions of retinal processing. We then briefly talk about processing that takes place after the retina, i.e., in the Lateral Geniculate Nucleus (LGN) and in the visual cortex.\nBefore studying the HVS, it is useful to start by discussing why we care about the HVS at all — after all, if you are a computer science and/or engineering student, why would you care? We will then discuss the methodology we will use when studying the HVS.",
    "crumbs": [
      "Human Visual System",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>From Light to Vision</span>"
    ]
  },
  {
    "objectID": "hvs-intro.html#sec-chpt-hvs-percept-ov",
    "href": "hvs-intro.html#sec-chpt-hvs-percept-ov",
    "title": "2  From Light to Vision",
    "section": "",
    "text": "2.1.1 Why Do We Study HVS?\nWhy do we care about studying the HVS? First and foremost, for the science itself — it is extremely satisfying to just understand “how stuff works”. Understanding the basics of the HVS will also allow us to investigate the unknowns of the HVS, and computer scientists have a lot to off. For instance, modern computational methods, especially deep (artificial) neural networks, have provided us a new toolbox to better understand the biological neural networks: if a signal representation or a learning paradigm is effective in deep neural networks, would it be possible that our HVS uses a similar representation or can learn based on similar representations?\nFor computer scientists and engineers working on visual computing systems, there is another reason, which is already illustrated in Figure 1.3. The psychological experiences of the users of a computing platform, be it an AR/VR headset or a smartphone, are what we want to influence, but we, for the most part, exert that influence indirectly, by designing and optimizing the imaging, rendering, and computer systems. The outputs of these systems, i.e., the visual stimuli coming out of the display, become the input to the HVS of a human whose psychological states we care to optimize. So if we understand the HVS, we could invert the HVS process, given the desired psychological states, to solve for the optimal visual stimuli, and from there we can then think about how to best design the various engineered systems.\nUnderstanding the cellular, molecular, and neural processes in the HVS has also inspired people to better engineer systems such as imaging systems (Liao et al. 2022; Wodnicki, Roberts, and Levine 1995) and deep neural networks, even though the output of these systems is not meant to be consumed by the HVS (Idrees et al. 2024).\n\n\n2.1.2 How Do We Study HVS?\nHow do photons in the real world give rise to perception and cognition in our brain when they enter our eyes? We want to show you that there is really no magic here. The perception and cognition we experience are fundamentally a result of the complicated, first optical and eventually electrical, signal processing in the physiological system — our eyes and brains.\nThis relationship between low-level electrical signals and high-level behavioral responses in humans is conceptually no different from one that we find in computers. This comparison is shown earlier in Figure 1.2. For someone unfamiliar with computer systems and chip design, it would seem rather magical that a computer does what it does. But we know that the high-level, observable behaviors of a computer program are a result of low-level processing in the electrical circuits. Similarly, the experiences humans have in response to visual stimuli are a result of the collective behaviors of the underlying neurons in the nervous system, whose behaviors result from the cellular and molecular processes within and between individual neurons.\nThe circuits in a computer are made of engineered material such as transistors, whereas circuits in the HVS are made of biological materials such as neurons. Fundamentally, however, it is all physics — electrons and/or ions move around and cause changes in voltage potentials and currents, and these changes are how information is propagated.\nWith the advancements in modern science and engineering, we can now measure, at a neuronal or even sub-neuronal level, the electrical responses of the HVS when presented with visual inputs. These measurements allow us to correlate electrical responses to perception and cognition, which, in turn, allow us to say something like “this part of the HVS supports or is responsible for that particular function (e.g., object detection).”” It is important to note, however, that we still do not know why the electrical responses cause our perception and cognition. The causation problem, for the moment, is at best a philosophical problem or, if you will, a religious one.\nThe goal of this chapter is to give you an overview of the Human Visual System (HVS). We will focus on the main components and key facts of the HVS so that you can start appreciating the connections between signal processing at the physiological level and perception, cognition, and action at the behavioral level while leaving many details to later chapters.\nThe signal processing in the HVS consists of three main components; this is illustrated in Figure 2.1. First, lights are processed in the optical domain as they enter our eyes and go through the eye optics. The optical signals then reach the retina and are first converted to electrical signals by the photoreceptors (cones and rods), which are further processed before exiting the retina. The retina output neurons, i.e., the retinal ganglion cells, encode low-level information such as wavelengths, contrast, timing of object motion, etc. The retinal outputs are then transmitted to the Lateral Geniculate Nucleus (LGN) and, for the most part, relayed to the visual cortex. Cortical processing essentially knits together the low-level, upstream information to give us vision. The retino-geniculo-cortical pathway is the main pathway for the electrical signals.\n\n\n\n\n\n\nFigure 2.1: Pupil, under the control of the iris, lets in lights. Cornea and lens focus light with the former contributing the most optical bending power. Lens contracts and relaxes to accommodate object depth under the control of the ciliary muscle. Retina transforms optical signals to electrical signals, which are further processed and exit the retina through the optic nerve. Retinal signals go through the Lateral Geniculate Nucleus and then are projected to the visual cortex. This retino-geniculo-cortical pathway carries the main information flow in the HVS, with the cortex also providing feedback to the LGN. Adapted from Selket (2007).",
    "crumbs": [
      "Human Visual System",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>From Light to Vision</span>"
    ]
  },
  {
    "objectID": "hvs-intro.html#sec-chpt-hvs-percept-optics",
    "href": "hvs-intro.html#sec-chpt-hvs-percept-optics",
    "title": "2  From Light to Vision",
    "section": "2.2 Eye Optics",
    "text": "2.2 Eye Optics\nThe optical signal impinging on the retina is called the optical image, which is a 2D continuous signal in that at any position on the retinal surface we can ask: how much optical power is there here1? Ideally, the optical image is a perfect perspective projection from the 3D physical world, with no loss of information other than the projection. The reality is much more complicated.\n\n\n\n\n\n\nFigure 2.2: Much of the optical bending power in the eye is contributed by the cornea, which has a large refractive index difference with respect to its adjacent ocular media (Snell’s law). The lens also contributes to light bending, albeit with a lower contribution. Cornea is rigid but the lens is malleable, so accommodation is attributed exclusively to the lens. From LaValle (2023, fig. 4.25).\n\n\n\n\n2.2.1 The Main Goal is to Focus Lights\nThe main goal of the eye is to focus light on the retina. To focus light the optics need to bend light, which is achieved collectively by all the ocular media in the eye, including the cornea, aqueous humour, lens, and vitreous humour. This is illustrated in Figure 2.2. Lights bend because of the difference in refractive index between adjacent ocular media. Most of the bending is done by the cornea because there is a large difference in the refractive index between the cornea and the air. The lens also contributes to light bending, albeit with a lower contribution, because the differences in refractive index between the lens and its adjacent media (aqueous fluid and vitreous fluid) are relatively small.\nThe cornea is fixed in shape. Lens, in contrast, is malleable in its shape. The ciliary muscle controls the contraction and relaxation of the lens, which changes the focal length, and thus bending power, of the lens, and by extension the entire eye optical system. Adjusting the focal length to bring an object into focus is called accommodation.\nBut if the ciliary muscle cannot properly adjust the lens, we get defocused blur, which is a form of optical aberration. There are a number of other optical aberrations; astigmatism and chromatic aberration are two common ones found in eyes. While not an optical aberration, diffraction also contributes substantially to visible blurs when the pupil size is very small (e.g., under strong illumination).\nFor our purpose, “imperfections” introduced by eye optics (aberration and diffraction) can be modeled by the Point Spread Function (PSF) of the optical system, which we will see later in Section 15.5.\n\n\n2.2.2 Ocular Media Absorb Light Selectively\nWhile all the ocular media are generally transparent, they still absorb some amount of light. Critically, the absorption and, by extension, transmittance, are strongly wavelength dependent. Color vision is fundamentally tied to the power distribution of light over wavelengths, so the selective absorption of light by the ocular media significantly influences our color vision.\n\n\n\n\n\n\nFigure 2.3: Transmittance spectra of ocular media. Adapted from Boettner and Wolter (1962, fig. 7)\n\n\n\nBoettner and Wolter (1962, fig. 7) measured the spectral transmittance of the eye, which defines the amount of light allowed to transmit through the media at each wavelength; the results are shown in Figure 2.3. Each curve represents the percentage of light remaining at each ocular media and the retina (including both direct transmission and forward scattering). Considering the visible range (we will discuss in the next Chapter why there are even invisible lights) roughly between 380 \\(\\text{nm}\\) and 780 \\(\\text{nm}\\), we can see the ocular media significantly reduces the light power at short wavelengths. As a result, the transmittance spectrum of the ocular media is generally lower at short wavelengths, which means the ocular media generally absorbs blue-ish lights; so if the incident light is white-ish, the light would appear yellow after traveling through the ocular media.",
    "crumbs": [
      "Human Visual System",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>From Light to Vision</span>"
    ]
  },
  {
    "objectID": "hvs-intro.html#sec-chpt-hvs-percept-retina",
    "href": "hvs-intro.html#sec-chpt-hvs-percept-retina",
    "title": "2  From Light to Vision",
    "section": "2.3 Retina: Basic Facts",
    "text": "2.3 Retina: Basic Facts\nNow the photons have arrived at the retina. The retina is where optical signals are transformed into electrical signals. The electrical signals undergo further processing on the retina and are then carried by the optic nerve to the brain. The signal transduction and processing are carried out through layers of neurons on the retina, of which there are five categories (each of which has sub-categories). They are the photoreceptors, bipolar cells, horizontal cells, amacrine cells, and retinal ganglion cells (RGCs). This is illustrated in Figure 2.4.\nThe main information flow starts from the photoreceptors, flows through the bipolar cells, which synapse with photoreceptors and send their outputs to the RGCs. The horizontal cells synapse with the photoreceptors (and other horizontal cells), and the amacrine cells connect with both the bipolar cells and the RGCs (and other amacrine cells). Identifying the different classes of neurons and their connections is largely due to Santiago Ramón y Cajal2.\nInterestingly, while we might be used to neurons communicating through spikes, i.e., action potentials3, the RGCs are the only type of neurons on the retina that spike. The rest of the neurons are non-spiking neurons; they communicate through graded potentials.\n\nOptical-to-Electrical Signal Transduction Takes Place in Photoreceptors\nPhotoreceptors are where optical signals are transformed into electrical signals. Photoreceptors absorb incident photons; once a photon is absorbed, it could generate electrical responses through the process of phototransduction cascade (Wald 1968)4. The electrical response can be represented as photocurrents or, equivalently, photovoltages across the cell membrane of the photoreceptor. We will have a lot to say about this process later in Chapter 3.\n\n\n\n\n\n\nFigure 2.4: The basic neural network on the retina. The photoreceptors convert optical signals to electrical signals. The electrical signals go through the bipolar cells and then to the retinal ganglion cells, which carry all the output of the retina. Horizontal and amacrine cells mediate lateral interactions, giving rise to important features such as the receptive field. Since the RGCs are at the outer most layer of the retina, the optical information and the electrical information flow in opposite directions. Adapted from Purves et al. (2017, fig. 11.5B)\n\n\n\n\n\nFunctional and Anatomical Organizations of the Retina are Opposite\nThe functional organization of the cells is opposite to the anatomical organization of the cells. This is illustrated in Figure 2.4.\nFunctionally, the first layer of the retina is the photoreceptor cells, which convert photons to electrical responses, and the last layer is the RGCs, which carry all the retinal output information and are directly connected to the optic nerve, which are effectively the axons of the RGCs. Anatomically, however, the RGCs lie at the outermost layer of the retina, and the photoreceptors are the innermost layer. Therefore, photons upon reaching the retina first hit the RGCs and then go through other neurons before eventually hitting the photoreceptors, where the signal transduction takes place. As far as a photon is concerned, neurons before the photoreceptors are transparent and simply let the photon through without doing much about it — with an exception that we will see soon.\n\n\nBlind Spot Exists Because of the Routing Issue\nAn implication of the anatomical organization is that the optic nerve must be routed from the front of the retina and through the retina at a single location, which is called the optic disk. The optic disk must be free from any neurons, including photoreceptors, simply for the optic nerve to exit. Since photoreceptors sense light, the optic disk is also called the blind spot. This is illustrated in Figure 2.5. Some vertebrates, like the octopus, do not have this “wiring” issue, since their retinal signals exist from the back of the retina.\n\n\n\n\n\n\nFigure 2.5: Vertebrate eyes have a blind spot (scotoma) because the RGC axons exit the retina from the front of the retina. It is purely a “wiring” issue. Octopus eyes do not have this issue. Adapted from Caerbannog (2016).\n\n\n\nIt is unclear whether there are evolutionary advantages of having a blind spot on our retina, but it does not seem to be a disadvantage: we clearly do not notice the blind spot in our daily life — the downstream visual system fills in the missing information there. Our head and eye movements further mitigate the impact of the blind spot.\n\n\nipRGCs are Light-Sensitive but Do Not Contribute to Image-Forming Vision\nPhotoreceptors are the only type of neurons on the retina that are sensitive to light and contribute to image-forming vision. There is another type of neuron, a sub-type of the RGCs actually, called the intrinsically photosensitive RGCs (ipRGCs) that are also sensitive to light (i.e., they absorb photons and convert optical signals to electrical signals), but interestingly they do not (primarily) contribute to image-forming vision.\nThe ipRGCs were discovered fairly recently, and it is fair to say that the discovery was a big deal for the field (Berson, Dunn, and Takao 2002; Hattar et al. 2002). For the past 150 years or so, human vision could be adequately explained by photoreceptors being the only light-sensitive neurons. Now, if the ipRGCs are also light sensitive, do we have to rewrite the science behind human vision? It turns out that while the ipRGCs do respond to lights, they primarily contribute to non-image-forming vision (but see Dacey et al. (2005)). For instance, they are shown to impact circadian rhythms, mood, and pupillary light reflex (Lazzerini Ospri, Prusky, and Hattar 2017; Do and Yau 2010).",
    "crumbs": [
      "Human Visual System",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>From Light to Vision</span>"
    ]
  },
  {
    "objectID": "hvs-intro.html#sec-chpt-hvs-percept-retinafunc",
    "href": "hvs-intro.html#sec-chpt-hvs-percept-retinafunc",
    "title": "2  From Light to Vision",
    "section": "2.4 Retinal Structure and Functions",
    "text": "2.4 Retinal Structure and Functions\nRetina is organized to perform a set of low-level tasks that are crucial to vision. “Low-level” here refers to the fact that information encoded by the retina forms the building blocks for more complicated visual functions later in the HVS. At the risk of over-simplication, each task is achieved by a visual stream of neurons. These visual streams are also called parallel pathways. This section briefly discusses a set of basic functions of the retina and their visual streams.\n\n2.4.1 Rod vs. Cone Specialization\n\nSensitivity and Kinetics\nThere are two types of photoreceptors: rods and cones. Perhaps the most important difference between the two is that rods are much more sensitive to light than cones. This is evident in Figure 2.6, which compares the single-photon response of rods and cones in primates. The response here is represented by the photocurrent, the change of current that flows into the photoreceptor as a result of photon absorption, which we will talk about in detail later in Chapter 3.\n\n\n\n\n\n\nFigure 2.6: Comparing single photon responses (photocurrents) of rod and cone on a primate. Rods are more sensitive with a slower kinetics. From Angueyra-Aristizábal (2014, fig. 1.4C).\n\n\n\nDue to the high sensitivity, rod responses saturate quickly as the ambient light level increases, so they are primarily responsible for vision at low illumination levels (e.g., at night); rod-mediated vision is called the scotopic vision. Cones are much less sensitive, so they are responsible for vision at normal illumination levels, such as during the day. Cone-mediated vision is called the photopic vision. Figure 2.7 shows the luminance range that both the scotopic and the photopic vision are sensitive to. The sensitivity range overlap, so there is a luminance range where both rods and cones contribute to vision, which is called the mesopic vision.\n\n\n\n\n\n\nFigure 2.7: Sensitivity range of rod-mediated vision and cone-mediated vision. From Purves et al. (2017, fig. 11.11).\n\n\n\nCones also have faster response kinetics than rods: cone responses rise and fall much faster than rods; this is illustrated in Figure 2.6. The faster kinetics allows cones to track moving objects better than rods do. To reason about the influence of the response kinetics, think of a camera where the exposure time is very long: the resulting image is (motion) blurred. Shorter exposure/shutter time captures motion better. Cones have a shorter effective “exposure time” than rods.\n\n\nSpectral Sensitivity and Color Vision\nYet another important difference between rods and cones is that the cone-mediated vision provides color information whereas rod-mediated vision encodes only light intensity but not color. This is because there is only one class of rods but three different classes of cones, each with a different (linearly independent) wavelength sensitivity function. Fundamentally, color arises from the wavelength information in incident lights. Having three types allows cones to have a stronger capability of encoding wavelength information than rods. The entire Chapter 4 is devoted to color vision; for now, let us just appreciate how different cones have different wavelength selectivities.\n\n\n\n\n\n\nFigure 2.8: The absorbance spectra of the three cones (L, M, S) and the rod (R) in humans; data from Dartnall, Bowmaker, and Mollon (1983). The spectra are normalized to peak at 1\n\n\n\nOne way to measure the spectral differences between photoreceptors is using a technique called microspectrophotometry (MSP), which measures the fraction of photons that gets absorbed by a photoreceptor at each wavelength. Using MSP, Dartnall, Bowmaker, and Mollon (1983) collected data for cones and rods from human donors, shown in Figure 2.8. The \\(y\\)-axis plots absorbance, which is \\(\\log(I_{\\text{incident}}/I_{\\text{transmitted}})\\), i.e., the log ratio between the incident light intensity and transmitted (i.e., unabsorbed) light intensity5.\nWhile many cones were measured, there were only three distinct spectra, whose absorbance peaks at relatively long, medium, and short wavelength, respectively. We call them the L, M, and S cones. The rod’s peak is in-between that between the S and the M cones. Note that the spectra in Figure 2.8 are normalized to peak at unity. The absolute absorbance of rods is slightly lower than that of the cones.\nNotably, the L and M cones exhibit greater similarity to each other than to the S cones, suggesting that the S cones are quite different from the L and M cones. This is a clue about the evolution of the three cone types. Most mammals have only two cone types, one that is sensitive to short-wavelength light and the other that is sensitive to long-wavelength lights; the former is evolved into the S cones, and the latter separated into the L cones and M cones through a local gene duplication (Jacobs 2008). Since the duplication is relatively recent (about 30 to 35 million years ago), the L and M cones are rather similar.\nBowmaker et al. (1978) shows similar data for a macaque. There, the L and M cone spectra are also closer to each other than to the S cone spectrum, indicating that the divergence between the L and M cones occurred before the split between modern Old World monkeys and great apes (including humans).\n\n\nSpatial Distribution\nThere are about 120 million rods and about 6 millions cones. The left panel in Figure 2.9 shows the distribution of both cones and rods on the retina. Almost all the cones are concentrated at fovea, a small, central pit on the retina that is approximately 2 mm in diameter and subtends a visual angle of about 1\\(^{\\circ}\\). The position in the fovea that has the peak cone density is defined to have an eccentricity of 0\\(^{\\circ}\\). There are no rods in the fovea; all the rods are placed at the retina periphery, peaking at about 20\\(^{\\circ}\\) away from the fovea.\nThe right panel in Figure 2.9 are images of photoreceptors at the fovea and at the periphery, taken by Curcio et al. (1990). Cones exclusively occupy the fovea and they become sparser and larger in the periphery. Rods fill in the spaces in the periphery.\nThere are many important implications of the photoreceptor mosaic and distribution. First, the visual acuity decreases in the visual periphery. Think of photoreceptors as sampling the continuous optical image impinging upon the retina. A higher density leads to a higher sampling rate. In addition, larger cone sizes in the periphery are equivalent to higher degrees of blurring, since photons hitting a cone are integrated together just like by a camera pixel (although, critically, the electrical response of a photoreceptor is not proportional to the photon count, unlike a camera pixel), and integration is a form of low-pass filtering.\n\n\n\n\n\n\nFigure 2.9: Left: cone and rod distribution on the retina; the x-axis is the eccentricity (angular distance from the fovea, which has an eccentricity of 0\\(^{\\circ}\\)). From Glassner (1995, fig. 1.4). Right: photos of photoreceptors in the fovea and periphery; rods are absent in the fovea, and cones become sparser and larger in the periphery. Adapted from Curcio et al. (1990).\n\n\n\nWe hasten to add that the lower acuity in the periphery is not exclusively attributed to the photoreceptor mosaic. As we will see shortly, how photoreceptors communicate with other neurons on the retina plays an important role, too.\nSecond, since the fovea has the highest visual acuity, our ocular motor system has evolved in such a way that when we want to see fine details of an object, we move our eyes so that light from the object is captured by the fovea. This means that we cannot see fine details of an object in dim environments if we fixate at it. Instead, we would have a better chance of seeing details if we intentionally placed the object in our peripheral vision.\n\n\nRod vs. Cone Pathways and Visual Streams\nRods and cones have their own pathways initially and merge later. This is shown in Figure 2.4. Both rods and cones synapse with bipolar cells, but they synapse with distinct bipolar cells. That is, an individual bipolar cell receives information from either rods only or cones only. The rod pathway and cone pathway are parallel streams at this point. The bipolar cells then feed their outputs to the RGCs. A RGC can mix information from both rod and cone bipolar cells. This mixing is enabled by amacrine cells, which synapse with both the rod and cone bipolar cells and with the RGCs. Thus, the distinct information in the rod pathway and the cone pathway gets merged in the RGC layers.\nWhy are rod and cone pathways initially parallel but merge later? The initial parallel pathways allow rods and cones to extract low-level information, such as contrast, independently under different lighting conditions, but once the information is collected, it is processed similarly, so there is really no need to duplicate the processing circuitry.\n\n\n\n2.4.2 Contrast Detection\nAnother important function of the retina is to extract contrast information. Arguably most interesting information in the physical world exists all in image contrast, i.e., local differences in light intensities. Take a look at your surroundings; uniform light levels where there is absolutely no change in light are rare and do not present much useful information. Fine details of an object are really encoded in contrasts.\nThis imposes two requirements on our visual system. First, we need to extract contrasts and encode them in neural signals so that they can be processed by the rest of the brain. This is the focus of this section. Second, we must reliably encode contrast across a wide range of ambient light levels, which is the focus of Section 2.4.3.\n\n\n\n\n\n\nFigure 2.10: Weber contrast is often used for detecting objects against a uniform background, and Michelson contrast is used for detecting patterns. The two definitions are compatible: they both describe the ratio between the maximal variation of the signal over the mean.\n\n\n\n\nContrast is Variation Over Mean\nBefore discussing how the RGCs meet these requirements, we must first define contrast more rigorously. Intuitively, contrast describes how much variation there is in a signal relative to the average strength of the signal. There are two commonly used definitions, both of which are compatible with this intuition. They are usually used in different scenarios. Figure 2.10 illustrates the two definitions.\nWeber contrast is often used in scenarios where there is a small object against a relatively uniform background. The contrast \\(C_w\\) is defined as:\n\\[\n    C_w = \\frac{I-I_b}{I_b},\n\\tag{2.1}\\]\nwhere \\(I_b\\) is the background luminance and \\(I\\) is the object luminance. If the object is small, the mean luminance of the entire field is approximately the background luminance, and naturally \\(I-I_b\\) is the maximal variance over the mean.\nThe Michelson contrast is used in scenarios where we want to detect patterned signals. Taking a sinusoidal pattern as an example (and recall any arbitrary pattern can be decomposed into sinusoidal basis patterns), the contrast \\(C_m\\) of a sinusoidal signal is usually defined as:\n\\[\n    C_m = \\frac{I_{max} - I_{min}}{I_{max} + I_{min}},\n\\tag{2.2}\\]\nwhere \\(I_{max}\\) and \\(I_{min}\\) are the highest and lowest luminance, respectively, of the signal. We can see that \\(C_m\\) can also be interpreted as the ratio between the variation and the mean of the signal. A higher \\(C_m\\) would mean that the pattern is more easily detected, and vice versa.\n\n\nRGC Pools Signals from Many Photoreceptors\nThere are about 120 million rods, 6 million cones, and 1 million RGCs on the retina. Therefore, a single RGC necessarily receives signals from multiple rods and/or cones. Pooling signals from multiple neurons into a single neuron is generally called neural convergence, a many-to-one mapping. Evidently, there is a much higher degree of neural convergence in rods than in cones. The fovea, which, recall, contains only cones, is an extreme case where there is no neural convergence. In fact, each foveal cone sends its signal to multiple RGCs, so there is a one-to-many mapping there.\n\n\n\n\n\n\nFigure 2.11: Dendritic field sizes (of two RGC subtypes) increase with eccentricity, indicating a higher degree of neural convergence at the periphery. From Wandell (1995, fig. 5.7), which is after Dacey and Petersen (1992, fig. 2A).\n\n\n\nThe higher degree of neural convergence in the rod pathway is another reason why rod-mediated vision is more sensitive than cone-mediated vision: the responses of different rods that are pooled together to the same downstream RGC, so that the RGC could generate responses faster to the brain than if the RGC receives input from only a single cone at the fovea. The flip side of the higher degree of convergence is that rod vision offers low spatial acuity. If an RGC generates a response, we could not resolve the source of that response since it could come from anywhere within a large group of photoreceptors being stimulated. From a signal processing perspective, summation is a form of low-pass filtering (equivalent to convolving the signal with a box filter), which naturally reduces the frequency of the signal.\nThe degree of neural convergence increases as the eccentricity increases. Figure 2.11 shows the dendritic field sizes of two RGC subtypes; the size increases with the eccentricity. The higher degree of neural convergence is another reason why peripheral acuity is much worse than that at the fovea.\n\n\nRGCs Have a Center-Surround Receptive Field\nNeural convergence gives rise to an important concept called receptive field, which is central to contrast encoding. The receptive field of a neuron is the retinal area that influences the neuronal activity. For an RGC, its receptive field is the collection of photoreceptors whose output signals converge at that RGC. Due to the one-to-one mapping relationship at the fovea, the RGCs that are connected to the fovea cones have a receptive field of only one cone.\nThe way an RGC aggregates information from the receptive field is not to simply sum up the signals from the individual photoreceptors. If we illuminate the entire receptive field of an RGC uniformly, the RGCs respond similarly regardless of the illumination intensity. This is a form of light adaptation, which we will discuss shortly. Let’s call the RGC’s response rate under a uniform illumination its spontaneous rate.\n\n\n\n\n\n\nFigure 2.12: RGCs have a center-surround receptive field with two types. The ON-center RGCs are excited by stimuli presented at the center but inhibited by stimuli presented at the surround (stimulus 2 on the left); OFF-center RGCs have the opposite response (stimulus 4 on the right). Drawn after Hubel (1995, p. 41).\n\n\n\nIf uniformly changing the light levels does not change the RGC’s response rate, what does? It turns out that you need to have variations in the illumination within the receptive field. The RGCs respond best to variation patterns that have a center-surround structure. For about half of the RGCs, their response rate is maximized if we present bright lights to the center photoreceptors and dark lights to the surround photoreceptors. These are called ON-center, OFF-surround RGCs, since they have an excitatory center (excited by light) and inhibitory surround (inhibited by light). The other half prefers the opposite pattern: dark at the center and bright at the surround. They are the OFF-center, ON-surround RGCs, since they have an inhibitory center and an excitatory surround. The RGCs are said to have a center-surround receptive field. Figure 2.12 illustrates the receptive fields of the two RGCs.\nH.K. Hartline measured the RGC responses from horseshoe crabs (Hartline and Graham 1932), using which he famously demonstrated inhibitory signals (Hartline 1949; Hartline, Wagner, and Ratliff 1956)6; he was also the first to use the term receptive field (Hartline 1938, 1939, 1940a, 1940b). Barlow (1953) demonstrated the inhibitory signals in a frog’s RGC; Stephen Kuffler (Kuffler 1952, 1953) was the first to demonstrate the center-surround receptive-field structure in a mammalian (cat) RGC, with Barlow also making significant contributions (Barlow, Fitzhugh, and Kuffler 1957).\n\n\nCenter-Surround Receptive Fields are Designed to Encode Contrasts\nLooking at the preferred stimulus of the two RGC types in Figure 2.12 (stimulus 2 for ON-center and stimulus 4 for OFF-center), evidently the RGCs are designed to extract illuminant variations, i.e., contrast. If a visual field has a high (positive) Weber contrast, i.e., there is a small object that is significantly lighter than the background, the ON-center RGC would respond well to it. Similarly, an OFF-center RGC would respond well to a dark object placed against a light background.\n\n\n\n\n\n\nFigure 2.13: Contrast sensitivity function (CSF) under an ON-center midget RGC7; the filled circle is the represents the sensitivity of a uniform signal (i.e., 0 Hz). CSF is bandpass. \\(x\\)-axis is the cycles per degree (CPD) of the retinal signal (see Figure 2.14). Adapted from Derrington and Lennie (1984, fig. 3C).\n\n\n\nWe can also quantify the how the center-surround receptive fields respond to patterns of different Michelson contrast. A complication is that a pattern is described not only by its contrast but also by the frequency. At each frequency, we determine the minimal amount of contrast needed to produce a criterion level of RGC response (say 30 spikes/second)8. The contrast sensitivity at that frequency is defined as the reciprocal of the threshold contrast. We then sweep the frequency and repeat this exercise for each frequency. The result of such a measurement is called the Contrast Sensitivity Function (CSF); Figure 2.13 shows one such example.\nWe can see that the RGC’s CSF is bandpass, where there is a preferred frequency to which an RGC responds the best. When the frequency is too low, the signal is equivalent to a uniform background (filled circle); when the frequency is too high, the positive and negative cycles of the signal cancel each other. In both cases, an RGC would respond weakly, so the contrast needed to produce a criterion level of response is high (i.e., the sensitivity is low). With a spatial frequency of about 5 cycles per degree, the positive amplitude coincides with the ON-center of the cell and the negative amplitude coincides with the OFF-surround of the cell. As a result, the contrast required to produce the same level of response can afford to be low, resulting in a higher sensitivity.\nNote that the Michelson contrast of a signal, as defined in Equation 2.2, is bounded between 0 and 1, so long as the signal is positive everywhere (which is of course the case in real-world visual signals). As a result, the contrast sensitivity is lower-bounded by 1. Sometimes we will see CSF plots where the \\(y\\)-axis goes below 1, because usually people fit a smooth CSF curve based on the measured data and do not cut off the curve below 1. In practice, any contrast sensitivity below 1 should be interpreted as “not detectable”.\n\n\n\n\n\n\nFigure 2.14: The relationship between ordinary spatial frequency and cycle per degree (CPD). Signals \\(S_1\\) and \\(S_2\\) have different spatial frequencies but the same CPD, as they project to the same retinal signal \\(R_b\\). \\(S_2\\) and \\(S_3\\) share the same spatial frequency but differ in CPD, as they correspond to different retinal signals (\\(R_a\\) and \\(R_b\\)). Since retinal signal is what matters for vision, we usually use CPD to represent signal frequency (e.g., Figure 2.13).\n\n\n\nFigure 2.13 quantifies the spatial frequency of a signal using Cycle Per Degree (CPD), which is the number of cycles/periods in a degree (\\(\\pi/180\\) of a radian). CPD is an angular measure of spatial frequency. The relationship between the ordinary spatial frequency and CPD is illustrated in Figure 2.14. Why do we prefer CPD when describing the spatial frequency? This is because CPD better quantifies the frequency of retinal signals, which is what matters for vision, not the frequency of the physical objects themselves.\nConsider an object \\(S_1\\) in the object space. It produces a retinal signal \\(R_b\\), which dictates how well the pattern in \\(S_1\\) is detected. Now we shrink the size of \\(S_1\\) but move it closer to the eye to obtain another signal \\(S_2\\). Clearly \\(S_2\\) has a higher spatial frequency that does \\(S_1\\), but they produce identical retinal signals (assuming the eye optics is approximated as a pinhole system), so the patterns in \\(S_1\\) and \\(S_2\\) are equally detected. This is captured by the fact that \\(S_1\\) and \\(S_2\\) have the same CPD. In contrast, if we move \\(S_2\\) closer to our eye, we get another signal \\(S_3\\) that has as an identical ordinary spatial frequency as that of \\(S_2\\) but whose pattern is more easily detected. This is adequately captured by the lower CPD in \\(S_3\\).\nThe CSF in Figure 2.13 allows us to study the joint effect of spatial frequency and contrast in detecting a patterned signal. In general, the ability of pattern detection depends on a number of other factors, such as the spatial frequency, eccentricity, color, and temporal frequency (if the stimulus is time-varying) (Mantiuk, Ashraf, and Chapiro 2022; Ashraf et al. 2024). Customarily, this high-dimensional data is plotted as a set of different CSFs, each quantifying the contrast sensitivity as a function of other factors.\nFunctionally, detecting contrast allows us to detect edges and contours: information across the two sides of an edge has the highest contrast. We will see shortly how later processing stages in the HVS leverage the contrasts to extract more specific information from the visual field to aid tasks such as object recognition.\n\n\n\n2.4.3 Light Adaptation\nLooking at Figure 2.12 again, the RGC responses do not change much with uniform illuminations (stimulus 1 and stimulus 3) regardless of the illumination level. This is true for a wide range of illumination levels. In some sense, the RGCs are able to “discount” the ambient light level so that the contrast is reliably encoded at arbitrary light levels. This is called light adaptation.\n\n\n\n\n\n\nFigure 2.15: Illustration of the RGC adaptation. Through the increment-threshold experiment, we show that, over a wide range of the background intensity \\(I_b\\), the threshold \\(\\Delta I\\) needed for the spot light to be detectable is linearly proportional to \\(I_b\\). That is, the minimal detectable contrast \\(\\frac{\\Delta I}{I_b}\\) is roughly constant, a.k.a., the Weber’s law, the result of light adaptation. The extended dashed line shows that the Weber’s law does not hold for all the luminance levels. Enroth-Cugell, Hertz, and Lennie (1977, fig. 6) and Sakmann and Creutzfeldt (1969) report actual data for cat’s RGC.\n\n\n\nFigure 2.15 illustrates an experiment showing the effect of light adaptation. It uses the “increment-threshold” paradigm, where there is a uniform background light with an intensity of \\(I_b\\) and a spot light is superimposed over the background; the spot light has an intensity increment \\(\\Delta I\\) over \\(I_b\\). The entire stimulus (background + spot light) is impinging on the receptive field of an RGC. The goal is to adjust the increment of the spot light so that the RGC’s response reaches a criterion level (e.g., 30 spikes per second). The plot in Figure 2.15 shows the minimal amount of increment (\\(y\\)-axis) under different background intensities (\\(x\\)-axis).\nWe can see that over a wide range of background intensity \\(I_b\\), the threshold \\(\\Delta I\\) needed for the spot light to be detectable is linearly proportional to \\(I_b\\). That is, the minimal detectable (Weber) contrast \\(\\frac{\\Delta I}{I_b}\\) is roughly constant. We could also perform this increment-threshold experiment behaviorally on human participants, through which we can derive the minimal \\(\\Delta I\\) needed for the spot light to be detectable to humans (Blakemore and Rushton 1965; Fuortes, Gunkel, and Rushton 1961; Aguilar and Stiles 1954; Barlow 1957). Perhaps unsurprisingly, the same trend holds: over a rather wide range of background levels, the increment threshold varies linearly with the background intensity. This means, behaviorally, the minimal detectable contrast is also constant, and this constancy could potentially be accounted for by the physiological constancy9.\n\nWeber’s Law Means Desensitization\nMinimally detectable contrast being constant over different background intensities is called the Weber’s law or the “Weber–Fechner law” (Fechner 1860). A direct interpretation of the Weber’s law is that stronger signals are needed at high ambient light levels for a signal to be barely detectable. It is almost like our visual system is desensitized at higher ambient light levels. This desensitization is very well documented for photoreceptors (Matthews et al. 1988; Nakatani and Yau 1988; Fain et al. 2001), and it is unsurprising that photoreceptor desensitization can lead to (although does not fully account for) the desensitization observed in the RGCs and in the behavioral experiments (Dunn, Lankheet, and Rieke 2007).\nThis desensitization allows us to extract contrasts rather than absolute light levels, which is of significant advantage to us. The ambient level varies over several orders of magnitude, but the contrast of a scene is relatively stable regardless of the ambient light level. Consider our ape ancestors who need to find apples from a tree to survive. As the ambient light level increases, both the apple and the tree become brighter, but the contrast is relatively constant. To be able to reliably detect the apple, an ape needs to reliably extract contrast at all light levels but not the absolute light level itself.\n\n\nWeber’s Law Fails at Low and High Intensities\nSharp readers like you have most definitely noticed that Weber’s law does not hold at all background illumination levels (Kolb, Fernandez, and Nelson 2005, pt. VIII Light and Dark Adaptation). The extended dashed line in Figure 2.15 indicates that Weber’s law fails at very low background levels. When the ambient light level is very low, Weber’s law fails because the retinal responses are dominated by noise, both retinal internal noise (called dark light or dark noise) (Barlow 1957; Blakemore and Rushton 1965; Donner 1992) and external photon shot noise (Rose 1948; De Vries 1943). At extremely high background levels, Weber’s law also fails because of photoreceptor saturation. All in all, however, Weber’s law holds reasonably well under a very wide range of normal lighting conditions that we encounter in every life.\n\\[\n    \\Delta I =kI_b,\n\\tag{2.3}\\]\nwhere \\(k\\) is a constant representing how fast the threshold increases with the background and is called the Weber’s constant.\nWhen Equation 2.3 is written in the log-log domain, as is plotted in Figure 2.15, we have:\n\\[\n    \\log(\\Delta I) = \\log(k) + \\log(I_b).\n\\tag{2.4}\\]\nWe can see that in the log-log plot, the Weber’s constant affects the intercept of the threshold-vs-background line (the intersection of the dashed line and the \\(y\\)-axis; not shown in Figure 2.15).\nFor the Weber’s law to hold exactly, the slope of the threshold-vs-background line in the log-log plot must be 1, which is roughly the case in Figure 2.15 (for the range where the relationship is linear). In many measurements, the slope fit from the data is not exactly 1. To account for this, the Weber’s law is extended, phenomenologically, to take the following form, which is also called the Stevens’s power law (Stanley S. Stevens 1957; Stanley Smith Stevens 1961):\n\\[\n\\begin{aligned}\n    \\Delta I =kI_b^d, \\\\\n    \\log(\\Delta I) = \\log(k) + d\\log(I_b),\n\\end{aligned}\n\\tag{2.5}\\]\nwhere \\(d\\) is a free parameter that permits this additional degree of freedom.\n\n\nDark and Chromatic Adaptations\nA concept related to light adaptation is dark adaptation. Dark adaptation deals with the situation where the eye is first exposed to light at a certain level and then the light is removed. We can all tell from experience that our visual sensitivity is terrible when the light is just removed but will improve over time as we spend more time in the dark. Dark adaptation is concerned with quantifying the dynamics of the visual sensitivity recovery at different times in the dark. Once again, dark adaptation can be studied both psychophysically (Hecht, Haig, and Chase 1937; Crawford 1937, 1947) and physiologically (T. D. Lamb and Pugh 2006; T. Lamb and Pugh Jr 2004).\nWhile light and dark adaptations are concerned with visual experiences under different intensity levels, chromatic adaptation is concerned with how our vision adapts to illuminant colors. It turns out that our visual system can pretty reliably discount the color of the lighting illunminating a scene so that an object’s color appears relatively stable under different illuminations. We will study light, dark, and chromatic adaptations in greater depth in Chapter 6.",
    "crumbs": [
      "Human Visual System",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>From Light to Vision</span>"
    ]
  },
  {
    "objectID": "hvs-intro.html#sec-chpt-hvs-percept-postretina",
    "href": "hvs-intro.html#sec-chpt-hvs-percept-postretina",
    "title": "2  From Light to Vision",
    "section": "2.5 Post Retinal Processing",
    "text": "2.5 Post Retinal Processing\nThe signals leaving the retina are first routed to the Lateral Geniculate Nucleus (LGN) and then to the cortex, where vision is formed.\n\n2.5.1 Lateral Geniculate Nucleus\nDifferent classes of RGCs project to distinct LGN layers with virtually the same RFs: midget RGCs project to the Parvocellular layers (P cells) in the LGN (forming the P pathway/stream), parasol RGCs project to the Magnocellular layers (M cells) in the LGN (forming the M pathway/stream), and bistratified RGCs project to the Koniocellular layers (K cells) in the LGN (forming the K pathway/stream).\nSimilar to the RGCs, the LGN neurons also have center-surround receptive fields, and their receptive-field organizations are almost exact copies of that of the corresponding RGCs. This is why, by and large, LGN has been thought to be mainly a relay station, transmitting information from the retina to the brain. Interestingly, the way the LGN relays information to the brain is to gather information from one hemifield and send it to the other side of the cortex.\nIf LGN simply relays information, why does it exist at all? It turns out that LGN receives about 90% of its inputs from the cortex (Sherman and Koch 1986). This is different from the retina, which is a “closed” system that does not receive information from the rest of the brain. The feedback from the brain serves to regulate the visual signals before they are sent to the brain. Higher-order brain regions encode cognitive information such as attention, and one can imagine how attention can be used to influence what subsequent information is sent to the brain (O’Connor et al. 2002). If the brain were to send the feedback signals to the retina, the blind spot would have been 10 times larger, so the LGN seems like a convenient and cost-effective place where the feedback-driven regulation can take place.\n\nAnother Example of Parallel Pathways\nRods vs. cones is an example of parallel pathways in the HVS. The parvocellular vs. magnocellular pathway is another example; they encode different spatial/temporal frequency information. The magnocellular pathway responds to high temporal frequency well, is sensitive to low spatial frequency, and responds strongly to contrast changes. The parvocellular pathway, in large part, behaves oppositely. It is worth noting that these two visual streams start from the retina, where they start from distinct RGC cell types, and remain physically separated all the way into the primary visual cortex V1. This is different from the rod vs. cone pathways, which start at the photoreceptors and merge at the RGC layer.\n\n\n\n2.5.2 Visual Cortex\nOnce in the cortex, the visual signals are first processed in the primary visual cortex, also known as visual area 1 (V1) or the striate cortex. V1 neurons primarily encode edge orientations but are also tuned to edge lengths, object motion direction, and specific colors. David Hubel and Torsten Wiesel were the first to elucidate the responses of V1 neurons and the architecture of V1 in general (Hubel and Wiesel 1959, 1962, 1968)10.\n\nV1 Simple Cells are Orientation Selective\nPerhaps the most striking feature of V1 neurons is that they are orientation selective. The left panel of Figure 2.16 shows the responses of a cat V1 neuron, recorded by Hubel and Wiesel (1959), when presented with a slit of illumination at different orientations. This neuron responds best to a particular orientation (vertical in this case) and responds very weakly, if at all, to other orientations. The right panel in Figure 2.16 plots the neuron responses (spikes/second) as a function of the illumination orientation; a plot like this is called the neuron’s orientation tuning curve.\n\n\n\n\n\n\nFigure 2.16: Left: orientation selectivity of a cat V1 simple cell; from Hubel and Wiesel (1959, fig. 3). Right: orientation tuning curves of two illustrative V1 simple cells (do not necessarily correspond to the experimental data on the left); different cells can have different preferred orientations.\n\n\n\nPerhaps the most striking feature of V1 neurons is that they are orientation selective. The left panel of Figure 2.16 shows the responses of a cat V1 neuron, recorded by Hubel and Wiesel (1959), when presented with a slit of illumination at different orientations. This neuron responds best to a particular orientation (vertical in this case) and responds very weakly, if at all, to other orientations. The right panel in Figure 2.16 plots the neuron responses (spikes/second) as a function of the illumination orientation; a plot like this is called the neuron’s orientation tuning curve.\n\n\n\n\n\n\nFigure 2.17: Left: responses of a V1 simple cell to spot lights at different locations in the receptive field. \\(\\triangle\\): inhibitory areas; \\(\\times\\): excitatory areas. \\(f\\) is when the entire field is illuminated uniformly. Right: the receptive field of the cell. From Hubel and Wiesel (1959, fig. 1).\n\n\n\nWhy would this neuron be tuned to a specific orientation? The reason lies in its receptive field structure. Figure 2.17 shows the response of such a neuron when illuminated with spot lights at different locations. When the neuron is illuminated by spot lights across the vertical axis, it is inhibited, and it is excited when the spot lights are across the horizontal axis. The right panel shows the receptive field of such a neuron, where the skinny, tall central area is inhibited and the flanking areas are excitatory. There are other neurons where the excitatory and inhibitory regions are swapped.\nThis receptive field explains why a neuron could have an orientation selectivity: when the orientation of the stimulus coincides with the excitatory region of the receptive field the neuron is optimally stimulated11. Other orientations would involve both the excitatory and inhibitory regions, reducing or abolishing the response. V1 cells with such a receptive field are called simple cells. Different simple cells might have different preferred orientations; for instance, the first cell in the right panel of Figure 2.16 prefers a 90\\(^{\\circ}\\) orientation.\n\n\n\n\n\n\nFigure 2.18: Bottom: typical receptive-field maps for V1 simple cells (C – G); while there are on and off regions, they are not organized in a center-surround fashion as they are in RGCs/LGN (A and B). Top: multiple center-surround (LGN) neurons synpase with a V1 simple cell, producing the receptive field in C at the bottom. \\(\\triangle\\): inhibitory areas; \\(\\times\\): excitatory areas. Adapted from Hubel and Wiesel (1962, figs. 2, 19).\n\n\n\nC–G in Figure 2.18 illustrate typical receptive fields found in V1 simple neurons. All are oriented (only one orientation is shown) but differ in arrangements. In comparison, A and B show the center-surround receptive fields found in RGCs and LGN neurons. Clearly, center-surround receptive fields simply cannot be orientation selective: try superimposing an edge and rotating it over the center-surround receptive field; will the response change much?\nHow would a V1 simple neuron acquire such an oriented receptive field? This can be explained by looking at how LGN neurons are connected to a V1 simple neuron. The top panel in Figure 2.18 illustrates the model suggested by Hubel and Wiesel (1962), which is supported by later electrophysiological results (Clay Reid and Alonso 1995). Each V1 simple cell synapses with and sums the inputs from multiple LGN neurons (which, recall, also have the center-surround receptive fields as the RGCs), whose receptive fields abut and overlap on the retina, and are arranged in an oblique angle. When those receptive fields all have the same ON-center (or OFF-center) structure, the simple cell would tune for an oblique, elongated edge. Therefore, even if center-surround cells do not have orientation selectivity, V1 simple cells can.\n\n\nDirection, Length, and Binocular Vision Emerge from (Hyper)Complex Cells\nThe majority of neurons in V1 are actually not simple cells. Three-quarters of the V1 neurons are complex cells, which have, well, complex selectivities. Fundamentally, their receptive fields cannot be subdivided into excitatory and inhibitory areas. That is, they do not respond to a spot light no matter where the light is placed in the receptive field. Therefore, their responses to complicated geometries cannot be explained/predicted by their responses to spot lights, unlike those of simple cells.\nThe complex cells are also orientation selective, but unlike simple cells, many complex cells respond only to a properly oriented edge sweeping across the receptive field as if (but not actually) the entire receptive field is excitatory. However, when we present a properly oriented, stationary edge, complex cells do not respond at all, or only weakly, at the onset or the turning off of the edge. This further shows that the responses of complex cells are not a linear superposition of responses to spot lights.\n\n\n\n\n\n\nFigure 2.19: Some V1 complex neurons prefer properly oriented edges sweeping across their receptive field; these neurons also have direction selectivity — even under the same orientation. From Hubel and Wiesel (1968, fig. 2).\n\n\n\nInterestingly, about one-fifth of the complex cells prefer movement in a particular direction, showing the direction selectivity of many complex cells. Hubel and Wiesel (1968) measured the direction selectivity of V1 complex cells in monkeys, and some of the results are shown in Figure 2.19. In this example, the cell is excited by a properly oriented edge moving in upward directions, but not the opposing, orthogonal directions, showing selectivity toward motion directions.\nHubel and Wiesel (1968) also discovered a set of what they call the end-stopping neurons or hypercomplex cells in V1. Those neurons are tuned to properly oriented edges with a specific length, beyond which the neurons are inhibited. These neurons play a role in encoding corners, curvatures, and sudden breaks in lines (Hubel 1995, p. 85).\nFinally, Hubel and Wiesel also found that some V1 neurons respond to stimuli only from the left eye or only from the right eye, a property termed ocular dominance. There are also binocular cells that can be stimulated independently by stimulus from either eye. There cells represent the first stage where information from the left and right hemi-fields converge, which is critical for depth perception.\n\n\n“Be More Specific”\nAn obvious conclusion we can draw from comparing the V1 neurons and the retina/LGN neurons is this: as we progress along the visual pathway, the stimulus we present to the visual system must be more specific. Put another way, our visual system increasingly extracts more specific information as signals progress in the pathway.\nBeing more specific is critical, as that allows us to recognize objects by their subtle details. For instance, the RGCs/LGN neurons provide the contrast/edge detection capability, but virtually any object has contrasts and edges, so they are not terribly useful in recognizing specific objects. The V1 simple neurons, however, allow us to detect orientations, and that is critical to our vision — from orientations we can then infer shapes, as we recognize objects mostly by their shapes.\nCritically, however, the V1 simple neurons offer orientation selectivity precisely because the RGCs/LGN neurons have contrast/edge detection capabilities, as demonstrated in Figure 2.18 (top). This is why we say the early visual system extracts low-level information, but the later visual system extracts high-level information: the former is used as the building blocks by the latter.\n\n\nThe Rest of the Cortex\n\n\n\n\n\n\nFigure 2.20: Once in the cortex, signals are projected from area V1 to other areas, each generally specialized in a particular information process. The two main pathways from V1 are the ventral pathway (“what”) and the dorsal pathway (“where”). There is top-down feedback in the cortex from higher-order areas to lower-order areas. Adapted from Dowling and Dowling Jr (2016, fig. 1.3).\n\n\n\nFrom V1, signals are projected to other areas such as V2, V4, IT, MT, etc. There are two main projection pathways (Nassi and Callaway 2009; Ungerleider and Mishkin 1982; Mishkin, Ungerleider, and Macko 1983), as shown in Figure 2.20. The first is the dorsal pathway, which is concerned with observing objects in space, such as their spatial location and motion, information that is also useful to guide actions (Goodale et al. 1991). Therefore, this pathway is also called the “where/how” pathway. The other is the ventral pathway, or the “what” pathway, that carries information of the details and identity of objects and supports visual functions such as object recognition, facial recognition, and color perception. The two pathways interact. For instance, to guide visual action we not only need to know the position and motion of the objects but also the shape, color, etc.\nThe discussion so far focuses on the bottom-up information flow, the flow of information from lower-order representations in the hierarchy, such as V1, to higher-order representations, such as V4 and beyond. There is also a top-down information flow from the higher regions to the lower regions. This information flow provides feedback information such as attention, knowledge, and expectation to influence the early information processing in the cortex (Gilbert and Li 2013; Briggs 2020). Combining the bottom-up and the top-down flows, the HVS acts essentially as a self-adaptive system that automatically optimizes its performance for a given task.\n\n\n\n\nAguilar, M, and WS Stiles. 1954. “Saturation of the Rod Mechanism of the Retina at High Levels of Stimulation.” Optica Acta: International Journal of Optics 1 (1): 59–65.\n\n\nAngueyra-Aristizábal, Juan M. 2014. “The Limits Imposed in Primate Vision by Transduction in Cone Photoreceptors.” PhD thesis, University of Washington Libraries.\n\n\nAshraf, Maliha, Rafał K Mantiuk, Alexandre Chapiro, and Sophie Wuerger. 2024. “castleCSF—a Contrast Sensitivity Function of Color, Area, Spatiotemporal Frequency, Luminance and Eccentricity.” Journal of Vision 24 (4): 5–5.\n\n\nBarlow, Horace B. 1953. “Summation and Inhibition in the Frog’s Retina.” The Journal of Physiology 119 (1): 69.\n\n\n———. 1957. “Increment Thresholds at Low Intensities Considered as Signal/Noise Discriminations.” The Journal of Physiology 136 (3): 469.\n\n\nBarlow, Horace B, Roo Fitzhugh, and SW Kuffler. 1957. “Change of Organization in the Receptive Fields of the Cat’s Retina During Dark Adaptation.” The Journal of Physiology 137 (3): 338.\n\n\nBerson, David M, Felice A Dunn, and Motoharu Takao. 2002. “Phototransduction by Retinal Ganglion Cells That Set the Circadian Clock.” Science 295 (5557): 1070–73.\n\n\nBlakemore, CB, and WA Rushton. 1965. “Dark Adaptation and Increment Threshold in a Rod Monochromat.” The Journal of Physiology 181 (3): 612.\n\n\nBoettner, Edward A, and J Reimer Wolter. 1962. “Transmission of the Ocular Media.” Investigative Ophthalmology & Visual Science 1 (6): 776–83.\n\n\nBowmaker, JK, HJ Dartnall, JN Lythgoe, and JD Mollon. 1978. “The Visual Pigments of Rods and Cones in the Rhesus Monkey, Macaca Mulatta.” The Journal of Physiology 274 (1): 329–48.\n\n\nBriggs, Farran. 2020. “Role of Feedback Connections in Central Visual Processing.” Annual Review of Vision Science 6 (1): 313–34.\n\n\nCaerbannog. 2016. “Comparison of structures in vertebrate’s eye (left) with octopus’ eye (right); CC BY-SA 3.0 license.” https://en.wikipedia.org/wiki/Blind_spot_(vision)#/media/File:Evolution_eye_2.svg.\n\n\nClay Reid, R, and Jose-Manuel Alonso. 1995. “Specificity of Monosynaptic Connections from Thalamus to Visual Cortex.” Nature 378 (6554): 281–84.\n\n\nCrawford, BH. 1937. “The Change of Visual Sensitivity with Time.” Proceedings of the Royal Society of London. Series B-Biological Sciences 123 (830): 69–89.\n\n\n———. 1947. “Visual Adaptation in Relation to Brief Conditioning Stimuli.” Proceedings of the Royal Society of London. Series B-Biological Sciences 134 (875): 283–302.\n\n\nCurcio, Christine A, Kenneth R Sloan, Robert E Kalina, and Anita E Hendrickson. 1990. “Human Photoreceptor Topography.” Journal of Comparative Neurology 292 (4): 497–523.\n\n\nDacey, Dennis M, Hsi-Wen Liao, Beth B Peterson, Farrel R Robinson, Vivianne C Smith, Joel Pokorny, King-Wai Yau, and Paul D Gamlin. 2005. “Melanopsin-Expressing Ganglion Cells in Primate Retina Signal Colour and Irradiance and Project to the LGN.” Nature 433 (7027): 749–54.\n\n\nDacey, Dennis M, and Michael R Petersen. 1992. “Dendritic Field Size and Morphology of Midget and Parasol Ganglion Cells of the Human Retina.” Proceedings of the National Academy of Sciences 89 (20): 9666–70.\n\n\nDartnall, Herbert JA, James K Bowmaker, and John Dixon Mollon. 1983. “Human Visual Pigments: Microspectrophotometric Results from the Eyes of Seven Persons.” Proceedings of the Royal Society of London. Series B. Biological Sciences 220 (1218): 115–30.\n\n\nDe Vries, HL. 1943. “The Quantum Character of Light and Its Bearing Upon Threshold of Vision, the Differential Sensitivity and Visual Acuity of the Eye.” Physica 10 (7): 553–64.\n\n\nDerrington, AM, and P Lennie. 1984. “Spatial and Temporal Contrast Sensitivities of Neurones in Lateral Geniculate Nucleus of Macaque.” The Journal of Physiology 357 (1): 219–40.\n\n\nDo, Michael Tri Hoang, and KW Yau. 2010. “Intrinsically Photosensitive Retinal Ganglion Cells.” Physiological Reviews.\n\n\nDonner, Kristian. 1992. “Noise and the Absolute Thresholds of Cone and Rod Vision.” Vision Research 32 (5): 853–66.\n\n\nDowling, John E, and Joseph L Dowling Jr. 2016. Vision: How It Works and What Can Go Wrong. MIT Press.\n\n\nDunn, Felice A, Martin J Lankheet, and Fred Rieke. 2007. “Light Adaptation in Cone Vision Involves Switching Between Receptor and Post-Receptor Sites.” Nature 449 (7162): 603–6.\n\n\nEnroth-Cugell, Christina, B Gevene Hertz, and P Lennie. 1977. “Cone Signals in the Cat’s Retina.” The Journal of Physiology 269 (2): 273–96.\n\n\nFain, Gordon L, Hugh R Matthews, M Carter Cornwall, and Yiannis Koutalos. 2001. “Adaptation in Vertebrate Photoreceptors.” Physiological Reviews 81 (1): 117–51.\n\n\nFechner, Gustav Theodor. 1860. Elemente Der Psychophysik. Vol. 2. Breitkopf u. Härtel.\n\n\nFuortes, MGF, RD Gunkel, and WAH Rushton. 1961. “Increment Thresholds in a Subject Deficient in Cone Vision.” The Journal of Physiology 156 (1): 179.\n\n\nGilbert, Charles D, and Wu Li. 2013. “Top-down Influences on Visual Processing.” Nature Reviews Neuroscience 14 (5): 350–63.\n\n\nGlassner, Andrew S. 1995. Principles of Digital Image Synthesis. Elsevier.\n\n\nGoodale, Melvyn A, A David Milner, Lorna S Jakobson, and David P Carey. 1991. “A Neurological Dissociation Between Perceiving Objects and Grasping Them.” Nature 349 (6305): 154–56.\n\n\nHartline, H Keffer. 1938. “The Response of Single Optic Nerve Fibers of the Vertebrate Eye to Illumination of the Retina.” American Journal of Physiology-Legacy Content 121 (2): 400–415.\n\n\n———. 1939. “Excitation and Inhibition of the\" Off\" Response in Vertebrate Optic Nerve Fibers.” Am. J. Physiol 126:527.\n\n\n———. 1940a. “The Effects of Spatial Summation in the Retina on the Excitation of the Fibers of the Optic Nerve.” American Journal of Physiology-Legacy Content 130 (4): 700–711.\n\n\n———. 1940b. “The Receptive Fields of Optic Nerve Fibers.” American Journal of Physiology-Legacy Content 130 (4): 690–99.\n\n\n———. 1949. “Inhibition of Activity of Visual Receptors by Illuminating Nearby Retinal Areas in the Limulus Eye.” Federation Proceedings 8 (1): 69.\n\n\nHartline, H Keffer, and Clarence Henry Graham. 1932. “Nerve Impulses from Single Receptors in the Eye.” Journal of Cellular & Comparative Physiology.\n\n\nHartline, H Keffer, Henry G Wagner, and Floyd Ratliff. 1956. “Inhibition in the Eye of Limulus.” The Journal of General Physiology 39 (5): 651–73.\n\n\nHattar, Samer, H-W Liao, Motoharu Takao, David M Berson, and KW Yau. 2002. “Melanopsin-Containing Retinal Ganglion Cells: Architecture, Projections, and Intrinsic Photosensitivity.” Science 295 (5557): 1065–70.\n\n\nHecht, Selig, Charles Haig, and Aurin M Chase. 1937. “The Influence of Light Adaptation on Subsequent Dark Adaptation of the Eye.” The Journal of General Physiology 20 (6): 831–50.\n\n\nHodgkin, Alan L, and Andrew F Huxley. 1952. “A Quantitative Description of Membrane Current and Its Application to Conduction and Excitation in Nerve.” The Journal of Physiology 117 (4): 500.\n\n\nHubel, David H. 1995. Eye, Brain, and Vision. Scientific American Library/Scientific American Books.\n\n\nHubel, David H, and Torsten N Wiesel. 1959. “Receptive Fields of Single Neurones in the Cat’s Striate Cortex.” J Physiol 148 (3): 574–91.\n\n\n———. 1962. “Receptive Fields, Binocular Interaction and Functional Architecture in the Cat’s Visual Cortex.” The Journal of Physiology 160 (1): 106.\n\n\n———. 1968. “Receptive Fields and Functional Architecture of Monkey Striate Cortex.” The Journal of Physiology 195 (1): 215–43.\n\n\nIdrees, Saad, Michael B Manookin, Fred Rieke, Greg D Field, and Joel Zylberberg. 2024. “Biophysical Neural Adaptation Mechanisms Enable Artificial Neural Networks to Capture Dynamic Retinal Computation.” Nature Communications 15 (1): 5957.\n\n\nJacobs, Gerald H. 2008. “Primate Color Vision: A Comparative Perspective.” Visual Neuroscience 25 (5-6): 619–33.\n\n\nKolb, Helga, Eduardo Fernandez, and Ralph Nelson. 2005. “The Organization of the Retina and Visual System.” Webvision-the Organization of the Retina and Visual System.\n\n\nKuffler, Stephen W. 1952. “Neurons in the Retina: Organization, Inhibition and Excitation Problems.” In Cold Spring Harbor Symposia on Quantitative Biology, 17:281–92. Cold Spring Harbor Laboratory Press.\n\n\n———. 1953. “Discharge Patterns and Functional Organization of Mammalian Retina.” Journal of Neurophysiology 16 (1): 37–68.\n\n\nLamb, TD, and Edward N Pugh Jr. 2004. “Dark Adaptation and the Retinoid Cycle of Vision.” Progress in Retinal and Eye Research 23 (3): 307–80.\n\n\nLamb, Trevor D, and Edward N Pugh. 2006. “Phototransduction, Dark Adaptation, and Rhodopsin Regeneration: The Proctor Lecture.” Investigative Ophthalmology & Visual Science 47 (12): 5138–52.\n\n\nLaValle, Steven M. 2023. Virtual Reality. Cambridge university press.\n\n\nLazzerini Ospri, Lorenzo, Glen Prusky, and Samer Hattar. 2017. “Mood, the Circadian System, and Melanopsin Retinal Ganglion Cells.” Annual Review of Neuroscience 40 (1): 539–56.\n\n\nLiao, Fuyou, Zheng Zhou, Beom Jin Kim, Jiewei Chen, Jingli Wang, Tianqing Wan, Yue Zhou, et al. 2022. “Bioinspired in-Sensor Visual Adaptation for Accurate Perception.” Nature Electronics 5 (2): 84–91.\n\n\nMantiuk, Rafał K, Maliha Ashraf, and Alexandre Chapiro. 2022. “stelaCSF: A Unified Model of Contrast Sensitivity as the Function of Spatio-Temporal Frequency, Eccentricity, Luminance and Area.” ACM Transactions on Graphics (TOG) 41 (4): 1–16.\n\n\nMatthews, HR, RLW Murphy, GL Fain, and TD Lamb. 1988. “Photoreceptor Light Adaptation Is Mediated by Cytoplasmic Calcium Concentration.” Nature 334 (6177): 67–69.\n\n\nMishkin, Mortimer, Leslie G Ungerleider, and Kathleen A Macko. 1983. “Object Vision and Spatial Vision: Two Cortical Pathways.” Trends in Neurosciences 6:414–17.\n\n\nNakatani, K, and KW Yau. 1988. “Calcium and Light Adaptation in Retinal Rods and Cones.” Nature 334 (6177): 69–71.\n\n\nNassi, Jonathan J, and Edward M Callaway. 2009. “Parallel Processing Strategies of the Primate Visual System.” Nature Reviews Neuroscience 10 (5): 360–72.\n\n\nO’Connor, Daniel H, Miki M Fukui, Mark A Pinsk, and Sabine Kastner. 2002. “Attention Modulates Responses in the Human Lateral Geniculate Nucleus.” Nature Neuroscience 5 (11): 1203–9.\n\n\nPurves, Dale, George J. Augustine, David Fitzpatrick, William Hall, Anthony-Samuel LaMantia, Richard D. Mooney, Michael L. Platt, and Leonard E. White. 2017. Neurosciences. 6th ed. Oxford University Press.\n\n\nRose, Albert. 1948. “The Sensitivity Performance of the Human Eye on an Absolute Scale.” Journal of the Optical Society of America 38 (2): 196–208.\n\n\nSakmann, Bert, and Otto D Creutzfeldt. 1969. “Scotopic and Mesopic Light Adaptation in the Cat’s Retina.” Pflügers Archiv 313:168–85.\n\n\nSelket. 2007. “The ventral vs. dorsal stream; CC BY-SA 3.0.” https://commons.wikimedia.org/wiki/File:Ventral-dorsal_streams.svg.\n\n\nSherman, SM, and Christof Koch. 1986. “The Control of Retinogeniculate Transmission in the Mammalian Lateral Geniculate Nucleus.” Experimental Brain Research 63:1–20.\n\n\nStevens, Stanley S. 1957. “On the Psychophysical Law.” Psychological Review 64 (3): 153.\n\n\nStevens, Stanley Smith. 1961. “To Honor Fechner and Repeal His Law: A Power Function, Not a Log Function, Describes the Operating Characteristic of a Sensory System.” Science 133 (3446): 80–86.\n\n\nUngerleider, Leslie G, and Mortimer Mishkin. 1982. “Two Cortical Visual Systems.” In Analysis of Visual Behavior, edited by David J Ingle, Melvyn A Goodale, Richard JW Mansfield, et al., 549–86. Mit Press Cambridge, MA.\n\n\nWald, George. 1968. “Molecular Basis of Visual Excitation.” Science 162 (3850): 230–39.\n\n\nWandell, Brian A. 1995. Foundations of Vision. Sinauer Associates.\n\n\nWodnicki, Robert, Gordon W Roberts, and Martin D Levine. 1995. “A Foveated Image Sensor in Standard CMOS Technology.” In Proceedings of the IEEE 1995 Custom Integrated Circuits Conference, 357–60. IEEE.",
    "crumbs": [
      "Human Visual System",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>From Light to Vision</span>"
    ]
  },
  {
    "objectID": "hvs-intro.html#footnotes",
    "href": "hvs-intro.html#footnotes",
    "title": "2  From Light to Vision",
    "section": "",
    "text": "The power at an infinitesimal point is called irradiance; see Chapter 8.↩︎\nCajal shared the Nobel Prize in 1906 with Camillo Golgi, who invented a method that Cajal used to study neuronal connections.↩︎\nwhich were first recorded by Edgar Adrian, a Nobel Prize laureate in 1932 who developed the all-or-none theory of action potentials; Hodgkin and Huxley (Hodgkin and Huxley 1952), who shared the Nobel Prize in 1963, explained the ionic mechanisms underlying the action potentials.↩︎\nGeorge Wald won his Nobel Prize in 1967 by essentially elucidating this process.↩︎\n\\(\\text{absorbance} = \\log(I_{\\text{incident}}/I_{\\text{transmitted}})\\), and the fraction absorbed, i.e., \\(\\text{absorptance} = 1 - I_{\\text{transmitted}}/I_{\\text{incident}}\\). Therefore, \\(\\text{absorptance} = 1-e^{-\\text{absorbance}}\\). Numerically, absorbance is approximately absorption when absorbance is low, which is the case here when using MSP to illuminate the photoreceptors.↩︎\nHartline he won the Nobel Prize in 1967 because of this discovery.↩︎\nThis is actually a parvocellular LGN neuron, which is directly projected from the midget RGC and shares the same receptive field with that of the midget RGC.↩︎\nThe implicit assumption here is that once the RGC responses reach a criterion level, the pattern becomes subjectively detectable at the behavioral level.↩︎\nWe emphasize “potentially” because while correlation is easy to establish, claiming causation requires ruling out other factors.↩︎\nThey shared the Nobel Prize in 1981.↩︎\nNote that the receptive field in Figure 2.17 has an inhibitory central region and excitatory flanking areas, but the receptive field of the neuron in Figure 2.16 evidently has an opposite excitatory vs. inhibitory regions, so the two figures do not share the same underlying data.↩︎",
    "crumbs": [
      "Human Visual System",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>From Light to Vision</span>"
    ]
  },
  {
    "objectID": "hvs-receptor.html",
    "href": "hvs-receptor.html",
    "title": "3  Photoreceptors",
    "section": "",
    "text": "3.1 Counting Photons: Principle of Univariance\nIn this chapter we will review how photoreceptors work. We will start from the highest level of abstraction, where photoreceptors can be seen as photon counting devices. The photon counting capability is fundamentally a result of the photoreceptors’ ability to absorb photons (selectively) over wavelengths. We will then lower the level of abstraction, reviewing in detail how a photon is absorbed in a photoreceptor, a.k.a., the phototransduction process and its recovery. Understanding photoreceptors at this low level, in turn, allows us to reason about key high-level behaviors of vision, such as sensitivity and saturation.\nAnatomically, a photoreceptor has two parts: an inner segment and an outer segment. Photons enter from the inner segment, which for the most part can be thought of as a waveguide that funnels the photons to the outer segment. The outer segment contains the photon-absorbing pigments. This is illustrated in Figure 3.1.\nConceptually, we can think of each photoreceptor as a bucket that collects photons. There are millions of buckets sitting on the retina, taking a shower of photons. Many photons entering the eye will not hit any bucket: they are absorbed before they reach the bucket (e.g., by the lens). Any photon that does hit the bucket has a certain probability of being absorbed. The absorption probability varies with the photoreceptor type and the photon’s wavelength.\nFundamentally, a photoreceptor can absorb photons because it contains light-sensitive, photon-absorbing pigments, each of which is able to absorb one photon. Each rod photoreceptor has tens of millions of such pigments (Milo and Phillips 2015, p. 142–47; Nathans 1992). Why is a photon’s absorption probability not 100% once it enters a photoreceptor? For one, a photon might not meet a photopigment as it travels through the photoreceptor before the exit. Even if a photon hits a pigment, its absorption is still probabilistic, as absorption is dictated fundamentally by quantum mechanics.\nOnce a photon is absorbed, it has a certain probability of “exciting” or “isomerizing” the pigment. A pigment excitation or isomerization generates a certain level of electrical signal — in the form of a current or voltage change across the cell membrane of the photoreceptor. The excitation probability once a photon is absorbed is called the quantum efficiency of the pigment. Quantum efficiency is about two-thirds in the visible spectrum and is not wavelength sensitive (H. J. A. Dartnall 1972; Kropf 1982; Fu 2010). A pigment excitation is also called pigment bleach, since the pigment after excitation is no longer responsive to light as if it is bleached.\nThe process of initiating an electrical response upon a photon absorption is called phototransduction. We will study this process in detail shortly in Section 3.4. For now, it is important to know that for a given photoreceptor class, the electrical response caused by a photon absorption is constant regardless of the photon’s wavelength. This is called the Principle of Univariance (W. A. Rushton 1972; W. H. Rushton 1972; Naka and Rushton 1966): each photon that generates an electrical response has the same effect as any other photon that does so. In other words, the only effect that wavelength has is to impact the probability a photon gets absorbed; after absorption, the wavelength information is lost.\nA crucial implication of this principle is that any two lights that are equally absorbed/excited will be seen as the same light by the human vision. For the purpose of comparing two lights, we can think of each bucket as having a counter; every time a photon is absorbed and excites a pigment, the counter gets incremented by 1. If two lights lead to the same counter value, they are perceptually the same. Crucially, if a bucket’s counter is, say, twice as high as another’s, it does not mean the electrical responses produced by the first photoreceptor are twice as high as that of the second. We will see why this is the case shortly.",
    "crumbs": [
      "Human Visual System",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Photoreceptors</span>"
    ]
  },
  {
    "objectID": "hvs-receptor.html#sec-chpt-hvs-receptor-counting",
    "href": "hvs-receptor.html#sec-chpt-hvs-receptor-counting",
    "title": "3  Photoreceptors",
    "section": "",
    "text": "Figure 3.1: Anatomical structures of rods and cones. The outer segment contains photon-absorbing photopigments, which of which has the capability of absorbing a photon and being excited/isomerized after absorption. Adapted from Ivo Kruusamägi (2010) and Kosigrim (2007).",
    "crumbs": [
      "Human Visual System",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Photoreceptors</span>"
    ]
  },
  {
    "objectID": "hvs-receptor.html#sec-chpt-hvs-receptor-absorb",
    "href": "hvs-receptor.html#sec-chpt-hvs-receptor-absorb",
    "title": "3  Photoreceptors",
    "section": "3.2 Spectral Absorbance, Absorptance, and Sensitivity",
    "text": "3.2 Spectral Absorbance, Absorptance, and Sensitivity\nTo understand vision in everyday scenarios, what we care about is not the probability of how a single photon is absorbed and excites a pigment, but the collective behavior of a flux of photons that enter our eyes. Conveniently, when we have a large population of photons, the probability that an individual photon causes an excitation translates to the percentage of incident photons that are absorbed and/or cause excitations. In fact, the percentage of absorption is the quantity that we can directly measure.\nThere are in general two ways to estimate the absorption rate of a flux of photons. The direct way of measurement is using a technique called microspectrophotometry (MSP). The indirect method is to measure the electrical responses using, e.g., a suction electrode, and then estimate the absorption. The indirect method gives us only relative absorption across wavelengths, while the MSP approach gives us absolute absorption.\n\n3.2.1 Absorbance Spectra from Microspectrophotometry\nThe idea of MSP is to shine a beam of light through a photoreceptor and then measure, at the other side, the percentage of photons that are transmitted, i.e., unabsorbed (J. Bowmaker 1984). Ignoring back-scattering1, photon absorption by photopigments in a photoreceptor can be modeled using the Beer-Lambert law. The transmittance at wavelength \\(\\lambda\\) is defined as the ratio between the amount of transmitted photons \\(I_d(\\lambda)\\) and the amount of total incident photons \\(I_0(\\lambda)\\):\n\\[\n    T(\\lambda) = \\frac{I_d(\\lambda)}{I_0(\\lambda)} = e^{-\\epsilon(\\lambda) c l},\n\\]\nwhere \\(\\epsilon(\\lambda)\\) is called the absorption coefficient and is wavelength dependent, \\(c\\) is the concentration (the number of pigments per unit volume), and \\(l\\) is the optical length, the length through which a photon has to travel2. Both \\(c\\) and \\(l\\) are inherent properties of a photoreceptor and are not dependent on photon wavelength.\nAbsorptance \\(a(\\lambda)\\), the percentage of absorption at \\(\\lambda\\), is naturally \\(1-T(\\lambda)\\). Absorbance \\(A(\\lambda)\\) (also called the optical density), whose spelling is subtly and annoyingly different from that of absorptance, is defined as:\n\\[\n    A(\\lambda) = -\\ln(T(\\lambda)) = \\epsilon(\\lambda) c l.\n\\tag{3.1}\\]\nTherefore, absorptance \\(a(\\lambda)\\) and absorbance \\(A(\\lambda)\\) are related by:\n\\[\n    a(\\lambda) = 1 - e^{-A(\\lambda)}.\n\\tag{3.2}\\]\n\n\n\n\n\n\nFigure 3.2: Left: Log-scaled spectral sensitivities of the cones (L, M, S) in a macaque measured using the suction electrode method; adapted from Baylor, Nunn, and Schnapf (1987, fig. 3A). Right: Absorbance spectra of the three cones (L, M, S) and the rod (R) in a human, measured using MSP; adapted from H. J. Dartnall, Bowmaker, and Mollon (1983, fig. 2). The spectra in both cases are normalized to peak at 1/100.\n\n\n\nWe would repeat this experiment over a frequency range and obtain the axial absorbance at each sampled frequency. The resulting plot is usually called the absorbance spectrum. One such example is shown in the right panel in Figure 3.2, measured by H. J. Dartnall, Bowmaker, and Mollon (1983) on humans. Much such data has been obtained in the literature, the earliest of which is perhaps by George Wald and his colleagues (Marks, Dobelle, and MacNichol Jr 1964; Brown and Wald 1964) who identified three distinct absorbance spectra in cone-mediated vision and, thus, provided direct physiological evidence for the existence of three classes of cones. The three cone types are generally referred to as the L, M, and S cones, since their absorbances peak at, relatively, long, medium, and short wavelengths.\nDecades before the work by Wald et al., Ragnar Granit measured the spectral sensitivities of the retinal ganglion cells (Granit 1941, 1943, 1945b, 1945a)3. Granit showed the existence of two classes of RGCs: 1) one that has a broader spectral sensitivity whose peak shifts to shorter wavelengths from photopic vision to scotopic vision, and 2) one whose spectral sensitivities are narrower and fall generally into three main groups. The former is the physiological version of the Purkinje shift that we will discuss in Section 4.3.2, and provides direct evidence for the convergence of the rod and the cone vision pathways (Section 2.4.1.4 and Figure 2.4). The latter is the first direct evidence of the existence of three dinstinct wavelength encoding mechanisms (albeit not at the photoreceptor level), essential for the trichromatic color vision. \n\nNormalization\nQuite often, the absorbance spectrum is normalized to peak at unity, as is the case in Figure 3.2 (right). According to Equation 12.9, normalizing absorbance across different wavelengths is equivalent to normalizing \\(\\epsilon\\) across wavelengths, since \\(c\\) and \\(l\\) are not wavelength specific, whereas \\(\\epsilon\\) is. \\(c\\) and \\(l\\) might vary across species and across individuals and might differ between different illumination methods (see below), but \\(\\epsilon\\), which is fundamental to the photopigment, does not. Therefore, the normalized absorbance spectrum tells us something fundamental about the wavelength sensitivity of the photopigments.\nPerhaps a subtlety but quite confusing when perusing the literature, the maximum absolute absorbance across all wavelengths (i.e., the peak of an absorbance spectrum) is usually simply called the optical density; “peak optical density” would have been more accurate, as optical density is wavelength specific. Using the peak optical density and the normalized absorbance spectrum, we can reconstruct the absolute absorbance spectrum; from there we can get the absolute absorptance spectrum.\nWhile not shown here, the peak absorbance between rods and cones is not that different. The peak absorbance of rods is about 0.475, and the value is about 0.375 for foveal S cones and 0.525 for foveal L/M cones (J. Bowmaker et al. 1978; J. K. Bowmaker and Dartnall 1980). The large sensitivity difference between rod vision and cone vision is not primarily attributed to the difference in their ability to absorb photons. \n\n\nCorrecting for Transverse Illuminations\nThere is one more complication. With MSP, we illuminate a photoreceptor transversely, i.e., the light passes from one side of the photoreceptor to the other side. In reality, when a photon enters a photoreceptor, it travels axially from the inner segment through the outer segment. The main difference between these two scenarios is the optical length that a photon has to travel. A photoreceptor is tall and skinny, so its width is much smaller than its length, about 2.5 \\(\\mu\\text{m}\\) wide and 35 \\(\\mu\\text{m}\\) long for a fovea L/M cone (Polyak 1941). \nTherefore, we need to first calculate the absorbance per unit length, called the specific absorbance, and then scale the specific absorbance by the axial length of the photoreceptor to obtain the axial absorbance, from which we can estimate the axial absorptance using Equation 12.9. This is shown below (omitting \\(\\lambda\\) for simplicity):\n\\[\n\\begin{aligned}\n    A_{\\text{transverse}} &= -\\ln(p_{\\text{transverse}}) = \\epsilon c l_{\\text{transverse}} \\\\\n    A_{\\text{specific}} &= \\frac{A_{\\text{transverse}}}{l_{\\text{transverse}}} = \\epsilon c \\\\\n    A_{\\text{axial}} &= A_{\\text{specific}} l_{\\text{axial}} \\\\\n    T_{\\text{axial}} &= e^{-A_{\\text{axial}}}\n\\end{aligned}\n\\tag{3.3}\\]\n\n\n\n3.2.2 Relative Absorptance Spectra from Electrical Responses\nAnother method is by measuring the photoreceptor’s electrical responses across different wavelengths using techniques such as suction electrode, which records the electrical responses of an isolated photoreceptor sucked into a micropipette. Spectra so estimated are usually called the spectral sensitivity of the photoreceptors in the literature, but as we will see, they are equivalent to the normalized absorptance spectra.\n\n\n\n\n\n\nFigure 3.3: Each curve shows the electrical response (in photovoltage; \\(y\\)-axis) as a function of light intensity (\\(x\\)-axis) at a given wavelength. To estimate the relative sensitivity/absorptance between two wavelengths, we laterally shift one curve so that it coincides with the other. Given the Principle of Univariance, the amount of shift is proportional to the relative absorptance. From Baylor and Hodgkin (1973, fig. 8).\n\n\n\nFigure 3.3 illustrates the idea, where Baylor and Hodgkin (1973) measured the peak electrical response of a turtle cone (in photovoltage; \\(y\\)-axis) as a function of light intensity (\\(x\\)-axis) at two wavelengths: 644 \\(\\text{nm}\\) and 539 \\(\\text{nm}\\). We see that the curves are almost parallel to each other: we can laterally move the 644 \\(\\text{nm}\\) curve to coincide with that of 539 \\(\\text{nm}\\). In this case, we have to shift the former by 1.27 log units.\nThink for a second what this means. When we increase the intensity at 644 \\(\\text{nm}\\) by a factor of \\(10^{1.27}\\), the sheer number of photons absorbed at 644 \\(\\text{nm}\\) is increased by a factor of \\(10^{1.27}\\), too. That means without scaling the number of incident photons at 644 \\(\\text{nm}\\) is \\(10^{-1.27}\\) (about 5.4%) of that at 539 \\(\\text{nm}\\). Even with just 5.4% of incident photons, 644 \\(\\text{nm}\\) light is able to produce the same level of electrical response, i.e., cause the same amount of photon absorption (given the Principle of Univariance), as the light at 539 \\(\\text{nm}\\). Therefore, we can say the absorption rate (absorptance) at 644 \\(\\text{nm}\\) is \\(10^{1.27}\\) higher than of that at 539 \\(\\text{nm}\\).\nWe repeat this experiment across other wavelengths and obtain the relative spectral sensitivity/absorptance spectra. The left panel in Figure 3.2 shows one such example obtained by Baylor, Nunn, and Schnapf (1987) on macaque cones. Critically, the absorptance so obtained is relative: we do not know the absolute absorptance at either wavelength. The \\(y\\)-axis is necessarily normalized to peak at unity. Similar data have been collected on humans as well (Schnapf, Kraft, and Baylor 1987; Kraft, Schneeweis, and Schnapf 1993).\n\nNormalized Spectra From the Two Methods Match Well\nIt is worth noting that the suction electrode method also uses transversely illuminated lights. Since the absorptance obtained here is relative, we cannot easily use the method in Equation 3.3 to obtain the absorptance spectra for the axial illumination.\nHere is the catch. Numerically, \\(1 - e^{-A} \\approx A\\) when \\(A\\) is small. When we illuminate a photoreceptor transversely, the optical length \\(l\\) is the photoreceptor width, which is short, which means \\(A\\) is small (Wyszecki and Stiles 1982, p. 588, 594). Therefore, the transverse absorbance and transverse absorptance are approximately equal. So the normalized absorptance spectrum given by the suction electrode method is approximately the normalized absorbance spectrum, as is shown experimentally (Schnapf, Kraft, and Baylor 1987; Stockman and Sharpe 2000, fig. 11).\nThe normalized absorbance spectra are still not sufficient, since to use the method in Equation 3.3 we need to know the absolute absorbance spectra. People usually have to resort to another data source that provides absolute peak absorbance or fit the data against, e.g., psychophysical measurements that provide some form of absolute measures (Kraft, Schneeweis, and Schnapf 1993; Baylor, Nunn, and Schnapf 1984, 1987).",
    "crumbs": [
      "Human Visual System",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Photoreceptors</span>"
    ]
  },
  {
    "objectID": "hvs-receptor.html#sec-chpt-hvs-receptor-fundamentals",
    "href": "hvs-receptor.html#sec-chpt-hvs-receptor-fundamentals",
    "title": "3  Photoreceptors",
    "section": "3.3 Cone Fundamentals: Cornea-Referred Spectral Sensitivities",
    "text": "3.3 Cone Fundamentals: Cornea-Referred Spectral Sensitivities\nOur discussions so far have focused on absorption by the photoreceptors, but for a flux of photons arriving at the cornea about to enter our eye, they are also absorbed even before reaching the photoreceptors. Accounting for these pre-receptoral filters is important to model human vision. Spectral sensitivities that account for these pre-receptoral filters are what we call the cornea-referred spectral sensitivities.\n\n3.3.1 Cone Fundamentals From Physiology\nThere are two such pre-receptoral filters: the ocular media (Section 2.2.2) and the macular pigments, which are located at a small area in the fovea. Macular pigments absorb light presumably to counter some of the aberrations from the ocular media and to protect the retina from light damage (Snodderly et al. 1984). Both ocular media and macular pigments absorb light selectively over the spectrum, just like photoreceptors do.\nWe can model \\(E(\\lambda)\\), the fraction of photons arriving at the cornea that are absorbed by the photoreceptors:\n\\[\n    E(\\lambda) = l(\\lambda) m(\\lambda) a(\\lambda),\n\\]\nwhere \\(a(\\lambda)\\) represents the photoreceptor absorptance spectrum, \\(l(\\lambda)\\) and \\(m(\\lambda)\\) represent ocular and macular transmittance spectrum, respectively, i.e., the fraction of photons at \\(\\lambda\\) unabsorbed by the ocular media (e.g., lens) (Boettner and Wolter 1962; Norren and Vos 1974) and the macular pigments. Figure 3.4 illustrates this process.\n\n\n\n\n\n\nFigure 3.4: Cornea-referred spectral sensitivity function measures the percentage of photons arriving at the cornea (about to enter the eye) that are absorbed by each photoreceptor type at each wavelength. The cone versions of this are called the cone fundamentals and are visualized in the bottom right. The sensitivity metric relates to the rate of pigment excitation only by a constant scaling factor (i.e., the photoreceptor quantum efficiency, which is about two-thirds). It is a product of ocular transmittance (top left), macular transmittance (top right), and photoreceptor absorptance (bottom left). The macular transmittance and lens transmittance are from Stockman, Sharpe, and Fach (1999); the cone absorptance spectra are estimated from Stockman and Sharpe (2000) (different from the H. J. Dartnall, Bowmaker, and Mollon (1983) data used in Figure 2.8; see the methodology in CVRL (n.d.)).\n\n\n\n\nWe call \\(E(\\lambda)\\) the cornea-referred spectral sensitivity function, since it is calculated with respect to the incident lights at the cornea surface. When \\(a(\\lambda)\\) is replaced by the absorptance spectra of the three classes of cones, the resulting sensitivity functions are more commonly referred to as the cone fundamentals.\nWe can make a few general observations about the cone fundamentals. First, the cone sensitivity drops to 0 beyond the 380 \\(\\text{nm}\\) and 780 \\(\\text{nm}\\) range, a range we usually call the visible spectrum, since there will be no pigment excitation beyond that range: lights beyond that range are invisible. Second, S cones are generally the least sensitive of the three cone types, but it is not because of the photoreceptors but because of the pre-receptoral filters, which absorb mostly low-wavelength lights. Finally, the sensitivity peaks of the L cones and M cones are very similar (off by about 20 \\(\\text{nm}\\)), but both are rather far from the peak of the S cones. We have discussed the reason behind this when discussing Figure 2.8.\n\n\n3.3.2 Cone Fundamentals From Psychophysics\nThe spectral sensitivities discussed above are measured physiologically. We can also measure such functions through psychophysics using the increment-threshold method. A typical set up is one where there is a uniform background illumination and a spot light superimposed at the center of the background. We ask a participant to adjust a knob to control the intensity of the spot light so that it is just noticeable from the background. The sensitivity is then defined as the reciprocal of the threshold intensity. We repeat this experiment for each sampled wavelength across the spectrum to obtain a sensitivity curve. This method is first used in the pioneering work done by W.S. Stiles (Stiles 1939, 1959, 1964), and is adopted in virtually all later work (Wald 1964; Smith and Pokorny 1975; Stockman, Sharpe, and Fach 1999; Stockman and Sharpe 2000).\nA curious question is how we can separate the sensitivity of different photoreceptor types, given that the spectral sensitivities of the four photoreceptor types overlap. There are two methods to isolate rods from cones. We can either use very dim lights, to which cone responses are too small to contribute to vision (Crawford 1949), or we could measure from people with rod monochromacy — individuals who have only rods. When measuring cone sensitivities, we will use intense lights that almost completely saturate rods.\nIsolating the three cone types from each other is generally challenging with individuals with normal vision. W.S. Stiles’ initial work (Stiles 1939, 1959) designed special conditions of background illumination to suppress the sensitivity of two unwanted cone types while sparing the one under study. Modern studies usually turn to color-deficient individuals who lack one or two cone types. Isolating S cones is done by measuring from S-cone monochromats (Stockman, Sharpe, and Fach 1999). Isolating L and M cones is challenging because individuals with only L or M cones are very rare and the spectral sensitivities of the L and M cones overlap substantially. Instead, a common approach is to resort to Protanopes and Deuteranopes; the former has only M and S cones, and the latter has only L and S cones. To isolate M (L) cones from the S cones, we measure from Protanopes (Deuteranopes) using lights that have high spatial and/or temporal frequencies, to which S cones are known to be insensitive (Stockman and Sharpe 2000; Smith and Pokorny 1975).\n\n\n3.3.3 Physiological and Psychophysical Sensitivities Match Well\nWe can then compare the spectral sensitivity data from physiology and from psychophysics. This is shown in Figure 3.5. See the figure caption for details. Overall it is fair to say that the two sets of data match well.\n\n\n\n\n\n\nFigure 3.5: Left: comparison of the physiologically-obtained rod spectral sensitivity from macaques (open circles) and the rod spectral sensitivity from human psychophysics (filled circles); adapted from Baylor (1987, fig. 14), which is after Baylor, Nunn, and Schnapf (1984, fig. 5). Right: comparison of the spectral sensitivity of the three cone types between physiology data (open circles; from macaques) and psychophysics data (curves; from humans); adapted from Baylor, Nunn, and Schnapf (1987, fig. 5). The human psychophysics data is from Stiles (1959) for cones and from Crawford (1949) for rods.\n\n\n\nThink about what this comparison means. What we measure in psychophysics is the threshold intensity (at each wavelength) needed to evoke a criterion level of human behavioral response (i.e., just noticeability). The threshold intensity in the physiological measurement represents how much light is needed (at each wavelength) to cause the same amount of pigment absorption and, by the Principle of Univariance, the amount of electrical responses. The fact that the two sets of data match suggests that the amount of electrical response we need to evoke a just-noticeable level of perception is a constant regardless of wavelength, a perhaps unsurprising inference.\nInterestingly, the physiological data in Figure 3.5 is obtained from macaques, and the psychophysical data is from humans. The fact that they match well suggests the similarities of the visual system among primates — we have come a long way since the monkey days, but our photoreceptors have not changed much. Other studies obtaining the physiological sensitivity data from humans show similarly good matches with human psychophysical data (Crescitelli and Dartnall 1953; Kraft, Schneeweis, and Schnapf 1993; J. K. Bowmaker and Dartnall 1980; Mollon 1982).",
    "crumbs": [
      "Human Visual System",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Photoreceptors</span>"
    ]
  },
  {
    "objectID": "hvs-receptor.html#sec-hvs-receptor-cascade",
    "href": "hvs-receptor.html#sec-hvs-receptor-cascade",
    "title": "3  Photoreceptors",
    "section": "3.4 Beyond Counting Photons: Phototransduction and Recovery",
    "text": "3.4 Beyond Counting Photons: Phototransduction and Recovery\nThe discussion so far about (spectral) sensitivity has focused on the ability of different types of photoreceptors to absorb (“count”) photons at different wavelengths, and by that measure, rods and cones are not that different: their peak absorbances differ by less than 10% (Section 3.2.1). So then why do we say rod vision is more sensitive than cone vision? What we have ignored is the absolute strength of the electrical response once photons are absorbed. To understand the absolute response strength, we need to understand the cellular and molecular processes underlying how electrical responses are actually produced from pigment excitations. These processes constitute the so-called phototransduction cascade and are the focus of this section. George Wald is largely credited for elucidating these processes (Wald 1933, 1968).\n\n3.4.1 Phototransduction Cascade\nThe phototransduction cascades in rods and (different types of) cones are exactly the same. The differences appear to be quantitative rather than qualitative and are dictated by the genes expressing the isoforms of the molecules participating in phototransduction (Ingram, Sampath, and Fain 2016; Yau 1994). We will mainly use rods as an example to drive the discussion here.\n\nPhotoreceptor Has a Stable Transmembrane Current in Dark\nA visual pigment in a rod is a special molecule called a rhodopsin, which has two parts: a long strand of protein called opsin (which is insensitive to light) and the light-sensitive 11-cis retinal, a form of Vitamin A as discovered by Wald (1933), that is attached to the opsin and acts as a chromophore. Hofmann and Lamb (2023) presents a comprehensive survey of what is known about rhodopsin to date.\n\n\n\n\n\n\nFigure 3.6: cGMP-gated ion channels keep a transmembrane current of about -34 pA for each photoreceptor in the dark. The current is called the dark current, and its negative sign indicates that the direction of the current is inward. Adapted from Rodieck (1998, p. 172).\n\n\n\nIn the dark, there is a stable current of -34 pA that flows into the outer segment called the dark current. By convention, inward current is defined as negative. This is illustrated in Figure 3.6. The dark current is a result of a particular kind of cation-selective ion channel that is permeable to both Na+ and Ca2+ flowing into the outer segment. Critically, these channels are ligand-gated channel — gated by cyclic guanosine monophosphate (cGMP) molecules (Fesenko, Kolesnikov, and Lyubarsky 1985; Yau and Baylor 1989). Think of cGMPs as the guards of the channels; on average, each channel needs three cGMPs to remain open (Rodieck 1998, p. 169). In a rod outer segment in the dark, there is an ample amount (3-4 micromoles) of cGMPs, which bind to a large amount of channels and keep them open.\nAs a result of the dark current, the membrane potential of a photoreceptor in the dark is about -35 mV. If you are familiar with basic neuroscience, you would notice that the photoreceptor in the dark is depolarized, since the membrane potential is higher than that of the resting potential of a typical neuron. The depolarization is exactly caused by the inward flow of cations through the cGMP-gated channels.\n\n\nClosing Ion Channels Produces Electrical Responses\nOnce a photon is absorbed and, with a two-thirds chance, excites a pigment (dictated by quantum efficiency, as discussed in Section 3.1), the 11-cis retinal of the pigment changes to its isomer called all-trans retinal (which will later be separated from the pigment). This is why a pigment excitation is often called a photoisomerization. This all takes place remarkably fast: the absorption takes about 3 fs and the photoisomerization takes about 200 fs (Gruhl et al. 2023; Rodieck 1998, p. 162).\n\n\n\n\n\n\nFigure 3.7: Steps involved in phototransduction. A single excited rhodopsin pigment can close about 2% of the cGMP-gated ion channels, introducing a photocurrent (change in the transmembrane current) of about 0.7 pA; photocurrent is always positive, whereas the actual transmembrane current is always negative. Adapted from Rodieck (1998, p. 166–72).\n\n\n\nThe isomerization changes the conformation of a rhodopsin pigment, which becomes “activated”: it diffuses randomly and activates a transducin (a form of G protein) whenever they meet. This is illustrated by step ① in Figure 3.7. On average, an activated pigment activates about 700-800 transducins (Purves et al. 2017, p. 243; Rodieck 1998, p. 170). Each activated G protein then meets and binds to a molecule called “cGMP-specific phosphodiesterase” (PDE), activating the PDE. This is step ② in Figure 3.7. Each activated PDE has the ability to catalyze the hydrolysis of several dozen cGMPs (Foster Rieke and Baylor 1998), reducing the cGMP concentrations. This is step ③ in Figure 3.7.\nWe know that the ion channels that induce dark current are gated by cGMPs. A reduction in cGMP concentration will close some of these channels, reducing the dark current and increasing the membrane potential (i.e., the photoreceptor hyperpolarizes). This membrane current/potential change is the electrical response of photon absorptions and is the signal that will be delivered to the rest of the visual system to eventually give rise to vision.\n\nAt the peak of this transduction process, a single activated rhodopsin in a rod can reduce the number of cGMPs by about 1,400, which translates to about 2% closure of the cGMP-gated channels (Rodieck 1998, p. 170). Since each cGMP-gated channel carries the same amount of current, this means the total membrane current is reduced by about 2%. This is step ④ in Figure 3.7. In the literature, the change of membrane current and voltage potential is usually termed photocurrent and photovoltage, respectively. Since the change in the membrane current is positive and the change in the membrane voltage is negative, the photocurrent is positive and the photovoltage is negative. The actual transmembrane current and voltage are always negative. Some electrophysiological techniques measure the total membrane current/voltage, while others measure the photocurrent/photovoltage. Be careful of what quantity is being reported when perusing the literature.\n\n\n\n3.4.2 Deactivation of Phototransduction and Pigment Regeneration\nIf phototransduction continues without any hindrance, eventually 1) all pigments will be bleached (isomerized), and 2) all the ion channels will be open. If so, the photoreceptor and, ultimately, our visual system will not be able to respond to further lights: additional photons cannot be absorbed, and even if they are absorbed and excite pigments, there are no ion channels to close, and so no electrical response will be produced; at that point, our visual system saturates.\nIn order for our visual system to continue to respond to lights, two things must take place. First, there must be mechanisms to terminate the phototransduction and re-open the ion channels (Burns and Arshavsky 2005; Burns and Baylor 2001). Second, new pigments must be continuously regenerated. This is the job of the retinoid cycle or the visual cycle.\n\nPhototransductions are Continuously Being Deactivated\n\n\n\n\n\n\nFigure 3.8: There are activities constantly at work to deactivate the phototransduction. This can be seen by observing the response kinetics to a flash light (left) and to a step light (right). Without the deactivation activities, the responses to the flash light would not have been eliminated, and the responses to the step light would not have reached an equilibrium. Left: macaque rods; adapted from Baylor, Nunn, and Schnapf (1984, fig. 1); Right: macaque M cones; adapted from Schnapf et al. (1990, fig. 7A).\n\n\n\nThere are activities constantly at work attempting to terminate the phototransduction. This can be seen by observing the response kinetics of a photoreceptor to light. The left panel in Figure 3.8 shows the photocurrent kinetics of macaque rods in response to a flash light (Baylor, Nunn, and Schnapf 1984); the right panel shows the photocurrent kinetics of macaque M cones when presented with a step light, i.e., a constant background illumination (Schnapf et al. 1990). Without the deactivation activities, the responses to the flash light would not have been eliminated after the flash light was removed, and the responses to the step light would not have reached an equilibrium.\nFor the phototransduction cascade to terminate, there are mechanisms to deactivate every step in the transduction: activated rhodopsins must be deactivated so that they cannot activate more G proteins, activated G proteins must come off the PDE so that the PDE cannot hydrolyze more cGMPs, the cGMPs must be replenished, and the cGMP-gated ion channels must reopen. Every step must be deactivated; for instance, it is not sufficient to just deactivate the pigments: that just means the inactivated pigments will not activate more G proteins, but existing G proteins that are still activated will continue activating PDEs and the rest of the phototransduction.\n\n\n\n\n\n\nFigure 3.9: Steps involved in pigment deactivation and regeneration. An activated pigment is first deactivated, at which point the 11-cis retinal falls off the opsin and is transported to the RPE to be re-synthesized back to all-trans retinal. Top: adapted from Goldstein (2009, fig. 3.20); Bottom right: from Rodieck (1998, p. 511); R* denotes activated pigments, RK denotes rhodopsin kinase, and A denotes arrestin.\n\n\n\nThe deactivation also involves a set of biochemical reactions that we will not detail here, but just to give you a flavor, here is how the pigments are deactivated, and the process is illustrated in Figure 3.9. An enzyme, rhodopsin kinase (RK), binds to and phosphorylates an activated pigment. Another protein called arrestin (A) then binds to phosphorylated pigment, which inhibits the pigment’s ability to activate G proteins, essentially deactivating the pigment.\n\n\nDeactivation is Accelerated by Negative Feedbacks\nInterestingly, some of the deactivation steps are accelerated by negative feedback mechanisms mediated by Ca2+ concentration. One such mechanism is shown in Figure 3.10. Let’s briefly take a look at this, not only because it is a classical example of the dynamics common in visual neuroscience (and many dynamical systems) but also because we will come back to this negative feedback when we discuss light adaptation later in the class.\n\n\n\n\n\n\nFigure 3.10: Left: the deactivation is accelerated by the negative feedback from Ca2+. Right: the kinetics of macaque L cones; the negative feedback is so strong in cones that the response kinetics have undershoots; from Baylor (1987, fig. 11).\n\n\n\nThe closing of cGMP-gated channels from phototransduction reduces the inward flows of Ca2+ and Na+ to the outer segment. Importantly, Ca2+ inhibits guanylate cyclase (GC), which inherently re-synthesizes cGMPs. As the Ca2+ level reduces, the cGMP re-synthesis rate increases, which replenishes cGMPs and re-opens ion channels. That is, phototransduction initially reduces the cGMP concentration, and the very reduction of concentration serves to replenish the cGMPs — the feedback is negative. Without this negative feedback, the response still would have stabilized. For instance, soon after we move the flash light the cGMP concentration will stop falling while the GC-induced resynthesis is steadily going on. Eventually, the cGMP concentration will be restored to the original level4. The negative feedback simply accelerates this process.\nThe strength of the negative feedback is much stronger in cones than in rods (Yau and Hardie 2009; Burns and Baylor 2001). The stronger negative feedback is the reason why cone responses recover much faster than do rods: compare the kinetics of macaque rods under flash lights of varying intensities in Figure 3.8 (left) and the macaque L cone kinetics under the same set of lights in Figure 3.10 (right), and pay attention to the timescale on the \\(x\\)-axis. In cones, the negative feedback is so strong that there is actually a temporary over-provision of cGMPs during the deactivation phase, leading to an undershoot in the current (right panel in Figure 3.10).\nThe negative feedback through Ca2+ concentration also accelerates other steps in deactivation, including accelerating pigment deactivation (although with a much less potent effect than that on the GC (Nikonov, Lamb, and Pugh Jr 2000; Pugh Jr, Nikonov, and Lamb 1999)) and increasing the sensitivity of cGMP-gated channels to cGMPs (so that the channels can be open even at lower cGMP levels) (Hsu and Molday 1993).\n\n\nPigments are Continuously Being Regenerated\nDeactivating phototransduction is not enough; it reopens ion channels and replenishes all the materials involved in phototransduction — except the pigments themselves. Pigments must somehow be restored so that they are available for phototransduction again, and this is the job of the retinoid cycle. An activated pigment roams about randomly and gets deactivated when it meets rhodopsin kinase and arrestin. When an activated pigment is deactivated, the all-trans retinal falls off the pigment. This is shown in the top panel in Figure 3.9. The all-trans retinals then leave the photoreceptor and are transported to the Retinal Pigment Epithelium (RPE), which is a layer of special skin cells just outside the retina and is where all-trans retinals are converted back to 11-cis retinals, which are then transported back to the outer segment, recombining with the opsin portion of the pigment, at which point the pigment is reconstituted to its original form and is sensitive to photons again. This is shown in the bottom panel in Figure 3.9.\n\n\n\n\n\n\nFigure 3.11: Bleach all pigments using intense light, then remove the light and measure the fraction of available/bleached pigments vs. time in the dark. Left: human rod kinetics; adapted from W. A. H. Rushton (1963, fig. 3); Right: human cone kinetics; adapted from W. A. H. Rushton (1965, fig. 3).\n\n\n\nRushton used retinal densitometry to measure the pigment regeneration of both cones and rods in the living eye. The kinetics of the rod and cone pigment regenerations are shown in Figure 3.11. The half-life of of cone pigments regeneration is about 3 times shorter than that of rod pigments (W. A. H. Rushton 1961, 1963, 1965).  The fast regeneration of cone pigments means it is unlikely that the steady-state cone pigment bleaching level exceeds about 90% under the normal range of illumination levels throughout a day (Burns and Lamb 2014, p. 15). In contrast, under normal daylight, rod pigments are almost all bleached.\nCompared to the speed of pigment generation, the deactivation of phototransduction takes place much more rapidly. On average, a cone pigment is regenerated in about 2 minutes (Rodieck 1998, p. 184–85), and phototransduction deactivation (and activation for that matter) happens in millisecond scale. Imagine in your visual field there is a brief flash. You see the flash as a flash because the phototransduction initiated by the flash quickly goes away almost immediately as the flash is removed.\n\n\nSteady Vision Means Equilibrium\nBoth pigment regeneration and phototransduction deactivation are constantly at work, countering the effect of the phototransduction in light. The more pigments are excited and the more ion channels are open, the stronger the deactivation and pigment regeneration processes are.\nUnder a modest background illumination, the opposing forces reach an equilibrium where the rate of closing cGMP-gated ion channels matches that of re-opening them. So dynamically a fixed number, not all, of the ion channels are open. If after light exposure we move to a dark room, there are no photons coming in, so there is no phototransduction. The countering forces completely dominate, and eventually all the materials involved in phototransduction are replenished and all the ion channels are open — another equilibrium. If we flash a light on top of the background, some cGMP channels open for a short period of time and then close as the flash goes away; our vision goes back to the steady state.\nIf we increase the intensity of the steady background a little, again some cGMP channels will open, and simultaneously the countering forces are at work trying to close them. Eventually a new equilibrium is reached where more channels are steadily open than before. What if we keep increasing the background light’s intensity? Every time we intensify the background light a little, we reach a new equilibrium with fewer channels steadily open. This is readily seen in the right panel in Figure 3.8, where the steady-state response increases (i.e., fewer channels are open) as the background light intensity increases. Eventually all the channels will close, and our vision is said to be saturated. This is what we will study next.",
    "crumbs": [
      "Human Visual System",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Photoreceptors</span>"
    ]
  },
  {
    "objectID": "hvs-receptor.html#absolute-sensitivity-and-saturation-in-rods-vs.-cones",
    "href": "hvs-receptor.html#absolute-sensitivity-and-saturation-in-rods-vs.-cones",
    "title": "3  Photoreceptors",
    "section": "3.5 Absolute Sensitivity and Saturation in Rods vs. Cones",
    "text": "3.5 Absolute Sensitivity and Saturation in Rods vs. Cones\nNow that we understand phototransduction and its recovery, we can appreciate some fundamental differences between rods and cones in their absolute sensitivity and saturation levels, which shape our daily visual experience.\n\n3.5.1 Rods Have a Much Higher Signal-to-Noise Ratio Than Cones\nPsychophysical experiments show that humans can reliably detect a flash when only about 5 to 7 pigments are excited in a field of about 500 rods; in contrast, it takes about 5 pigment excitations per cone in a pool of about 10 cones for humans to signal a flash (Hecht, Shlaer, and Pirenne 1942; Barlow 1956; Donner 1992; Angueyra-Aristizábal 2014). Assuming the visual system requires the same level of electrical response to see a flash, the difference suggests that rods are able to produce the same amount of electrical response using fewer pigment excitations than cones.\nPart of this can be explained by the different levels of neural convergence in the rod and cone pathway, which we have discussed before. But the difference in sensitivity between the photoreceptors themselves also plays a significant role: rod photoreceptors have a higher electrical response and a lower noise floor than cones. This contributes to the lower detection threshold in rod vision. Let’s examine the signal and the noise separately.\n\nSingle Photon Response is Much Larger in Rods than in Cones\nThe photocurrent, i.e., the signal part of the SNR, from a single rod pigment excitation is about 20 times higher than that from a cone pigment excitation (34 pA vs. 0.7 pA) (Baylor, Lamb, and Yau 1979b; Baylor, Nunn, and Schnapf 1984; Baylor 1987; Schnapf et al. 1990; Ingram, Sampath, and Fain 2016; Angueyra-Aristizábal 2014). A single rod pigment excitation already closes about 2% of the ion channels in a rod, and about 30 pigment excitations would close half of the ion channels in a rod in the dark. In contrast, it requires about 650 pigment excitations in a cone photoreceptor to provide a half-maximal response in dark (Baylor, Nunn, and Schnapf 1984; Schnapf et al. 1990). \nPart of the reason why rods have larger responses than cones is because the phototransduction cascade in rods is much more rapid (Ingram, Sampath, and Fain 2016): the rate of PDE activation is higher, the rate of cGMP concentration reduction is higher, etc. As a result, a lot of cGMP-gated ion channels already close before much of the “countering forces” (that are simultaneously trying to deactivate phototransductions) kick in, resulting in higher peak responses in rods than in cones.\n\n\nCones are Much Noisier than Rods\nNot only do rods produce a higher response per pigment excitation, but the inherent noise in a rod is much lower than that in a cone. Photocurrents exist even in the dark due to noise, which comes from two main sources (Baylor, Matthews, and Yau 1980; Fred Rieke and Baylor 2000): 1) the spontaneous, thermal-induced activations of pigments, which show up as discrete spikes in photocurrents, and 2) the spontaneous activation of PDEs, which causes the cGMP concentration to fluctuate and shows up as the continuous rumbling of the photocurrent (F. Rieke and Baylor 1996). Photocurrent in the dark has the equivalent effect of a background illumination in the dark, so it is also termed dark noise, dark light, or “eigengrau” (German: one’s own light). As we can imagine, dark noise interferes with light detection when the signal is weak, i.e., illumination is dim (Barlow 1957), where behaviorally we cannot tell for certain if we are seeing actual light or dark light.\nDark noise is much higher in cones than in rods (Angueyra-Aristizábal 2014; Donner 1992). For instance, the dark light in rods is equivalent to about one pigment excitation every 90 seconds (Baylor, Nunn, and Schnapf 1984) and about 500 – 1,000 times per second in cones (Schnapf et al. 1990; Schneeweis and Schnapf 1999; Fred Rieke and Baylor 2000; Burns and Lamb 2014; Foster Rieke and Baylor 1998).  The dark light in cones is high enough to allow a rod to reach its half-maximum response (Tamura, Nakatani, and Yau 1991; Nakatani and Yau 1988; Matthews et al. 1988)!  As a result, the SNR in cones is much lower than that in rods. \n\n\n\n3.5.2 Rods Saturate Much More Easily Than Cones\nWe know the rod-mediated vision saturates under much lower light levels than does the cone-mediated vision. Because of this, rods are not useful in mediating vision in typical daylight illuminations. Cones are much harder to saturate, if ever, under normal daylight (Burns and Lamb 2014; Barlow 1972; Shevell 1977), so they are primarily responsible for daylight vision. But why do the rod-mediated vision saturate more easily, other than the higher degree of neural convergence? Again, the difference in the photoreceptors themselves plays a role.\nFirst, the photocurrent generated by a pigment excitation is much higher in rods than in cones, as discussed before. Second, the phototransduction kinetics is faster in cones than in rods. As we have seen in Figure 3.8, the electrical response of a pigment excitation is an event that does not finish instantaneously: it takes time for the photocurrent to rise, reach its peak, and then decay. The rising phase, as discussed in Section 3.5.1, is briefer in rods than in cones, but the decay phase is much longer in rods than in cones (Ingram, Sampath, and Fain 2016). The faster decay is due to the stronger phototransduction deactivation in cones than in rods (Yau and Hardie 2009; Burns and Baylor 2001) (which is attributed to the stronger negative feedbacks as discussed in Section 3.4.2), which means ion channels are more rapidly restored. Overall, the duration of a cone phototransduction is about four times shorter than that for a rod (100 ms vs. 400 ms) (Baylor 1987; Angueyra-Aristizábal 2014; Nakatani and Yau 1988; Rodieck 1998, p. 185).\nWhy does the time duration matter? It is because a longer duration integrates responses of more incoming photons. Consider an example where two excitations are taking place, say, 200 ms apart in a photoreceptor. In a rod, the time durations of these two excitations overlap, so the total electrical response the photoreceptor generates is greater than that if only one excitation takes place. In a cone, however, these two events do not overlap, and so their effects do not add up.\nAs a side, the slow kinetics of rods is the reason we cannot sense object movement very well in low light conditions. If you have ever played basketball at night, you would know that even though you can tell where the ball is when it is still, but you cannot track the movement of the ball well.\nIt is worth noting that even though cone pigments regenerate much faster than rods (Section 3.4.2), the slower pigment regeneration rate unlikely affects the saturation in rods. There are tens of millions of pigments in a mammalian rod cell (Nathans 1992; Milo and Phillips 2015, p. 142–47) but a rod is almost saturated by only several hundreds of pigment excitations, so when a rod is saturated, the vast majority of pigments are still available (Lamb 1980). It is the fact that almost all cGMP-gated ion channels are closed, rather than all pigments are bleached, that prevents rods from responding further to lights. The slow regeneration rate in rods does affect dark adaptation time, which we will discuss later in the class.",
    "crumbs": [
      "Human Visual System",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Photoreceptors</span>"
    ]
  },
  {
    "objectID": "hvs-receptor.html#sec-hvs-receptor-intensity",
    "href": "hvs-receptor.html#sec-hvs-receptor-intensity",
    "title": "3  Photoreceptors",
    "section": "3.6 Response vs. Light Intensity",
    "text": "3.6 Response vs. Light Intensity\nThe Principle of Univariance tells us that the electrical response from a photon absorption is constant without regard to the photon wavelength, but it does not tell us how the magnitude of the electrical response varies with the number of photons absorbed. With the basic understanding of phototransduction, we can now turn to this question. You might be tempted to think that the relationship is linear, and you would be wrong!\n\n\n\n\n\n\nFigure 3.12: Left: the electrical response (photocurrent) kinetics of macaque rods under varying flash light intensities (quantified by the total number of pigment excitations/isomerizations); from Baylor, Nunn, and Schnapf (1984, fig. 1). Right: the peak response (normalized to maximum response) as a function of flash intensity; from Rodieck (1998, p. 178), which uses the underlying data in the left plot. The tangential line shows that only 47 pigment excitations would saturate the photoreceptor if the photoreceptor does not desensitize.\n\n\n\n\n3.6.1 Peak Response is Not Linearly Proportional to Light Intensity\nFigure 3.12 (left) shows the response (photocurrents) kinetics of macaque rods under flash lights of different intensities. The right plot shows the peak response (normalized to the maximum response) as a function of flash intensity.\nIf the relationship between the response magnitude and the light intensity were linear, the curve would be a straight line. But in reality, we can see that the response grows quickly initially, but the growth slows down soon. What does the actual relationship tell us about photoreceptors? Let’s define the photoreceptor’s sensitivity, or its response rate, to flash lights as the additional response per unit increment in light intensity.\nThe sensitivity/response rate is given by the derivative of the curve, i.e., the slope at every point on the curve. Evidently, the response rate slows down as light becomes more intense; in other words, the photoreceptor becomes less sensitive as light becomes more intense. While the discussions here focus on photoreceptor responses to flash lights, the conclusion holds for responses to steady background lights as well.\nThis non-linear relationship can be used to explain our brightness perception. Our perceived brightness is not linear with respect to the light intensity. Imagine you walk into a dark room and turn one light on; the perceived brightness changes a lot (literally from 0 to 1); then you turn another light on and another light on; every time you turn on an additional light, your perceived brightness increases, but not as much as before. As you continue, the additional brightness you feel from turning one additional light on becomes smaller: you probably would not notice it if someone turned on one more light when there are 1,000 lights on already.\nThis non-linear relationship between perceived brightness and absolute light power is important when deciding how to effectively allocate digital bits when encoding pixel values. A classic example is the gamma encoding/compression in the popular sRGB color space, a topic we will turn to in Section 5.3.2.\n\n\n3.6.2 Why and How Do Photoreceptors Desensitize?\nThe reduction of sensitivity under stronger lights is called desensitization, and is stereotypical of photoreceptor light adaptation. A curious question is, does desensitization provide us any benefits? Do we not want our photoreceptors to be more sensitive to light? Without desensitization, i.e., if the initial response rate was maintained, the rod would saturate at about 47 pigment excitations, as shown in Figure 3.12. The desensitization allows the photoreceptors to extend their operating range, which, in turn, allows our vision to operate at higher light levels.\nWhat mechanisms cause photoreceptor desensitization? We will leave a thorough discussion for later in the class when we actually discuss adaptation, but briefly, there are two reasons.\nThe first reason is the natural exponential decay you would observe in pretty much any dynamical system. Recall that the reason a photoreceptor can produce electrical responses is because of the sequence of biochemical reactions, which require a bunch of materials, like the cGMPs, PDEs, etc., to bump into each other. Under stronger lights, the concentration of these materials is lower, which means they are less likely to meet each other. That in turn means the rate of cGMP concentration reduction becomes even slower, and the ion channels close even less frequently.\nThe second reason, first experimentally shown by Matthews et al. (1988) and Nakatani and Yau (1988), has to do with negative feedbacks regulated by Ca2+ ions. Interestingly, these negative feedback mechanisms are exactly the same as those that accelerate phototransduction deactivation, as discussed in Section 3.4.2. As we will discuss more quantitatively in the adaptation chapter (Section 6.1), this is not a coincidence: desensitization and faster recovery kinetics are two hallmarks of photoreceptor light adaptation.\n\n\n3.6.3 Linear Range\nIf you observe Figure 3.12 (right) closely, you will see that the response vs. flash intensity is linear when the lights are dim. This linear relationship is used to estimate the single photon response: we cannot easily measure the response of a single photon as it is difficult to precisely deliver just one single photon, but this linear relationship in the dim range allows us to estimate such a response by scaling the response of a dim light by its intensity.\nMathematically, the response vs. intensity relationship is modeled in literature either by a negative exponential function (when negative feedbacks are weak) (Baylor, Nunn, and Schnapf 1984; Lamb, McNaughton, and Yau 1981; Kraft, Schneeweis, and Schnapf 1993) or by the Michaelis equation (when negative feedbacks are not negligible) (Baylor and Fuortes 1970; Baylor, Hodgkin, and Lamb 1974; Baylor, Lamb, and Yau 1979a; Normann and Perlman 1979; Fain 1976; Schneeweis and Schnapf 1995; Ingram, Sampath, and Fain 2016). The negative exponential model would look something like \\(r/r_{max} = 1 - e^{-ki}\\), where \\(r\\), \\(r_{max}\\), and \\(i\\) denote the response, maximum response, and flash light intensity, respectively; \\(k\\) is a constant fit to data. The Michaelis equation, which is also called the Naka–Rushton equation (presumably because Naka and Rushton (1966) was the first to use it), looks like \\(r/r_{max} = \\frac{i}{i+\\sigma}\\), where \\(\\sigma\\) is a constant fit to data. Sometimes it would fit the data better to use the generalized Michaelis equation, which takes the form \\(r/r_{max} = \\frac{i^n}{i^n+\\sigma^n}\\), where \\(n\\) is the additional parameter that can fit to data. Both models can be approximated by a linear function when \\(i\\) is small. This linear region perhaps also explains why the sRGB encoding is a piece-wise function where the encoding is linear when light levels are low (Section 5.3.2).\n\n\n\n\nAngueyra-Aristizábal, Juan M. 2014. “The Limits Imposed in Primate Vision by Transduction in Cone Photoreceptors.” PhD thesis, University of Washington Libraries.\n\n\nBarlow, Horace B. 1956. “Retinal Noise and Absolute Threshold.” Josa 46 (8): 634–39.\n\n\n———. 1957. “Increment Thresholds at Low Intensities Considered as Signal/Noise Discriminations.” The Journal of Physiology 136 (3): 469.\n\n\n———. 1972. “Dark and Light Adaptation: Psychophysics.” In Visual Psychophysics, edited by Matthew Alpern, 1–28. Springer.\n\n\nBaylor, Denis A. 1987. “Photoreceptor Signals and Vision: The Proctor Lecture.” Investigative Ophthalmology & Visual Science 28 (1): 34–49.\n\n\nBaylor, Denis A, and MGF Fuortes. 1970. “Electrical Responses of Single Cones in the Retina of the Turtle.” The Journal of Physiology 207 (1): 77–92.\n\n\nBaylor, Denis A, and Alan L Hodgkin. 1973. “Detection and Resolution of Visual Stimuli by Turtle Photoreceptors.” The Journal of Physiology 234 (1): 163–98.\n\n\nBaylor, Denis A, AL Hodgkin, and TD Lamb. 1974. “The Electrical Response of Turtle Cones to Flashes and Steps of Light.” The Journal of Physiology 242 (3): 685–727.\n\n\nBaylor, Denis A, TD Lamb, and KW Yau. 1979a. “The Membrane Current of Single Rod Outer Segments.” The Journal of Physiology 288 (1): 589–611.\n\n\nBaylor, Denis A, Trevor D Lamb, and KW Yau. 1979b. “Responses of Retinal Rods to Single Photons.” The Journal of Physiology 288 (1): 613–34.\n\n\nBaylor, Denis A, G Matthews, and KW Yau. 1980. “Two Components of Electrical Dark Noise in Toad Retinal Rod Outer Segments.” The Journal of Physiology 309 (1): 591–621.\n\n\nBaylor, Denis A, BJ Nunn, and JL Schnapf. 1984. “The Photocurrent, Noise and Spectral Sensitivity of Rods of the Monkey Macaca Fascicularis.” The Journal of Physiology 357 (1): 575–607.\n\n\n———. 1987. “Spectral Sensitivity of Cones of the Monkey Macaca Fascicularis.” The Journal of Physiology 390 (1): 145–60.\n\n\nBoettner, Edward A, and J Reimer Wolter. 1962. “Transmission of the Ocular Media.” Investigative Ophthalmology & Visual Science 1 (6): 776–83.\n\n\nBowmaker, James K, and HJk Dartnall. 1980. “Visual Pigments of Rods and Cones in a Human Retina.” The Journal of Physiology 298 (1): 501–11.\n\n\nBowmaker, JK. 1984. “Microspectrophotometry of Vertebrate Photoreceptors: A Brief Review.” Vision Research 24 (11): 1641–50.\n\n\nBowmaker, JK, HJ Dartnall, JN Lythgoe, and JD Mollon. 1978. “The Visual Pigments of Rods and Cones in the Rhesus Monkey, Macaca Mulatta.” The Journal of Physiology 274 (1): 329–48.\n\n\nBrown, Paul K, and George Wald. 1964. “Visual Pigments in Single Rods and Cones of the Human Retina.” Science 144 (3614): 45–52.\n\n\nBurns, Marie E, and Vadim Y Arshavsky. 2005. “Beyond Counting Photons: Trials and Trends in Vertebrate Visual Transduction.” Neuron 48 (3): 387–401.\n\n\nBurns, Marie E, and Denis A Baylor. 2001. “Activation, Deactivation, and Adaptation in Vertebrate Photoreceptor Cells.” Annual Review of Neuroscience 24 (1): 779–805.\n\n\nBurns, Marie E, and Trevor D Lamb. 2014. “Visual Transduction by Rod and Cone Photoreceptors.” In The New Visual Neurosciences. MIT Press.\n\n\nCrawford, BH. 1949. “The Scotopic Visibility Function.” Proceedings of the Physical Society. Section B 62 (5): 321.\n\n\nCrescitelli, Frederick, and Herbert JA Dartnall. 1953. “Human Visual Purple.” Nature 172 (4370): 195–97.\n\n\nCVRL. n.d. “Photopigment curves based on the Stockman & Sharpe (2000) cone fundamentals.” http://www.cvrl.org/database/text/pigments/ssabance.htm.\n\n\nDartnall, Herbert J. A. 1972. “Photosensitivity.” In Photochemistry of Vision, edited by Herbert J. A. Dartnall, 122–45. Springer Berlin Heidelberg.\n\n\nDartnall, Herbert JA, James K Bowmaker, and John Dixon Mollon. 1983. “Human Visual Pigments: Microspectrophotometric Results from the Eyes of Seven Persons.” Proceedings of the Royal Society of London. Series B. Biological Sciences 220 (1218): 115–30.\n\n\nDonner, Kristian. 1992. “Noise and the Absolute Thresholds of Cone and Rod Vision.” Vision Research 32 (5): 853–66.\n\n\nFain, Gordon L. 1976. “Sensitivity of Toad Rods: Dependence on Wave-Length and Background Illumination.” The Journal of Physiology 261 (1): 71–101.\n\n\nFesenko, Evgeniy E, Stanislav S Kolesnikov, and Arkadiy L Lyubarsky. 1985. “Induction by Cyclic GMP of Cationic Conductance in Plasma Membrane of Retinal Rod Outer Segment.” Nature 313 (6000): 310–13.\n\n\nFu, Yingbin. 2010. “Phototransduction in Rods and Cones.” In WebVision: The Organization of the Retina and Visual System, edited by Helga Kolb, Eduardo Fernandez, and Ralph Nelson.\n\n\nGoldstein, E. Bruce. 2009. Sensation and Perception. 8th ed. Cengage Learning.\n\n\nGranit, Ragnar. 1941. “The Retinal Mechanism of Color Reception.” Journal of the Optical Society of America 31 (9): 570–80.\n\n\n———. 1943. “A Physiological Theory of Colour Perception.” Nature 151 (3818): 11–14.\n\n\n———. 1945a. “The Colour Receptors of the Mammalian Retina.” Journal of Neurophysiology 8 (3): 195–210.\n\n\n———. 1945b. “The Electrophysiological Analysis of the Fundamental Problem of Colour Reception.” Proceedings of the Physical Society 57 (6): 447–63.\n\n\nGruhl, Thomas, Tobias Weinert, Matthew J Rodrigues, Christopher J Milne, Giorgia Ortolani, Karol Nass, Eriko Nango, et al. 2023. “Ultrafast Structural Changes Direct the First Molecular Events of Vision.” Nature 615 (7954): 939–44.\n\n\nHecht, Selig, Simon Shlaer, and Maurice Henri Pirenne. 1942. “Energy, Quanta, and Vision.” The Journal of General Physiology 25 (6): 819–40.\n\n\nHofmann, Klaus Peter, and Trevor D Lamb. 2023. “Rhodopsin, Light-Sensor of Vision.” Progress in Retinal and Eye Research 93:101116.\n\n\nHsu, Yi-Te, and Robert S Molday. 1993. “Modulation of the cGMP-Gated Channel of Rod Photoreceptor Cells by Calmodulin.” Nature 361 (6407): 76–79.\n\n\nIngram, Norianne T, Alapakkam P Sampath, and Gordon L Fain. 2016. “Why Are Rods More Sensitive Than Cones?” The Journal of Physiology 594 (19): 5415–26.\n\n\nIvo Kruusamägi. 2010. “Cone cell anatomy; CC BY-SA 3.0 license.” https://commons.wikimedia.org/wiki/File:Cone_cell_en.png.\n\n\nKosigrim. 2007. “Rod cell anatomy; released into the public domain by the copyright holder.” https://commons.wikimedia.org/wiki/File:Rod%26Cone.jpg.\n\n\nKraft, TW, DM Schneeweis, and JL Schnapf. 1993. “Visual Transduction in Human Rod Photoreceptors.” The Journal of Physiology 464 (1): 747–65.\n\n\nKropf, Allen. 1982. “Photosensitivity and Quantum Efficiency of Photoisomerization in Rhodopsin and Retinal.” In Methods in Enzymology, 81:384–92. Elsevier.\n\n\nLamb, TD. 1980. “Spontaneous Quantal Events Induced in Toad Rods by Pigment Bleaching.” Nature 287 (5780): 349–51.\n\n\nLamb, TD, PA McNaughton, and K-W Yau. 1981. “Spatial Spread of Activation and Background Desensitization in Toad Rod Outer Segments.” The Journal of Physiology 319 (1): 463–96.\n\n\nMarks, WB, William H Dobelle, and Edward F MacNichol Jr. 1964. “Visual Pigments of Single Primate Cones.” Science 143 (3611): 1181–83.\n\n\nMatthews, HR, RLW Murphy, GL Fain, and TD Lamb. 1988. “Photoreceptor Light Adaptation Is Mediated by Cytoplasmic Calcium Concentration.” Nature 334 (6177): 67–69.\n\n\nMilo, Ron, and Rob Phillips. 2015. Cell Biology by the Numbers. Garland Science.\n\n\nMollon, John D. 1982. “Colour Vision and Colour Blindness.” In The Senses, edited by Horace B Barlow and John D Mollon, 165–91. Cambridge University Press.\n\n\nNaka, KI, and William AH Rushton. 1966. “S-Potentials from Colour Units in the Retina of Fish (Cyprinidae).” The Journal of Physiology 185 (3): 536–55.\n\n\nNakatani, K, and KW Yau. 1988. “Calcium and Light Adaptation in Retinal Rods and Cones.” Nature 334 (6177): 69–71.\n\n\nNathans, Jeremy. 1992. “Rhodopsin: Structure, Function, and Genetics.” Biochemistry 31 (21): 4923–31.\n\n\nNikonov, S, TD Lamb, and EN Pugh Jr. 2000. “The Role of Steady Phosphodiesterase Activity in the Kinetics and Sensitivity of the Light-Adapted Salamander Rod Photoresponse.” The Journal of General Physiology 116 (6): 795.\n\n\nNormann, Richard A, and I Perlman. 1979. “The Effects of Background Illumination on the Photoresponses of Red and Green Cones.” The Journal of Physiology 286 (1): 491–507.\n\n\nNorren, Dirk V, and Johannes J Vos. 1974. “Spectral Transmission of the Human Ocular Media.” Vision Research 14 (11): 1237–44.\n\n\nPolyak, Stephen Lucian. 1941. The Retina. Univ. Chicago Press.\n\n\nPugh Jr, EN, S Nikonov, and TD Lamb. 1999. “Molecular Mechanisms of Vertebrate Photoreceptor Light Adaptation.” Current Opinion in Neurobiology 9 (4): 410–18.\n\n\nPurves, Dale, George J. Augustine, David Fitzpatrick, William Hall, Anthony-Samuel LaMantia, Richard D. Mooney, Michael L. Platt, and Leonard E. White. 2017. Neurosciences. 6th ed. Oxford University Press.\n\n\nRieke, F, and Denis A Baylor. 1996. “Molecular Origin of Continuous Dark Noise in Rod Photoreceptors.” Biophysical Journal 71 (5): 2553–72.\n\n\nRieke, Foster, and Denis A Baylor. 1998. “Single-Photon Detection by Rod Cells of the Retina.” Reviews of Modern Physics 70 (3): 1027.\n\n\nRieke, Fred, and Denis A Baylor. 2000. “Origin and Functional Impact of Dark Noise in Retinal Cones.” Neuron 26 (1): 181–86.\n\n\nRodieck, Robert W. 1998. The First Steps in Seeing. Sinauer Associates.\n\n\nRushton, William AH. 1972. “Visual Pigments in Man.” In Photochemistry of Vision, edited by Herbert J. A. Dartnall, 364–94. Springer Berlin Heidelberg.\n\n\nRushton, William Albert Hugh. 1961. “Rhodopsin Measurement and Dark-Adaptation in a Subject Deficient in Cone Vision.” The Journal of Physiology 156 (1): 193.\n\n\n———. 1963. “Cone Pigment Kinetics in the Protanope.” The Journal of Physiology 168 (2): 374.\n\n\n———. 1965. “The Ferrier Lecture, 1962 Visual Adaptation.” Proceedings of the Royal Society of London. Series B. Biological Sciences 162 (986): 20–46.\n\n\nRushton, WilliamA H. 1972. “Review Lecture. Pigments and Signals in Colour Vision.” The Journal of Physiology 220 (3): 1P.\n\n\nSchnapf, JL, TW Kraft, and Denis A Baylor. 1987. “Spectral Sensitivity of Human Cone Photoreceptors.” Nature 325 (6103): 439–41.\n\n\nSchnapf, JL, BJ Nunn, M Meister, and Denis A Baylor. 1990. “Visual Transduction in Cones of the Monkey Macaca Fascicularis.” The Journal of Physiology 427 (1): 681–713.\n\n\nSchneeweis, David M, and Julie L Schnapf. 1995. “Photovoltage of Rods and Cones in the Macaque Retina.” Science 268 (5213): 1053–56.\n\n\n———. 1999. “The Photovoltage of Macaque Cone Photoreceptors: Adaptation, Noise, and Kinetics.” Journal of Neuroscience 19 (4): 1203–16.\n\n\nShevell, Steven K. 1977. “Saturation in Human Cones.” Vision Research 17 (3): 427–34.\n\n\nSmith, Vivianne C, and Joel Pokorny. 1975. “Spectral Sensitivity of the Foveal Cone Photopigments Between 400 and 500 Nm.” Vision Research 15 (2): 161–71.\n\n\nSnodderly, DM, PK Brown, FC Delori, and JD Auran. 1984. “The Macular Pigment. I. Absorbance Spectra, Localization, and Discrimination from Other Yellow Pigments in Primate Retinas.” Investigative Ophthalmology & Visual Science 25 (6): 660–73.\n\n\nStiles, WS. 1939. “The Directional Sensitivity of the Retina and the Spectral Sensitivities of the Rods and Cones.” Proceedings of the Royal Society of London. Series B-Biological Sciences 127 (846): 64–105.\n\n\n———. 1959. “Color Vision: The Approach Through Increment-Threshold Sensitivity.” Proceedings of the National Academy of Sciences 45 (1): 100–114.\n\n\n———. 1964. “Appendix by WS Stiles: Foveal Threshold Sensitivity on Fields of Different Colors.” Science 145 (3636): 1016–17.\n\n\nStockman, Andrew, and Lindsay T Sharpe. 2000. “The Spectral Sensitivities of the Middle-and Long-Wavelength-Sensitive Cones Derived from Measurements in Observers of Known Genotype.” Vision Research 40 (13): 1711–37.\n\n\nStockman, Andrew, Lindsay T Sharpe, and Clemens Fach. 1999. “The Spectral Sensitivity of the Human Short-Wavelength Sensitive Cones Derived from Thresholds and Color Matches.” Vision Research 39 (17): 2901–27.\n\n\nTamura, Tarnura, K Nakatani, and KW Yau. 1991. “Calcium Feedback and Sensitivity Regulation in Primate Rods.” The Journal of General Physiology 98 (1): 95–130.\n\n\nWald, George. 1933. “Vitamin A in the Retina.” Nature 132 (3330): 316–17.\n\n\n———. 1964. “The Receptors of Human Color Vision: Action Spectra of Three Visual Pigments in Human Cones Account for Normal Color Vision and Color-Blindness.” Science 145 (3636): 1007–16.\n\n\n———. 1968. “Molecular Basis of Visual Excitation.” Science 162 (3850): 230–39.\n\n\nWyszecki, Günther, and WS Stiles. 1982. Color Science: Concepts and Methods, Quantitative Data and Formulae. John wiley & sons.\n\n\nYau, KW. 1994. “Phototransduction Mechanism in Retinal Rods and Cones. The Friedenwald Lecture.” Investigative Ophthalmology & Visual Science 35 (1): 9–32.\n\n\nYau, KW, and Denis A Baylor. 1989. “Cyclic GMP-Activated Conductance of Retinal Photoreceptor Cells.” Annual Review of Neuroscience 12 (1): 289–327.\n\n\nYau, KW, and Roger C Hardie. 2009. “Phototransduction Motifs and Variations.” Cell 139 (2): 246–64.",
    "crumbs": [
      "Human Visual System",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Photoreceptors</span>"
    ]
  },
  {
    "objectID": "hvs-receptor.html#footnotes",
    "href": "hvs-receptor.html#footnotes",
    "title": "3  Photoreceptors",
    "section": "",
    "text": "Without back-scattering, we can assume any unabsorbed photons will be transmitted through, and measured at the other side of, the photoreceptor. In reality, a very small amount of some photons might be scattered backward toward where they come from and will not be measured either, but the effect is small.↩︎\n\\(\\epsilon(\\lambda)\\) so-defined has a unit of \\(m^2\\), and \\(c\\) so-defined has a unit of \\(1/m^3\\). In the literature, sometimes people define \\(\\epsilon(\\lambda)\\) to be the molar absorption coefficient, which has a unit of \\(m^2/mol\\), and define \\(c\\) to be the molar concentration (the number of moles of pigments per unit volume), which has a unit of \\(mol/m^3\\).↩︎\nGranit shared the Nobel Prize in 1967 with George Wald and Haldan Keffer Hartlin largely due to this work.↩︎\nYou might be wondering: if the GC-induced re-synthesis is always going on, wouldn’t the outer segment of a photoreceptor be packed with cGMP molecules? It turns out that in the dark even unactivated PDEs can hydrolyze cGMPs — at a much lower rate than activated PDEs do. These two forces counter each other and maintain a steady cGMP level in dark (Rodieck 1998, p. 373).↩︎",
    "crumbs": [
      "Human Visual System",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Photoreceptors</span>"
    ]
  },
  {
    "objectID": "hvs-color.html",
    "href": "hvs-color.html",
    "title": "4  Color Vision",
    "section": "",
    "text": "4.1 Color Encoding at Photoreceptors\nThis chapter studies color vision. We will review two main retinal stages responsible for color vision: wavelength encoding by the photoreceptors and the opponent processes that take place post-receptorally. We discuss both the behavioral phenomena as well as the potential neural and physiological basis. That this chapter almost exclusively focuses on the retinal mechanisms should in no way be taken to downplay the significance of cortical mechanisms to color vision (Gegenfurtner 2003). We take this approach because: 1) the retinal mechanisms are much better understood and 2) many real-world applications such as color reproduction and detecting colored patterns could be adequately modeled by retinal mechanisms. This chapter concludes by briefly reviewing the evolution of color vision and deficient color vision.\nNewton presumably did the famous experiment where he showed that a beam of white light is really a mixture of photons at different wavelengths, and each wavelength gives a different color percept. Color is very much our subjective sensation. What is the physical reality is the spectral power distribution of light. In Newton’s words: “rays of Light in falling upon the bottom of the eye excite vibrations in the retina. Which vibrations, being propagated along the solid fibres of the optick Nerves into the Brain, cause the sense of seeing.” (Newton 1704).",
    "crumbs": [
      "Human Visual System",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Color Vision</span>"
    ]
  },
  {
    "objectID": "hvs-color.html#sec-chpt-hvs-color-receptor",
    "href": "hvs-color.html#sec-chpt-hvs-color-receptor",
    "title": "4  Color Vision",
    "section": "",
    "text": "4.1.1 From Light Spectrum to Cone Responses\nAs we have seen before, there are three classes of cones, each with a different spectral sensitivity function or a cone fundamental. We will now see how the cone fundamentals encode wavelength information that eventually gives rise to color vision.\nThe cone fundamentals we have seen in Figure 3.4 tell us the absolute spectral sensitivities of photoreceptors. It is customary to normalize the cone fundamentals to peak at unity. This normalization eliminates the differences at peak across photoreceptor types, but retains the relative spectral sensitivity within a particular type. Thus, this normalization is useful when we care only about comparing the sensitivity of different wavelengths of a particular type of photoreceptor, but not across different types of photoreceptor.\nIn addition, the cone fundamentals in Figure 3.4 are defined on an “equal-quantal” basis: the sensitivities at different wavelengths are given assuming each wavelength has the same amount of photons. Sometimes, especially in CIE standards, the cone fundamentals (and other functions related to cone fundamentals, such as luminous efficiency function and color matching functions, both of which we will discuss later) are defined based on “equal-energy”, assuming each wavelength has the same energy/power, not the same amount of photons. As we will see shortly, the equal-energy definition is practically useful since the spectrum of a light is defined as power/energy distribution, rather than quantal distribution, over wavelength.\n\n\n\n\n\n\nFigure 4.1: Physiological measurements give us absolute spectral sensitivities on an equal-quantal basis (left), but in color science each cone fundamental function is usually normalized to peak at unity and then converted to an equal-energy form (right).\n\n\n\nFigure 4.1 compares the absolute, equal-quantal cone fundamentals with the normalized, equal-energy cone fundamentals. A normalized, equal-energy sensitivity function tells us the relative amount of photon absorption given a unit power at each wavelength. For instance, the normalized L cone response is 1 at 570 \\(\\text{nm}\\) and 0.4 at 630 \\(\\text{nm}\\). This means that given two lights that have the same power/energy, one with photons only at 570 \\(\\text{nm}\\) and the other with photons only at 630 \\(\\text{nm}\\), the fraction of photons absorbed in the 630 \\(\\text{nm}\\) light is about 40% of that in the 570 \\(\\text{nm}\\) light.\nCritically, this also means if we have a 570 \\(\\text{nm}\\) light at 1 \\(\\text{W}\\) and a 630 \\(\\text{nm}\\) light at 2.5 W, the two lights would cause the same amount of pigment excitations in L cones. If we had only L cones, these two lights would be seen as the exact same light, because the HVS will receive the exact amount of electrical responses — according to the Principle of Univariance. This explains why we could not see colors at night, when only rods are functioning.\nIn reality, of course, most humans have three classes of cones, so what is the signal we receive? Given the Spectral Power Distribution (SPD) of a light \\(\\Phi(\\lambda)\\), we can calculate the total number of photon absorptions for each cone type, given by:\n\\[\n\\begin{aligned}\n  L &= \\int_\\lambda L(\\lambda) \\Phi(\\lambda) d\\lambda \\\\\n  M &= \\int_\\lambda M(\\lambda) \\Phi(\\lambda) d\\lambda \\\\\n  S &= \\int_\\lambda S(\\lambda) \\Phi(\\lambda) d\\lambda\n\\end{aligned}\n\\tag{4.1}\\]\nwhere \\(L(\\lambda)\\), \\(M(\\lambda)\\) and \\(S(\\lambda)\\) represent the cone sensitivity functions. The fact that we can directly multiply \\(\\Phi(\\lambda)\\) with, say, \\(L(\\lambda)\\) is a result of defining \\(L(\\lambda)\\) on an equal-energy/power basis. The L/M/S values we calculate represent the total number of photon absorptions given an incident light. You would know why we care about photon absorption: it is equivalent to pigment excitation up to a constant scaling factor, and pigment excitations produce electrical signals that our brain actually receives. We sometimes simply call the L/M/S value the cone responses or tristimulus values of a light, but you should know that they do not represent the actual magnitude of the electrical responses of the cones, since the magnitude is not linearly proportional to absorption as we have discussed before.\nIn actual computation we discretize the spectra and perform summation rather than integration. We also limit the summation to within the [380 \\(\\text{nm}\\), 780 \\(\\text{nm}\\)] range, since the cone fundamentals are practically 0 beyond that range. Assuming that we are quantizing the spectra at a 1-\\(\\text{nm}\\) interval, the cone responses are linearly related to the light spectrum by:\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\nL(380), L(381), \\cdots, L(780)\\\\\nM(380), M(381), \\cdots, M(780)\\\\\nS(380), S(381), \\cdots, S(780)\\\\\n\\end{bmatrix}\n\\times\n\\begin{bmatrix}\n\\Phi(380)\\\\\n\\Phi(381)\\\\\n\\vdots \\\\\n\\Phi(780)\\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nL\\\\\nM\\\\\nS\\\\\n\\end{bmatrix}\n\\end{aligned}\n\\tag{4.2}\\]\nWe can see that this is a huge dimensionality reduction. That is, our brain receives only the three-dimensional cone responses, not the actual spectrum of the light, which is of a much higher dimension. This is the basis of the trichromatic theory of color vision: color is a three-dimensional system. The theory was first proposed by Young (1802), who conjectured that there are three types of receptors, and later rediscovered, popularized, and extended by Hermann von Helmholtz in the later part of the nineteenth century.\nThe huge dimensionality reduction also means there are infinitely many lights (with different SPDs) that will be seen as having the same color, as long as they cause the same cone responses. One way to understand this is if we try to solve the system of linear equations in Equation 4.2 given \\([L, M, S]^T\\), with the constraint that the \\(\\Phi\\) vector must be non-negative everywhere (since power cannot be negative), we would generally end up with infinitely many solutions, since it is an under-determined system. The fact that multiple physically different lights can end up having the same color is called metamerism, and these lights are called metamers of each other.\n\n\n4.1.2 Cone Excitation Space, Spectral Locus, and HVS Gamut\n\n\n\n\n\n\nFigure 4.2: Spectral locus in LMS cone space; from the interactive tutorial in Zhu (2022a).\n\n\n\nThe cone fundamentals essentially give us a color space, which we call the LMS cone space or cone excitation space. A color space allows us to geometrically interpret a color as a point in the coordinate system. In the cone space, the color of a light is interpreted as the amount of responses in each of the three cone classes produced by the light (as calculated by Equation 4.2).\nThe spectral locus is a curve on which each point represents the color of a spectral light at a wavelength. Figure 4.2 shows the spectral locus in the LMS cone space on the right and the cone fundamentals on the left. The L, M, and S cone responses of a spectral light at, for instance, 605 \\(\\text{nm}\\) are 0.775, 0.265, and 0, which corresponds to the point [0.775, 0.265, 0] in the cone space. Connecting these points for all the spectral lights gets us the spectral locus in the LMS space.\nWe know a color corresponds to a point in the cone space, but does an arbitrary point in the cone space correspond to a real color? No. For instance, if a point has a negative coordinate it obviously could not be a color of a real light, since that a negative cone response would require negative power in the light. Also, [1, 0, 0] is also not a real color, since there is no real light that can produce only L cone response but no responses from M and S cones — if you examine the cone fundamentals carefully. We call these colors imaginary colors, since they cannot be produced by physically realizable lights, where the power must be non-negative at any wavelength.\nIn principle, an [L, M, S] point corresponds to a real color if Equation 4.2 has a non-negative solution for \\(\\Phi\\). The total set of [L, M, S] points that have a non-negative \\(\\Phi\\) solution corresponds to all the colors that humans can see, which is called the gamut of the human visual system. Geometrically, if a point in the cone space cannot be constructed through a positive, linear combination of the points on the spectral locus, it then is not a real color, since the SPD of a real light must be a positive, linear combination of the SPDs of the spectral lights.\nFor instance, the line segment connecting two points on the spectral locus contains real colors that can be produced by mixing some amount (i.e., positive linear combinations) of the two spectral lights. Of course we can apply this iteratively: once you get a real color through combining spectral colors, the color itself can then be used as a basic color to create other colors. Zhu (2022d) is an interactive tutorial that visualizes the HVS gamut in the cone space (and others), which you are invited to go through.",
    "crumbs": [
      "Human Visual System",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Color Vision</span>"
    ]
  },
  {
    "objectID": "hvs-color.html#sec-chpt-hvs-color-cme",
    "href": "hvs-color.html#sec-chpt-hvs-color-cme",
    "title": "4  Color Vision",
    "section": "4.2 Trichromatic Color Matching",
    "text": "4.2 Trichromatic Color Matching\nWe can produce, in theory, any color by mixing three other colors, which we call the primary colors. Here is the mathematical intuition. Let’s say the SPDs of the three primary lights are \\(R(\\lambda)\\), \\(G(\\lambda)\\), \\(B(\\lambda)\\). What is the power of each of the primary lights we need to produce the color of a target light \\(\\Phi(\\lambda)\\)? For the color of the mixed light to match that of the target light, their corresponding cone responses must match:\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n\\sum R(\\lambda)L(\\lambda),~ \\sum G(\\lambda)L(\\lambda),~ \\sum B(\\lambda)L(\\lambda)\\\\\n\\sum R(\\lambda)M(\\lambda),~ \\sum G(\\lambda)M(\\lambda),~ \\sum B(\\lambda)M(\\lambda)\\\\\n\\sum R(\\lambda)S(\\lambda),~ \\sum G(\\lambda)S(\\lambda),~ \\sum B(\\lambda)S(\\lambda)\\\\\n\\end{bmatrix}\n\\times\n\\begin{bmatrix}\nr\\\\\ng\\\\\nb \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\sum\\Phi(\\lambda)L(\\lambda)\\\\\n\\sum\\Phi(\\lambda)M(\\lambda)\\\\\n\\sum\\Phi(\\lambda)S(\\lambda)\\\\\n\\end{bmatrix},\n\\end{aligned}\n\\tag{4.3}\\]\nwhere \\(r, g, b\\) represent the power of the three primary lights, respectively. This system in general has one unique solution because we have the same number of unknowns (\\(r, g, b\\)) as the number of equations. Each of the three equations constrains the cone-response matching of one class of cones. This means there is a single unique way to mix three primary lights to produce the color of an arbitrary target light.\nWhat if we have more than three primary lights? We would end up with an under-determined system (e.g., three equations but four unknowns if given four primary lights), which means there are infinitely many ways to mix the primaries to produce the target color. If we have only two primaries, we end up with an over-determined system, where there is in general no solution.\n\n\n4.2.1 Color Matching Experiments and Color Matching Functions\nEquation 4.3 gives a mathematical explanation for trichromatic color matching, but it requires knowing the cone fundamentals, which, as we have seen before in Section 3.2, were not experimentally measured until the mid 20th century, first through microspectrophotometry (Marks, Dobelle, and MacNichol Jr 1964; Brown and Wald 1964; Dartnall, Bowmaker, and Mollon 1983) and then through suction electrode (Schnapf, Kraft, and Baylor 1987). But even without the cone fundamentals, nothing prevents us from performing an actual experiment to find the amount of primaries for producing a color. Thomas Young apparently had no interest in such an experiment (Mollon 2003). Maxwell (1857) is believed to be the first to undertake an actual color matching experiment in the 19th century, but he did the experiments using rotating discs painted with different colors, relying on the temporal integration of the HVS.\nModern color matching experiments started with Wright and Guild (W. Wright 1928, 1930; W. D. Wright 1929; Guild 1931). International Commission on Illumination (CIE) in 1931 standardized the color matching experiment and synthesized Wright’s and Guild’s data (without any additional experiments) to obtain what is now known as the CIE 1931 RGB Color Matching Functions. This process is discussed in detail in Broadbent (2004), Broadbent (2008), Service (2016), and Zhu (2020). We summarize the key elements here; the experimental setup is illustrated in Figure 4.3.\n\n\n\n\n\n\nFigure 4.3: Color matching experiment setup. In CIE 1931 standardization of the experiment, the primary lights are spectral lights at 435.8 \\(\\text{nm}\\), 546.1 \\(\\text{nm}\\), and 700 \\(\\text{nm}\\), and they swept the visible spectrum [380 \\(\\text{nm}\\), 780 \\(\\text{nm}\\)] at a 5-\\(\\text{nm}\\) interval as the target light. Note that CIE 1931 did not do any actual experiments; they synthesized the data collected by Wright and Guild (W. Wright 1928, 1930; W. D. Wright 1929; Guild 1931).\n\n\n\nObservers are presented with a 2\\(^{\\circ}\\) visual field. They are given three primary lights, which in the CIE 1931 standard are spectral lights (lights that have photons at only one single wavelength; also called monochromatic lights) at wavelengths 435.8 \\(\\text{nm}\\), 546.1 \\(\\text{nm}\\), and 700 \\(\\text{nm}\\). The three primary lights are pointed at the same point on one side of the visual field. On the other side of the visual field is the target light. Their goal is to adjust the power of each of the three primary lights so that the colors from the two sides of the visual field match. CIE 1931 swept the entire visible spectrum for the target light at a 5-\\(\\text{nm}\\) interval.\n\nColor Matching Functions Require a Unit System and a White Point\nThe results obtained through the color matching experiments are shown in Figure 4.4 (left panel). The three curves are collectively called the CIE 1931 RGB Color Matching Functions (CMFs). Intuitively, the CMFs tell us the amount of primaries needed to match the color at each wavelength. But the devil is in the details. Let’s carefully walk through what this plot actually shows.\n\n\n\n\n\n\nFigure 4.4: Left: CIE 1931 RGB Color Matching Functions (CMFs); from Marco Polo (2007). The \\(y\\)-axis shows the number of units needed of each primary so that the mixture matches the color at each wavelength (\\(x\\)-axis) on an equal-energy basis. The unit system is so defined that mixing equal amounts (the number of units) of the three primaries produces the color of the equal-energy white, whose SPD is constant over the entire spectrum. Right: the negative values in the CMFs indicate that the corresponding primary light is to be mixed with the target light in order to match the color of the mixture of the other primaries.\n\n\n\nThe \\(y\\)-axis represents the number of units required of each primary so that the mixture matches the color at a given wavelength at \\(x\\)-axis. What is a unit? The unit system is so defined that mixing the three primaries in equal units produces the color of the Equal-Energy White (EEW), whose SPD is a constant across the spectrum.\nThere are two judgment calls here. First, CIE 1931 decided that EEW was going to be the “white” color in their RGB color space. In general, however, there is no single color that we universally define as white, so if you were to design a color space you get to pick whatever color that you think is white in the color space. That said, an intuitive choice of white is one that is achromatic (colorless), a color that, subjectively, can only be described as having a certain level of gray but that has no apparent hue. Daylights at different times of a day are perceptually achromatic and could be used as the white point in a color space. The daylight colors are shown to be very similar to the colors of black-body radiation at different temperatures (Judd et al. 1964), shown in Figure 4.5.\n\n\n\n\n\n\nFigure 4.5: Color from black-body radiation at different temperatures (\\(x\\)-axis; unit: Kelvin). CIE Standard Illuminant D65 approximates the SPD of a noon daylight; its color is similar to that of a 6500 K black-body radiation. From Bhutajata (2015).\n\n\n\nYou probably do not perceive most of the colors in Figure 4.5 as achromatic on the display right now, but when you are in an environment illuminated by one of these colors, e.g., outdoors at noon, you do perceive the illuminant as achromatic; this is because of chromatic adaptation, a topic we will discuss later in Section 6.3. Briefly, the human visual system is evolved to adapt to different daylight colors so that when you spend enough time under such an illuminant, you will see the illuminant as achromatic. The adaptation to other colors, however, is weak (or “incomplete” in chromatic adaptation parlance)1, so it probably does not make much sense to pick other colors as the white point if you want your user to see your white as achromatic. CIE has standardized a set of what they call Standard Illuminants (D series), each of which approximates a different daylight color. For instance, the D65 standard illuminant approximates noon daylight and is similar to the color of a black-body radiation at a temperature of 6500 K. Many common color spaces, such as the sRGB color space, use D65 as the white point.\nSecond, CIE 1931 RGB space, and virtually all color spaces, define units so that white, however defined, must be produced by an equal-unit mixture of the primaries. This, again, is a judgment call. One could totally design a color space where white is produced by mixing, say, 2 units of red and 1 unit of green and blue each — nothing wrong with that. It is just more intuitive for most people that white is produced by equal amounts of the primaries.\nThe \\(x\\)-axis in Figure 4.4 is defined on an equal-energy/power basis. That is, the CMFs are interpreted as showing the amount (units) of the primaries needed to produce spectral lights of equal power. So if we actually mix the three primaries at each wavelength as indicated by the CMFs, we will get a set of spectral lights that have the same power.\n\n\nWhat Does a Negative Unit Mean?\nIf you observe Figure 4.4 carefully, you will see that some CMFs are negative over certain ranges. For instance, the red CMF is negative at 500 \\(\\text{nm}\\). This is perhaps a bit surprising, but mathematically it is entirely possible that some values in \\([r, g, b]^T\\) are negative when solving Equation 4.3. Physically, however, what does it mean to have a negative amount/power of primary light? The right panel in Figure 4.4 provides the intuition. It turns out that it is impossible to find a combination of the three primary lights to match the color of a spectral light at 500 \\(\\text{nm}\\). What does provide a match is to add a little red primary to the target light, and then we can find a combination of the primaries such as the blue and green mixture has the same color as the target light and red primary mixture.\nIn fact, if you examine the CMFs, you will see that there is a negative contribution from a primary at all but three wavelengths — the only three exceptions are the wavelengths of the three primaries (where two of the primary contributions are zero and the other is positive). This means that no spectral light color (except the three special cases) can be physically produced by mixing the three primaries.\n\n\nRepresenting Colors Using CMFs\nGiven a set of CMFs, we can describe the color of a light with a SPD \\(\\Phi(\\lambda)\\) using the following equation:\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n\\bar{r}(380), \\bar{r}(381), \\cdots, \\bar{r}(780)\\\\\n\\bar{g}(380), \\bar{g}(381), \\cdots, \\bar{g}(780)\\\\\n\\bar{b}(380), \\bar{b}(381), \\cdots, \\bar{b}(780)\n\\end{bmatrix}\n\\times\n\\begin{bmatrix}\n\\Phi(380)\\\\\n\\Phi(381)\\\\\n\\vdots \\\\\n\\Phi(780)\\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nR\\\\\nG\\\\\nB\\\\\n\\end{bmatrix}\n\\end{aligned}\n\\tag{4.4}\\]\nwhere \\(\\bar{r}(\\lambda)\\), \\(\\bar{g}(\\lambda)\\), and \\(\\bar{b}(\\lambda)\\) are the CMFs, and \\(R\\), \\(G\\), and \\(B\\) are the amounts of the three primaries needed to match the color of \\(\\Phi(\\lambda)\\).\nThe CMFs give us another color space, where the color of a light is interpreted as the amount of primary lights needed to match the color of the light. Of course, if we choose a different set of primary lights, we might end up with a new set of CMFs and a new RGB color space.\n\n\n\n4.2.2 Connecting CMFs and Cone Fundamentals\nCMFs and cone fundamentals both yield trichromatic color vision, so they must be inherently related, as they are just different ways of describing the same thing. We show the two are linearly related in theory, and the measurement data of the two match well, too.\n\nDeriving Color Matching Functions From Cone Fundamentals\nGiven the cone fundamentals, we can derive the CMFs based on the linear system shown in Equation 4.3. The interactive tutorial by Zhu (2022a) walks through the process, which you are invited to go over, and we will describe the main steps here.\nIn order to construct the CMFs, we have to match the colors of all the spectral lights, which means we have to specify cone-response matching at each wavelength. Using the basic idea of Equation 4.3, we have:\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n\\sum R(\\lambda)L(\\lambda),~ \\sum G(\\lambda)L(\\lambda),~ \\sum B(\\lambda)L(\\lambda)\\\\\n\\sum R(\\lambda)M(\\lambda),~ \\sum G(\\lambda)M(\\lambda),~ \\sum B(\\lambda)M(\\lambda)\\\\\n\\sum R(\\lambda)S(\\lambda),~ \\sum G(\\lambda)S(\\lambda),~ \\sum B(\\lambda)S(\\lambda)\\\\\n\\end{bmatrix}\n\\times\n\\begin{bmatrix}\nr(380),\\cdots,r(780)\\\\\ng(380),\\cdots,g(780)\\\\\nb(380),\\cdots,b(780)\\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nL(380),\\cdots,L(780)\\\\\nM(380),\\cdots,M(780)\\\\\nS(380),\\cdots,S(780)\\\\\n\\end{bmatrix},\n\\end{aligned}\n\\]\nwhere \\(L(\\lambda)\\), \\(M(\\lambda)\\), and \\(S(\\lambda)\\) are the cone fundamentals; \\(L(\\lambda_0)\\) is the L cone response of the spectral light at a particular wavelength \\(\\lambda_0\\); \\([r(\\lambda_0), g(\\lambda_0), b(\\lambda_0)]^T\\) represents the (to-be-solved-for) power of each primary needed to match the color of the spectral light at \\(\\lambda_0\\); \\(R(\\lambda)\\), \\(G(\\lambda)\\), and \\(B(\\lambda)\\) are the SPDs of the primary lights used in the CIE 1931 color matching experiment. The first matrix is a constant matrix given a particular set of CMFs, and we will denote it as the \\(\\mathbf{M}\\) matrix. We can solve the system of equations by inverting the first matrix:\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\nr(380),\\cdots,r(780)\\\\\ng(380),\\cdots,g(780)\\\\\nb(380),\\cdots,b(780)\\\\\n\\end{bmatrix}\n=\n\\mathbf{M}^{-1}\n\\times\n\\begin{bmatrix}\nL(380),\\cdots,L(780)\\\\\nM(380),\\cdots,M(780)\\\\\nS(380),\\cdots,S(780)\\\\\n\\end{bmatrix}.\n\\end{aligned}\n\\]\nTo get the CMFs, however, we need to turn the power measure into a unit measure. Recall the requirement that white must be produced by equal units of the primaries. We calculate the power of each primary needed to produce the EEW; let’s denote the solution \\([r_w, g_w, b_w]^T\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\nr_{w}\\\\\ng_{w}\\\\\nb_{w}\\\\\n\\end{bmatrix}\n=\n\\mathbf{M}^{-1}\n\\times\n\\begin{bmatrix}\nL_{w}\\\\\nM_{w}\\\\\nS_{w}\\\\\n\\end{bmatrix},\n\\end{aligned}\n\\]\nwhere \\([L_w, M_w, S_w]^T\\) denotes the total L, M, and S cone responses of EEW. For the so-calculated \\([r_w, g_w, b_w]\\) to represent equal units, the last step is to scale \\([\\bar{r}(\\lambda), \\bar{g}(\\lambda), \\bar{b}(\\lambda)]^T\\) at each \\(\\lambda\\) by \\([r_w, g_w, b_w]\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n\\bar{r}(380), \\cdots, \\bar{r}(780)\\\\\n\\bar{g}(380), \\cdots, \\bar{g}(780)\\\\\n\\bar{b}(380), \\cdots, \\bar{b}(780)\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\nr_w,~0,~0\\\\\n0,~g_w,~0\\\\\n0,~0,~b_w\n\\end{bmatrix}\n\\times\n\\begin{bmatrix}\nr(380),\\cdots,r(780)\\\\\ng(380),\\cdots,g(780)\\\\\nb(380),\\cdots,b(780)\\\\\n\\end{bmatrix}\\\\\n&=\n\\begin{bmatrix}\nr_w,~0,~0\\\\\n0,~g_w,~0\\\\\n0,~0,~b_w\n\\end{bmatrix}\n\\times\n\\mathbf{M}^{-1}\n\\times\n\\begin{bmatrix}\nL(380),\\cdots,L(780)\\\\\nM(380),\\cdots,M(780)\\\\\nS(380),\\cdots,S(780)\\\\\n\\end{bmatrix}\\\\\n&=\n\\mathbf{T}_{lms2rgb}\n\\times\n\\begin{bmatrix}\nL(380),\\cdots,L(780)\\\\\nM(380),\\cdots,M(780)\\\\\nS(380),\\cdots,S(780)\\\\\n\\end{bmatrix},\n\\end{aligned}\n\\tag{4.5}\\]\nwhere \\([\\bar{r}(\\lambda), \\bar{g}(\\lambda), \\bar{b}(\\lambda)]^T\\) gives us the unit measure, i.e., the values of the CMFs, at each \\(\\lambda\\).\n\n\nCone Space and RGB Space are Related by a Linear Transformation\nThe rightmost matrix in Equation 4.5 is the cone fundamentals written out in the matrix form, and the leftmost matrix in Equation 4.5 is the CMFs written out at discrete wavelengths. So Equation 4.5 essentially describes a linear transformation from the cone fundamentals to the RGB CMFs, where the transformation is dictated by \\(\\mathbf{T}_{lms2rgb}\\). We can look at this in two ways. One, we can think of the cone fundamentals as the CMFs in the cone space: they tell us how much of each cone response we need to match the color of a spectral light. Two, just like how we can construct the spectral locus from cone fundamentals, the RGB CMFs also give us a way to construct the spectral locus — in the RGB space. \\(\\mathbf{T}_{lms2rgb}\\) essentially transforms these two representations of the spectral locus, and this is visualized in Figure 4.6.\n\n\n\n\n\n\nFigure 4.6: The spectral locus in the LMS cone space, CIE 1931 RGB space, and CIE 1931 XYZ space. The color spaces are a linear transformation away from each other. From the interactive tutorials in Zhu (2022a) and Zhu (2022b).\n\n\n\nThere is something deeper: \\(\\mathbf{T}_{lms2rgb}\\) not only transforms the spectral locus, it transforms the entire coordinate system from the cone space to the RGB space. In other words, it transforms every single color in the LMS space to its corresponding coordinates in the CIE 1931 RGB space. The way to think about this is to ask: given that the cone space and the CIE 1931 RGB space provide two ways to represent the color of a light \\(\\Phi\\), how are the cone-space representation \\([L_c, M_c, S_c]\\) and the RGB-space representation \\([R_c, G_c, B_c]\\) related? Using Equation 4.2, Equation 4.4, and Equation 4.5, it is easy to see that they are related by a linear transformation through \\(\\mathbf{T}_{lms2rgb}\\):\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\nR_c\\\\\nG_c\\\\\nB_c\n\\end{bmatrix}\n=\n\\mathbf{T}_{lms2rgb}\n\\times\n\\begin{bmatrix}\nL_c\\\\\nM_c\\\\\nS_c\n\\end{bmatrix}.\n\\end{aligned}\n\\]\n\n\nCone Responses Fully Explain Psychophysical Color Matching\nThe CMFs can be both experimentally measured and calculated if we know the cone fundamentals (through a linear transformation), but do the mathematical estimation and the measurement data match? If so, we can say that the physiological process of encoding light power as cone responses can fully account for the color matching experiments in psychophysics.\nBaylor, Nunn, and Schnapf (1987) performed one such comparison and showed the two sets of data matched very well. The results are shown in Figure 4.7, where the smooth curves are from W. Stiles and Burch (1955), which uses a different set of primaries and white point than those used in the CIE 1931 RGB CMFs. The markers are the predicted CMFs through a linear regression from the cone fundamentals measured from macaques, after accounting for ocular and macular absorptions2.\n\n\n\n\n\n\nFigure 4.7: Smooth curves are the CMFs from W. Stiles and Burch (1955), which uses a different set of primaries and white point than those used in the CIE 1931 RGB CMFs. The markers are the predicted CMFs based on the cone fundamentals measured from macaques. From Baylor, Nunn, and Schnapf (1987, fig. 4A).\n\n\n\nIn fact, the modern versions of the cone fundamentals are constructed so that they are precisely a linear transformation away from some RGB CMFs. For instance, the CIE 2006 “physiologically-relevant” LMS functions (based on Stockman, Sharpe, and Fach (1999) and Stockman and Sharpe (2000)) are constructed by 1) first experimentally measuring the cone fundamentals in psychophysics (from color-vision deficient observers), 2) calibrating the results with a set of RGB CMFs in W. S. Stiles and Burch (1959) (which uses a different set of primary lights from the CIE 1931 RGB CMFs) to derive a best-fit linear transformation, and 3) applying the linear transformation to the CMFs to derive a “clean” set of cone fundamentals.",
    "crumbs": [
      "Human Visual System",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Color Vision</span>"
    ]
  },
  {
    "objectID": "hvs-color.html#sec-chpt-hvs-color-oppo",
    "href": "hvs-color.html#sec-chpt-hvs-color-oppo",
    "title": "4  Color Vision",
    "section": "4.3 Post-Receptoral Color Encoding: Opponent Processes",
    "text": "4.3 Post-Receptoral Color Encoding: Opponent Processes\nCone-response encoding can perfectly explain the trichromatic theory of color vision, where any color can be mixed from three other colors. The trichromatic theory of color has a perfect neural basis: the human visual system has three classes of cones, so color is a three-dimensional system. But the trichromatic theory is not concerned with our subjective experience of color that we encounter on a daily basis. Here are two examples that highlight the difference between perceptual color experience and physical color mixing.\nFirst, when we see an orange color, we feel that it has a little bit of yellow in it and a little bit of red in it. Even though there are many ways to produce orange, some of which do not require mixing yellow and red lights, we cannot help but perceptually feel that orange combines yellow and red. Second, when we mix a red light with a green light, we get yellow, but perceptually, if we stare at yellow, most people would not say that yellow has contributions from red or green.\nHering (1878)3 hypothesized that, perceptually, there are four primary hues, which form two opposing pairs. Opposing hues cannot co-exist, perceptually, in a color. Any hue can be produced by combining two non-opposing hues. The four hues are: the Yellow and Blue opposing hues and the Red and Green opposing hues. Hering also considered light-dark as another opposing pair: no color can be simultaneously light and dark. In his theory, color vision is still a three-dimensional system, where the three axes are: Yellow-Blue axis, Red-Green axis, and light-dark axis. Any color, a point in this 3D space, is produced by mixing some amount of Red or Green, some amount of Yellow or Blue, and some level of lightness.\nThe opponent theory seems to contradict the trichromatic theory, which was dominant for the most part of the history — because it has both a solid psychophysical and neural basis. First, the color matching experiment quantitatively shows that, behaviorally, humans could match a color by mixing three other colors. In contrast, Hering had only a qualitative description of perceptual mixing. His description was something like “after this blue comes blue of increasing redness…(blue violet, red violet, purple red), until the last trace of blueness vanishes in a true red.” (Hering 1964, p. 41). To Hering’s theory’s rescue, Jameson and Hurvish performed a now-famous experiment, called the hue cancellation experiment, providing the first quantitative, psychophysical evidence of the opponent processes (Jameson and Hurvich 1955; Hurvich and Jameson 1957).\nSecond, the trichromatic theory has a clear neural and physiological basis (i.e., wavelength encoding by cone responses), and the physiological data match the behavioral data very well, as shown before. So a natural question is: are there neural mechanisms that can account for the opponent processes and, if so, how does that mechanism relate to the encoding mechanisms by the cone photoreceptors?\nIt turns out that we do need a set of new neural mechanisms to start accounting for the opponent processes. Not only do these new mechanisms not contradict the cone encoding mechanisms, they build on top of the cone encodings and operate post-receptorally. Schrödinger (1925)4 synthesized the earlier zone theory by Kries (1905) and argued that the trichromatic theory and the opponent processes were nothing more than different stages of color encoding in the visual system. That said, while these new neural mechanisms seem to have what it takes to form the basis for the behavioral opponent observations, they do not fully explain those observations yet; the link between the two is still very much an open research question.\nThe rest of this section will discuss the hue cancellation experiment and the quest for a neural and physiological basis in more detail.\n\n4.3.1 Hue Cancellation Experiment\nIn a landmark study, Jameson and Hurvich (1955) (while working for Eastman Kodak in Rochester) quantitatively measured the perceptual color opponency using a behavioral experiment. The participant is given a test light and is asked to first judge whether the light appeared blue-ish or yellow-ish. If the test light is judged to be blue-ish, the participant is then given a yellow-ish cancellation light (e.g., a spectral light at 588 \\(\\text{nm}\\)) and is asked to adjust the intensity of the cancellation light so that the mixture of the test and cancellation light perceptually appears neither blue nor yellow. If the test light is judged to be yellow-ish, the participant is then asked to adjust the power of a blue-ish cancellation light (e.g., a spectral light at 467 \\(\\text{nm}\\)) so that the test-cancellation mixture is again neither blue nor yellow. We sweep the spectrum from about 400 \\(\\text{nm}\\) to 700 \\(\\text{nm}\\) for the test light of equal energy, and record the energy of yellow or blue cancellation light needed at each step.\n\n\n\n\n\n\nFigure 4.8: Measurements from the hue cancellation experiment in Jameson and Hurvich (1955). (a) the Blue-Yellow measurement; the \\(y\\)-axis shows the intensity of the Yellow/Blue cancellation light, i.e., the relative strength of the “Blue-ness” and “Yellow-ness” in the test light. (b) the Red-Green measurement; notice the two zero-crossings for Green. (c) The same data as A and B except we invert the Blue and Green curves so the \\(y\\)-axis is interpreted as the strength of Red-ness and Yellow-ness.\n\n\n\nThe result for one subject is shown in Figure 4.8 (a), where the \\(y\\)-axis is showing the intensity of the yellow and blue cancellation light, i.e., the strength of blue-ness and yellow-ness of the test light. For the reference, we attached a colorbar showing roughly the color of the test light between 400 \\(\\text{nm}\\) and 700 \\(\\text{nm}\\), but take this color visualization as a huge grain of salt, since it is almost certain that your display will not be able to actually render the colors of the spectral lights.\nUnsurprisingly, we get two peaks, one in the blue range and the other in the yellow range, indicating, respectively, that the participant needs a lot of the yellow and blue cancellation lights in those two regions. The test light at about 500 \\(\\text{nm}\\) requires no cancellation light, indicating light there, which roughly has a green-ish color is yellow-blue neutral: it naturally looks neither blue nor yellow.\nJameson and Hurvich then repeated the same experiment, but this time measuring the red-green opponent process, where the two cancellation lights are a 700 \\(\\text{nm}\\) red-ish light and a 490 \\(\\text{nm}\\) green-ish light. The results are in Figure 4.8 (b), where the \\(y\\)-axis indicates the amount of red-ness and green-ness in the test light. Two observations are worth noting. First, while it is unsurprising that long-wavelength lights have a strong red component, it is perhaps surprising that short-wavelength lights appear red-ish too. That, however, becomes less surprising when we realize that short-wavelength lights (shorter than pure blue) appear violet, which perceptually is a red-ish blue. Second, because of the two red-ish regions over the spectrum, the entire red-green curve has two zero-crossings, one at about 470 \\(\\text{nm}\\) and the other near 570 \\(\\text{nm}\\): pure blue and pure yellow look neither green nor red.\nFigure 4.8 (c) summarizes the two sets of data by inverting the blur section of the curve in (a) and the green section of the curve in (b). That way, the \\(y\\)-axis can be simply interpreted as the relative strength of red-ness and yellow-ness over the spectrum.\n\n\n4.3.2 Light-Dark Mechanism and Luminous Efficiency Function\nHurvich and Jameson (1957) also performed a measurement of the white-black (light-dark) opponent process, asking participants to assess the “whiteness” of spectral lights between 400 \\(\\text{nm}\\) and 700 \\(\\text{nm}\\) of equal power. %Intuitively, the measurement tells us the perceived brightness of lights at different wavelengths. A more modern method to measure the luminance mechanism is heterochromatic flicker photometry, where we alternate between a test light and a fixed reference light at a frequency of, say, 25 Hz. We adjust the intensity of the test light so that the alternation produces no visual flickering, at which point we say the two lights produce the same level of luminance (Sharpe et al. 2005, 2011). We again sweep the entire visible spectrum for the test light and record the relative intensity at each step. The so-obtained function is called the luminance efficiency function (LEF). The dashed gray curve in Figure 4.9 shows a modern version of the photopic LEF (the so-called CIE 2008 “physiologically-relevant” 2-deg function)5.\n\n\n\n\n\n\nFigure 4.9: The grey solid curve is the scotopic luminous efficiency function (CIE 1951 standard; based on Wald (1945) and Crawford (1949)). The grey dashed curve is the photopic luminous efficiency function (CIE 2008 “physiologically-relevant” 2-deg function; based on Sharpe et al. (2005) and Sharpe et al. (2011)) The other three curves are the cone fundamentals, shown for the reference.\n\n\n\nThe way to interpret the LEF is that the \\(y\\)-axis is inversely proportional to the light power at each wavelength needed to produce the same level of perceptual brightness. The photopic LEF at 509 \\(\\text{nm}\\) is about 0.5, half of that at 555 \\(\\text{nm}\\). It means we need twice as much power at 509 \\(\\text{nm}\\) to produce the same level of brightness as that at 555 \\(\\text{nm}\\). It also explains the word “efficiency” in the name: if a wavelength needs less power to produce a criterion level of brightness, the wavelength is more efficient in its use of power. The way LEF is obtained, however, does not permit us to interpret the result as the relative brightness at different wavelengths. That is, 555 \\(\\text{nm}\\) is not twice as bright as 509 \\(\\text{nm}\\). This is similar to our interpretation of the cone fundamentals.\nFor comparison, the gray curve in Figure 4.9 is the scotopic LEF. The CIE 1951 scotopic LEF synthesizes the psychophysical measurements from Wald (1945) and Crawford (1949). Both used a threshold method where they measured the light intensity at each wavelength needed to produce a just detectable flash. Note that the photopic LEF peaks at about 555 \\(\\text{nm}\\) and the scotopic LEF peaks at about 507 \\(\\text{nm}\\).\nAs a result, the relative brightness of longer-wavelength colors and shorter-wavelength colors is inverted when our vision transitions from the cone-mediated photopic vision to the rod-mediated scotopic vision. This phenomenon is called the Purkinje shift. In the words of Glassner (1995, p. 21), “When the sun is still above the horizon, your cones are active, and the yellow flower will appear lighter than the leaves because yellow is closer to peak of the photopic sensitivity curve than dark green. When the sun has set and light levels are lower, your rods are the principal sensors. The scotopic sensitivity curve is more responsive in the shorter wavelengths, so the green leaves will now appear relatively lighter than the yellow flower, though both will of course be much darker due to the lower amount of incident light.”\n\n\n\n\n\n\nFigure 4.10: The solid curve is the white-black measurement, indicating the amount of whiteness in a light across the spectrum. The white-black curve in theory matches the luminous efficiency function. The two plots are for two participants. From Hurvich and Jameson (1957, fig. 4).\n\n\n\nCombining the light-dark (luminance efficiency) curve with the two opponent curves in Figure 4.8 (c), we again have three spectral sensitivity functions. Figure 4.10 puts the three opponent measurements in one plot (the two plots are for two separate participants). Compare this plot with the cone fundamentals in Figure 4.1. Once again, a light with its SPD can be reduce to three-dimensional point, using Equation 4.1, except 1) instead of the three cone fundamentals we use the three opponent functions and 2) instead of getting the three cone responses we get the strength of the three opponent mechanisms. Effectively, the hue cancellation curves and the light-dark curve construct a new three-dimensional color space. We call this the hue-opponent space, and we will return to this space in Section 4.4.3 and discuss how this space relates to the colorimetric spaces we have discussed so far.",
    "crumbs": [
      "Human Visual System",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Color Vision</span>"
    ]
  },
  {
    "objectID": "hvs-color.html#sec-chpt-hvs-color-oppobasis",
    "href": "hvs-color.html#sec-chpt-hvs-color-oppobasis",
    "title": "4  Color Vision",
    "section": "4.4 Neural and Physiological Basis of Opponent Processes",
    "text": "4.4 Neural and Physiological Basis of Opponent Processes\nThe hue cancellation experiment solidifies Hering’s opponent theory at the level of psychophysics. But recall Figure 1.2; any behavioral responses measured through psychophysics are fundamentally the result of the underlying neural and physiological mechanisms. So the next natural step in the scientific quest is to understand what underlying neural and physiological mechanisms can account for the behavioral opponent processes.\n\n\n\n\n\n\nFigure 4.11: Responses of six typical classes of LGN neurons to incremental flashes of varying wavelengths. \\(y\\)-axis shows the spikes/second under spectral lights of equal energy. Each curve represents a particular energy level. (A): these cells are excited (activity exceeds the spontaneous firing rate) by red hues and inhibited by green hues, denoted +R-G cells. (B): +G-R cells. (C): +B-Y cells. (D): +Y-B cells. (E): non-opponent excitatory cell. (F): non-opponent inhibitory cell. From DeValois and DeValois (1990, fig. 7.5), which is adapted from R. L. De Valois, Abramov, and Jacobs (1966, figs. 9–12, 15–16).\n\n\n\n\n4.4.1 Spectrally-Opponent and Non-Opponent Neurons\nThere are RGC and LGN neurons that show opponent properties. G. Svaetichin (1953), G. Svaetichin (1956), and G. Svaetichin and MacNichol Jr (1958) are the first to identify opponent neurons in a fish retina; they recorded from horizontal cells. R. De Valois et al. (1958) and R. L. De Valois, Abramov, and Jacobs (1966) measured the responses of LGN neurons in macaques using monochromatic lights, and found spectral opponent neurons, which get excited or inhibited depending on the wavelengths. (A – D) in Figure 4.11 show the recordings of four classes of opponent cells. (A) shows a class of LGN cells whose firing rate exceeds the spontaneous rate under long-wavelength, red-ish lights and whose firing rate drops below the spontaneous rate under short-wavelength, blue-is lights. These cells are denoted +R-G (red-ON/green-OFF) cells. (B), (C), and (D) show that there exists +G-R, +B-Y, and +Y-B cells, respectively.\nR. L. De Valois, Abramov, and Jacobs (1966) also identified non-opponent cells, whose responses are universally inhibited or excited across the spectrum, as shown in (E) and (F) in Figure 4.11, respectively. These neurons are still wavelength-sensitive, but their responses are either universally excited or universally inhibited across the spectrum, unlike the spectrally-opponent neurons whose responses change polarity across the spectrum.\n\n\n4.4.2 Potential Neural Circuitries\nWhat are some of the underlying visual pathways that could potentially give rise to these spectral tuning curves? Recall that LGN cells/RGCs have antagonistic Receptive Fields (RFs), and the antagonism seems to be a perfect mechanism to implement the opponent process. This suggests that in order to understand the opponent cells we must study their RF structures.\nMuch of the early work is done by Wiesel and Hubel (1966). While De Valois and his collaborators used diffuse lights to illuminate a large visual field, Wiesel and Hubel (1966) used both small spot lights that stimulated the center of the RF and larger lights that covered the entire RF. By comparing the responses under these two stimuli across different wavelengths (and white), they suggested potential RF structures of both opponent and non-opponent cells in macaque LGN. Derrington, Krauskopf, and Lennie (1984) designed a clever experiment that explicitly tied cone responses to LGN cell responses and thus more directly revealed the RF structure.\nBefore getting into the details, it is worth reminding ourselves that studying the LGN cells and studying the RGCs are equivalent (Section 2.5.1), since different classes of RGCs project to distinct LGN layers with virtually the same RFs: midget RGCs project to the Parvocellular layers (P cells) in the LGN (forming the P pathway/stream), parasol RGCs project to the Magnocellular layers (M cells) in the LGN (forming the M pathway/stream), and bistratetified RGCs project to the Koniocellular layers (K cells) in the LGN (forming the K pathway/stream).\n\nY-B Opponent Cells\nThe visual pathway for the Y-B opponent cells seems to be clear. Derrington, Krauskopf, and Lennie (1984) showed that some LGN cells receive antagonistic inputs from S cone vs. L and M cones. Dacey and Lee (1994) later identified that the small bistratified RGCs (which project to the K cells in the LGN) are responsible for carrying such signals. The small bistratified RGCs are excited by S cone responses and inhibited by L and M cone responses (or vice versa). Since blue-ish lights produce strong S cone responses and red/green lights produce strong L/M cone responses (recall red + green is yellow), it stands to reason that if a cell is excited by S cones and inhibited by L and M cones, it would give a vigorous on-response under blue lights and a vigorous off-response under yellow lights, producing the kind of yellow-ON/blue-OFF spectral tuning curve that we see in Figure 4.11 (C).\n\n\n\n\n\n\nFigure 4.12: The small bistratefied RGCs might be the substrate for the Y-B pathway. (A): illustration of the receptive field structure of a small bistratefied RGC, which is S-on and L/M-off (there are also S-off and L/M-on ones); from Rodieck (1998, p. 348). (B): a small bistratefied RGC receives excitatory inputs from S cones through the S-cone bipolar cells and inhibitory inputs from L and M cones through another class of bipolar cells; from Rodieck (1998, p. 346). (C): membrane potential and spike rate of small bistratified cells under periodic, out-of-phase blue-yellow lights; adapted from Dacey and Lee (1994, fig. 3C).\n\n\n\nFigure 4.12 (A) illustrates the potential Receptive Field (RF) of a yellow-ON/blue-OFF small bistratefied cell, and (B) shows the neutral circuitry that gives rise to such an RF (but also see Field et al. (2007)). The small bistratefied RGC have a center-only RF, which receives excitatory responses from a S-cone bipolar cell and inhibitory responses from another class of bipolar cells that are connected to L and M cones. Dacey and Lee (1994) records both the membrane potential and the spiking rate of a small bistratified RGC, shown in (C), under periodic, out-of-phase blue and yellow (red+green) lights. The cell’s responses are the strongest under maximum yellow light (maximum excitatory S cone responses) and minimum blue lights (minimum inhibitory L and M cone responses).\n\n\nR-G Opponent Cells\nDerrington, Krauskopf, and Lennie (1984) showed that most of the midget RGCs (and thus P cells in LGN) are either excited by L cone responses and inhibited by M cone responses (L-ON/M-OFF) or the other way around. Given that, loosely, L cones are excited by red-ish lights but not so much by green-ish lights and M cones behave oppositely, it stands to reason that L-ON/M-OFF cells produce vigorous on-responses (above spontaneous rate) under red lights and vigorous off-responses (below spontaneous rate) under green lights, giving a spectral tuning curves shown in Figure 4.11 (A).\nThe actual RF structure of these cells takes two forms (Wiesel and Hubel 1966). Some of these cells have a center-surround RF, so there are four combinations: L+/M- (L center-ON/M surround-OFF), L-/M+, M-/L+, and M+/L-. Other midget RGCs have no center-surround arrangement. The excitatory and inhibitory regions have the same spatial extent. Either way, signals from the L cones and M cones are antagonistic in these cells.\n\n\nNon-Opponent Cells\nFinally, the parasol RGCs (and thus M cells in LGN) seem to be the most probable source for the luminance opponent mechanism (B. Lee, Martin, and Valberg 1988). These cells do have a center-surround RF but the L cones and M cones contribute to both the center and the surround (Wiesel and Hubel 1966); S cones seem to be contribute little, if any, to these cells (Lennie, Pokorny, and Smith 1993). When the total excitation by the L and M cones to the center out-weighs the total inhibition to the surround, the entire cell appears to be excited by L and M cone responses, giving a broadband, non-opponent spectral tuning curve in Figure 4.11 (E); otherwise we see a tuning curve like Figure 4.11 (F).\n\n\n\n\n4.4.3 A Cone-Opponent Model for Color-Opponent Mechanisms\nIt is clear that there are cells that receive opponent cone signals; the spectral tuning curves of these cells seem to largely account for the perceptual opponent mechanisms. Based on these observations, Derrington, Krauskopf, and Lennie (1984) proposed a cone-opponent color space, which is now commonly used (in color science and, to a large extent, visual neuroscience) to give a first-order approximation of the perceptual color-opponent processes. The color space is now famously known as the DKL color space6.\n\nThe Y-B channel is given by \\(a\\)S-(\\(b\\)L+\\(c\\)M), where \\(a\\), \\(b\\), and \\(c\\) are all positive values representing the contributions of the S, L, and M cones to the Y-B opponent process. It is generally said that this signal is delivered by the Koniocellular pathway.\nThe R-G channel is given by \\(d\\)L-\\(e\\)M, where \\(d\\) and \\(e\\) are all positive values representing the contributions of the L and M cones to the R-G opponent process. This opponent signal is generally said to be delivered by the Parvocellular pathway.\nThe Light-Dark or luminance channel is given by \\(f\\)L+\\(g\\)M, where \\(f\\) and \\(g\\) are all positive values representing the contributions of the L and M cones to the luminance channel. This luminance channel is meant to represent the LEF (Section 4.3.2), which generally is believed to be delivered by the Magnocellular pathway.\n\nThe DKL space operates not on raw cone responses but on response contrasts with respect to a perceptually neutral/achromatic color. The inherent assumption is that the achromatic color should have no strength in any of the three cone-opponent channels and be the origin in the cone-opponent space. The achromatic color depends on an observer’s state of chromatic adaptation, a topic we will discuss later in Section 6.3. People usually fit data to regress the values of the free parameters, and the exact values depend on which cone fundamentals are used and the normalization convention. Brainard (1996) describes one such procedure.\nSince the cone-opponent model operates on (contrast of) cone responses, a common theory of color vision is that it is a two-stage process: the wavelength encoding by cone photoreceptors followed by opponent encoding of cone responses post-receptorally. While the cone response encoding can perfectly explain the color matching experiments as we have see earlier, the cone opponent encoding is only an approximation of the hue cancellation experiments, as we will see next.\n\n\n4.4.4 There are Many Inconvenient Truths\nThe cone-opponent model is a good approximation for behavioral color-opponent mechanisms, but there are many inconsistencies between these two. Reconciling the two and thus elucidating how humans perceptually code opponent hues is still an open research question.\n\nP and K Pathways Do Not Fully Account For R-G and Y-B Opponent Processes\nThe opponent neurons clearly have what it takes to start accounting for the perceptual opponent processes, but the spectral tuning curves of those neurons have only a weak correlation with the hue cancellation curves. Thus, it is unlikely that excitation and inhibition in opponent neurons cause our perception of red-green and blue-yellow opponency.\nThe most jarring difference appears in the R-G process. The R-G hue cancellation curve (Figure 4.8 (C)) shows two perceptually neutral colors, as there are two zero-crossings. However, the spectral tuning curve of the R-G neurons (Figure 4.11 (A–B)) shows only one zero-crossing. These neurons do not predict the R-G neutral color in the short-wavelength range and, by extension, cannot explain the fact that short-wavelength violet-ish lights appear to have a red hue. Derrington, Krauskopf, and Lennie (1984) (also see Wandell (1995, fig. 9.18)) shows a great deal of variation of the spectral tuning property within P cells, making them even less certain as the sole candidate for R-G opponent mechanism.\nIn fact, people have shown that the perceptual R-G hue cancellation data can be fit by \\(a'\\)L-\\(b'\\)M+\\(c'\\)S, where \\(a'\\), \\(b'\\), and \\(c'\\) are cone contributions (Poirson and Wandell 1993; Bäuml and Wandell 1996). Intuitively, the contribution by S cones in the short-wavelength range could give rise to a positive response there. However, there is no physiological evidence that L cone and S cone responses combine at some point in the visual pathway, suggesting the phenomenological nature of these models.\nEven though the K pathway clearly shows the capability of carrying S vs. L+M signals, the latter do not accurately predict Y-B neutral signals and, thus, do not fully account for the Y-B hue opponency. That is, a color that leads to a null response (no significant increase or decrease compared to the spontaneous response rate) in the L-M channel is not perceptually pure yellow or pure blue (Shevell and Martin 2017, fig. 4f). Similarly, a color that causes a null response in the S-(L+M) channel is not perceptually pure red or pure green. That is, null-response colors in the DKL cone-opponent space are not perceptually neutral in the hue-opponent space, implying fundamental discrepancies between cone-opponent and hue-opponent spaces. \n\n\nM Pathway Does Not Fully Account For Luminance\nThe Magnocellular pathway (starting from the parasol RGCs) is said to be responsible for the dark-light opponent cells, but that poses a dilemma. We know that parasol RGCs have large RFs. A large RF is equivalently to applying an aggressive low-pass filter to the optical image; as a result, the M pathway has a low spatial acuity. So if the M pathway is fully responsible for mediating our luminance perception, we should be insensitive to spatial blurring (low-pass filtering) in the luminance signal. But the result is the opposite: our vision is very sensitive to spatial blurring in in the luminance channel (but relatively insensitive to blurring in the two color opponent channels).\n\n\n\n\n\n\nFigure 4.13: We take an image, decouple it into three channels: luminance, red-green, and blue-yellow. We then spatially blur one of the channels while keeping the other two channels unchanged and then reconstruct the image. Our vision is much more sensitive to spatially blurring in the luminance channel (a) than is to blurring in the red-green channel (b) and in the blue-yellow channel (c). This is the basis of chroma subsampling used in modern image and video compression algorithms. The original image is The Art of Painting from Johannes Vermeer Johannes Vermeer (1668). See another example in Wandell (1995, fig. 9.23).\n\n\n\nThis is illustrated in Figure 4.13, where we take an image, decouple it into three channels: luminance, red-green, and blue-yellow. We then spatially blur one of the channels while keeping the other two channels unchanged and then reconstruct the image. Our vision is much more sensitive to spatially blurring in the luminance channel (a) than is to blurring in the red-green channel (b) and in the blue-yellow channel (c); in fact, this is the basis of chroma subsampling, a key step in modern image and video compression algorithms. This suggests that the M pathway alone cannot be exclusively responsible for our luminance perception.\nGouras and Zrenner (1979) also shows that P cells, which are ordinarily thought of as L-M spectrally-opponent, could also give a LEF-like spectral tuning curve as if it acts as the luminance channel. The reason is that the surround signals reach a cell later than do the center signals, so at a high frequency the out-of-phase center-surround signals can actually come in the same phase.\n\n\nHue-Opponent Space is Not a Linear Transformation from Cone Space\nIt is perhaps not surprising, by now, that if there is a color space that can fully account for the perceptual coding of opponent hues, it is never going to be a linear transformation from the LMS space (or any other space that is a linear transformation away from the LMS space, e.g., the CIE 1931 XYZ space or the DKL cone-opponent space).\nAs we have seen above, for instance, the DKL space (Derrington, Krauskopf, and Lennie 1984), which is a linear transformation from the LMS cone space, does not fully account for the perceptual opponent processes, e.g., does not predict any unique hue. People have shown that one can construct a linear transformation from the LMS space that can accurately predict three of the four unique perceptual hues by fitting data from psychophysical measurements that do not presuppose the existence of opponent mechanisms (Poirson and Wandell 1993; Bäuml and Wandell 1996), but they cannot predict the fourth unique hue. Schrödinger (1925) also estimated a linear transformation between the cone response space and the hue-opponent space based on the four unique hues, but the transformation could not accurately predict the achromatic color (also see the commentary by Zaidi in Schrödinger (1994)).\nThe reason is that perceptually unique red and green hues are not collinear with white, the achromatic color that is perceptually neutral in both the Y-B and R-G channel, i.e., does not appear yellow, blue, red, nor green7. That is, red, white, and green do not lie on a line. Why is this significant? Assuming there was a linear transformation \\(T\\) from the cone responses to the strengths of the hue-opponent mechanisms:\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n\\text{Y/B}\\\\\n\\text{R/G}\\\\\n\\text{Lum}\n\\end{bmatrix}\n=\nT\n\\times\n\\begin{bmatrix}\nL\\\\\nM\\\\\nS\n\\end{bmatrix}\n\\end{aligned}\n\\]\nBoth unique red hue (\\([L_R, M_R, S_R]\\)) and unique green hue (\\([L_G, M_G, S_G]\\)) have no yellow (or blue) hue, so their response in the Y-B channel response would be 0:\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n\\text{0}\\\\\n|\\\\\n|\n\\end{bmatrix}\n=\nT\n\\times\n\\begin{bmatrix}\nL_R\\\\\nM_R\\\\\nS_R\n\\end{bmatrix},~~~\n\\begin{bmatrix}\n\\text{0}\\\\\n|\\\\\n|\n\\end{bmatrix}\n=\nT\n\\times\n\\begin{bmatrix}\nL_G\\\\\nM_G\\\\\nS_G\n\\end{bmatrix}\n\\end{aligned}\n\\]\nTherefore, any mixture of the unique red hue and the unique green hue would not appear to have a yellow hue either:\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n\\text{0}\\\\\n|\\\\\n|\n\\end{bmatrix}\n=\nT\n\\times\n\\begin{bmatrix}\na L_R + b L_G\\\\\na M_R + b M_G\\\\\na S_R + b S_G\n\\end{bmatrix},\n\\end{aligned}\n\\]\nwhere \\(a\\) and \\(b\\) are contributions of red and green to the mixed color. However, we know that when we mix red with green colors we get yellow. The fact that two colors without any yellow hue can generate a color that does have a yellow hue means the hue-opponent space cannot be a linear transformation from the LMS cone space.\n\n\n\n\n\n\nFigure 4.14: Circles are unique hues derived from psychophysics reported in Bäuml (1993). Fitting lines and extrapolating the lines give us estimations of unique hues that are spectral colors. Three of the four unique spectral hues (blue at 474 \\(\\text{nm}\\), green at 506 \\(\\text{nm}\\), and yellow at 568 \\(\\text{nm}\\)) can be accurately predicted by a linear transformation constructed by Bäuml and Wandell (1996), but not the unique red hue. The fact that the red, white, and green are not collinear suggests that there is no linear transformation between the hue-opponent space and the cone space. Adapted from Bäuml and Wandell (1996, fig. 12).\n\n\n\nFigure 4.14 illustrates this point with some real data. The empty markers are three sets of perceptually unique hues (which do not have to be spectral colors) measured psychophysically in Bäuml (1993). When we fit a straight line across each set of unique hues and extrapolate the line we can estimate what spectral colors are unique hues (blue Ⓑ, green Ⓖ, and yellow Ⓨ). No spectral color is seen as a unique red hue (all spectral red-ish colors appear to have a yellow hue), which requires a mixture of unique blue hue and a spectral red to cancel the yellow percept (Dimmick and Hubbard 1939b; Larimer, Krantz, and Cicerone 1975) (and also see the commentary by Zaidi in Schrödinger (1994)). Dimmick and Hubbard (1939a) measured that unique red hues Ⓡ are complementary to a spectral light at 494 \\(\\text{nm}\\); that is, spectral light at 494 \\(\\text{nm}\\), white Ⓦ, and unique red hues should fall on a straight line8.\nBäuml and Wandell (1996) constructed a linear transformation from the cone space to the hue-opponent space that can accurately predict the unique spectral hues of blue, green, and yellow. It is comforting, and corroborates others (Larimer, Cicerone, et al. 1974), that blue Ⓑ, white Ⓦ, and yellow Ⓨ are collinear, as would be required by a linear transformation from the cone space to the hue-opponent space: mixing colors that have no green or red hue will not give a color that does. But clearly the predicted red hue Ⓡ’ deviates significantly away from red hue Ⓡ from actual measurements. If we connect the unique green hue Ⓖ and unique red hue Ⓡ, the line would not across Ⓦ. This suggests that a simple linear transformation does not exist; at least the Y-B null-response axis is not linear with respect to the cone responses. Non-linear models have been proposed (Larimer, Krantz, and Cicerone 1975; Shevell and Martin 2017).",
    "crumbs": [
      "Human Visual System",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Color Vision</span>"
    ]
  },
  {
    "objectID": "hvs-color.html#sec-chpt-hvs-color-evo",
    "href": "hvs-color.html#sec-chpt-hvs-color-evo",
    "title": "4  Color Vision",
    "section": "4.5 Evolution of Color Vision",
    "text": "4.5 Evolution of Color Vision\nSee Lamb (2013), Lamb (2020), Lamb (2022), Lamb, Collin, and Pugh (2007), Shichida and Matsuyama (2009), Jacobs (2009), Bowmaker (2008), and Bowmaker (1998) for comprehensive discussions. We provide a concise summary of what is relevant to our discussions in this chapter.\n\n4.5.1 The Rise of Trichromacy\n\nProto-Vertebrates Had Four Cone Opsin Genes\nWhen studying the evolution of vision and performing comparative studies of vision across species, we must understand the differences in all the molecules that participate in phototransduction and the subsequent neural circuitry. The photopigment/opsin itself is one of the most important components (e.g., pigments differ in their peak absorption wavelength), and its evolution is the most well-understood. It is the main focus of our discussion.\n\nA primordial opsin gene that existed in ancestral bilateria (before the separation of protostomes and deuterostomes), through duplications and mutations, gave rise to many opsin genes, some of which are even expressed in non-visual opsins (e.g., ipRGCs and RPEs). One of them is called the Long-Wavelength Sensitive (LWS) opsin gene.  The LWS opsin gene was duplicated multiple times, each of which went through mutations, and eventually four opsin genes existed in proto-vertebrates. The sequence of duplications most likely went like the following. The LWS gene was duplicated, and the duplicated copy evolved to be Short-Wavelength Sensitive and is called the SWS opsin gene.  The SWS gene was duplicated again. One of the copies is called the SWS1 gene. The other was duplicated once again. Of its two copies, one is called the SWS2 gene, and the other is called the RHL gene. RHL means “rod like”; it is given the name because its duplicate would later evolve to be expressed in rhodopsin (photopigment in rods).\nAs a result, proto-vertebrates (primitive animals from which the vertebrates evolved; think of them as ancestors to vertebrates but are not vertebrates themselves) possessed 4 opsin genes: LWS, SWS1, SWS2, and RHL. These genes were expressed in pigments that mediate photopic vision, just like modern cones, so we will simply call them cone opsins, but keep in mind that proto-vertebrate “cones” are most definitely different from modern vertebrate cones.\nGenerally, a modern form of a gene is different from its ancestral form, but for simplicity, we usually call them by the same name. This might be a common source of confusion when studying evolutionary biology. For instance, modern vertebrates all have LWS opsin genes (that are slightly different); they are all evolved from, but most definitively different from, the ancestral LWS gene at the time of duplication that gave rise to the SWS branch. It is perhaps more rigorous to say, “the ancestral LWS was duplicated; one copy evolved to become the modern LWS genes, and the other evolved to become the ancestral SWS gene, which was duplicated again; its two copies are the ancestral SWS1 gene and the ancestral SWS2 gene”, but you can see how cumbersome this would be. Even with this it is sometimes unclear at which point in the evolution does “ancestral” refer to. \n\n\nMost Vertebrates are Tetrachromatic and Most Mammals are Dichromatic\nThen something remarkable happened. During the Cambrian epoch, at around 530 to 500 million years ago (Mya), there were two rounds of Whole Genome Duplication (WGD), a.k.a., polyploidy. Each WGD made a complete copy of all the genes. So two rounds of WGD would quadruple the number of genes. Each WGD was followed by a relatively short period of genome instability with extensive loss of genes. As a result, not all four copies (a result of two rounds of WGD) of a gene were retained. WGD in general is a major source of speciation, and the two rounds of WGD were responsible for the explosion of new species at the end of the Cambrian epoch (Cambrian explosion). WGDs increased the number of genes and made more genes available for mutation. As a result, there was a sudden radiation of species and diversification of life. Perhaps most importantly to human evolution, vertebrates appeared after the first round (1R) WGD.\nAs far as the four cone opsin genes are concerned, all their copies were lost except two copies of RHL, which are now called RH1 and RH2. So after the two rounds of WGD, our ancestral vertebrates possessed five cone opsin genes: SWS1, SWS2, RH1, RH2, and LWS. RH1 evolved to be expressed in rhodopsins (pigments for rods) to mediate scotopic vision in vertebrates; the other four evolved too but retained their ability to mediate photopic vision. This is why most vertebrates are tetrachromatic. From the sequence of duplications, we can also deduce that the rod pigment evolved after all four classes of cone pigment were already present (Okano et al. 1992). In fact, rod signals largely piggyback onto the pre-existing circuitry for cone signaling (Lamb 2016).\n\nEarly mammals arose at the age of dinosaurs and were nocturnal, so they had no need for strong color vision and lost two of the four cone opsin genes. Only LWS and SWS1 genes were retained. This is why most mammals are dichromatic.\n\n\n\n\n\n\nFigure 4.15: Left: Phylogenetic tree of vertebrate visual opsins; adapted from Bowmaker (2008, fig. 2). Note how humans are trichromatic (three cone opsins), chickens (non-mammalian vertebrates) are tetrachromatic (four cone opsins), and mice (mammals) are dichromatic (two cone opsins). Right: approximate spectral sensitivity of the five visual opsins; from Jacobs (2009, fig. 1).\n\n\n\nFigure 4.15 shows the phylogenetic tree of vertebrate visual opsins, of which there are five classes. I say “class” of opsin genes here because there are variations between, say, the SWS1 gene in humans and that in mice. Circles represent gene duplications. The first duplication on the ancestral LWS gene was local and gave rise to the SWS branch. The other duplications within the SWS branch are part of the two rounds of WGDs (which, recall, were followed by gene losses, which are omitted in the diagram). The local duplication within the LWS branch gave humans trichromacy, which we will discuss next.\n\n\nLWS Gene Duplication in Catarrhini Gave Human Trichromacy\nThen primates evolved from some mammals, first to prosimians, and then anthropoids (higher primates) split off from prosimians. Anthropoids include Platyrrhini (broad noses) and Catarrhini (narrow noses). Platyrrhini evolved into modern New World (South America) monkeys. Catarrhini is the common ancestor of Old World (Africa and Asia) monkeys, apes, and humans.\nTrichromacy emerged in an early Catarrhini through gene duplication. The LWS gene is duplicated. One copy is evolved to be expressed in modern L cones, and the other evolved to be expressed in modern M cones. Since the duplication occurred “only” about 30 Mya, which is fairly recent from an evolutionary standpoint, the L and M cone pigments are very similar. 96% of the amino acids in the L cone opsin and M cone opsin are the same (Nathans, Thomas, and Hogness 1986): they simply have not had much time to be mutated enough yet. With the SWS1 gene being expressed in S cones, all Catarrhini (including humans) are trichromatic. Figure 4.16 illustrates the duplication and the spectral sensitivities before and after the duplication.\n\n\n\n\n\n\nFigure 4.16: Left: LWS duplication gave two copies of the same gene in the X chromosome, and subsequent mutations to both copies gave rise to modern L and M cone opsin genes. Middle and Right: spectral sensitivities of the two cone pigments in mammals (middle) and the three sensitivity spectra in primates (note the \\(x\\)-axis is frequency rather than wavelength); from (Rodieck 1998, p. 218).\n\n\n\nWhat was the evolutionary pressure for the duplication that gave rise to the trichromacy? Since the duplication of the LWS gene gave us medium-wavelength pigments that peak at green-ish lights, one interesting theory is that the duplication offered the ability to distinguish red-ish colors from green-ish colors in order to select ripe from unripe fruit (or ripe fruits against the background of green leaves), which had an obvious evolutionary advantage (Hunt et al. 1998; Bowmaker 1998, p. 544).\n\n\n\n4.5.2 The Rise of Scotopic vs. Photopic Vision\n\nOnly Jawed Vertebrates Have a “Modern” Scotopic Vision\nVertebrates arose after 1R. 1R is important for the rod-cone duplex vision in today’s vertebrates. RH1, which eventually is evolved to be expressed in rhodopsins, appeared after 1R. 1R also duplicated other non-opsin genes important for phototransduction (e.g., G proteins, PDEs, cGMPs, etc.) (Lamb 2020). These genes evolved to be expressed in distinct isoforms of many of the molecules that participate in the rod vs. cone phototransduction cascades.\nJawless and jawed vertebrates (from which humans evolved) split after 1R, so they both possessed distinct scotopic and photopic vision. But there are differences in the scotopic vision between the two. Why? Because the second round (2R) WGD happened only to jawed vertebrates. 2R introduces a few new genes that allow jawed vertebrates to possess “true”, modern rod photoreceptors, where rod pigments are more thermally stable (lower dark noise) than cone pigments and regenerate faster than cone pigments (Lamb, Collin, and Pugh 2007; Lamb 2022). In jawless vertebrates, rods also have a cone-like anatomical structure, but 2R changed the morphology of rods in jawed vertebrates so that their rods look different from cones, e.g., having sealed-off discs. Interestingly, morphology apparently is not important for scotopic vision: northern hemisphere lampreys (a kind of jawless vertebrate) have rods that look like cones but physiologically behave like rods, e.g., they are very sensitive to lights, so they do mediate scotopic vision (Morshedian and Fain 2015). Whether you call the these photoreceptors rods or cones is largely a matter of definition.\n\n\nScotopic Vision Predates Vertebrates\nIt is also interesting to note that proto-vertebrates, likely some chordate ancestors of ours, also possessed distinct photopic and scotopic vision way before vertebrates, except those ancestral scotopic photoreceptors utilized pinopsin (Okano, Yoshizawa, and Fukada 1994), rather than rhodopsin, as their photopigments (Lamb 2020; Sato et al. 2018). Pinopsin evolved prior to 1R and, in fact, was duplicated along with LWS cone opsin in proto-vertebrates from a ciliary opsin (C-opsin).\nAfter 1R, presumably RH1-expressed rhodopsins had better performance under scotopic conditions, so they gradually superseded pinopsins to be the pigments expressed in scotopic photoreceptors. Mammals do not possess pinopsins anymore, but some vertebrate species still use pinopsins for scotopic vision (Sato et al. 2018), and many vertebrates use pinopsins for non-vision functions such as regulating circadian rhythm (as they are expressed in the pineal organ) (Takanaka et al. 1998).",
    "crumbs": [
      "Human Visual System",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Color Vision</span>"
    ]
  },
  {
    "objectID": "hvs-color.html#sec-chpt-hvs-color-cvd",
    "href": "hvs-color.html#sec-chpt-hvs-color-cvd",
    "title": "4  Color Vision",
    "section": "4.6 Color Vision Deficiencies",
    "text": "4.6 Color Vision Deficiencies\nNormal color vision is trichromatic in that there are three classes of cone photoreceptors on the retina. If the retina is deprived of the functions of one of two cone classes, the color vision is no longer trichromatic. Instead of being represented in a three-dimensional space, a color would now be expressed in a two-dimensional or one-dimensional space. Individuals with two functioning classes of cones are dichromatic, and those with one functioning cone class are monochromatic. Dichromatic vision is further classified into three types: Protanopia, where L cones are missing; Deuteranopia, where M cones are missing; and Tritanopia, where S cones are missing. An interesting form of dichromatic vision is called small-field dichromacy. At the central 20 arcmin of the fovea there are no S cones, so human vision is effectively dichromatic there. There are also individuals who have no functioning cones, only rods, and those people have rod monochromacy.\n\n\n\n\n\n\nFigure 4.17: Illustration of how cone fundamentals shift under each anomalous trichromatic vision; adapted from Milić, Novaković, and Milosavljević (2015).\n\n\n\nIn addition to strict dichromatic vision, another important form of CVD is anomalous trichromatic vision, where individuals have all three cone types, but the spectral sensitivity of a cone type deviates from normal. Protanomaly, Deuteranomaly, and Tritanomaly are the names given to the three types of anomalous trichromatic vision. Figure 4.17 illustrates how the cone fundamentals change under different anomalous trichromacy compared to normal trichromacy. For protanomalous and deuteranomalous individuals (by far the most common anomalous trichromats), their L and M cone fundamentals are closer than normal, so the L and M cone excitations are very similar. In theory their color vision is still three-dimensional, but the L and M dimensions are very correlated, which weakens their color discrimination ability.\nFinally, some females with anomalous trichromacy may have four cone classes on the retinal mosaic owing to random X chromosome inactivation. However, it is unclear whether their color vision is four-dimensional or they have a stronger color discrimination ability (Jordan et al. 2010; Simunovic 2010).\nWe will primarily focus on dichromatic vision, building a phenomenological model for simulating dichromacy, and then discuss the genetic basis of CVD now that we have a good understanding of the evolution of color vision.\n\n4.6.1 Models of Deficient Color Vision\nPerhaps one of the most important questions in CVD is this: what exactly do individuals with CVD actually see? In other words, how do we simulate a particular CVD? It is incredibly difficult to answer, since color is a subjective experience, and we can never be so certain of one another’s subjective experience. With the help of some luck (yes, some luck, from evolution) and some psychophysics, there is at least a consensus on how to model dichromatic vision. Taking Deuteranopia as an example, the interactive tutorial Zhu (2022c) walks through a commonly used model. We will give the main intuitions here.\n\nConfusion Lines\nDichromatic individuals see colors only in a 2D space, because they lack (the functionality of) one cone type. For instance, Deuteranopes lack the M cones and, thus, any color is encoded only in the L and S cone excitations, resulting in a 2D color space. Therefore, any colors that differ only in the M dimension are seen as the same color by a Deuteranope. A line parallel to the M dimension in the LMS space is called a Deuteranopic confusion line, as all colors on that line look the same to a Deuteranope. The left panel in Figure 4.18 plots two such confusion lines for Deuteranopia, although it is easy to see that there are infinitely many confusion lines.\n\n\n\n\n\n\nFigure 4.18: Left: two confusion lines for Deuteranopia (missing functioning M cones). Right: confusion lines in the CIE 1931 xy-chromaticity space; from Curran919 (2022).\n\n\n\nThe confusion lines are more commonly visualized in the CIE 1931 xy-chromaticity diagram, as illustrated in the three diagrams in Figure 4.18 to the right. We will discuss the xy diagram in the colorimetry lecture soon, but briefly, the xy diagram is a perspective projection from the LMS space that provides a useful 2D representation of colors by discarding the luminance dimension. Therefore, parallel confusion lines in the LMS space converge in the xy diagram.\n\n\n\n\n\n\nFigure 4.19: In a protanomalous color space, the L and M cone fundamentals are very similar, so the L and M cone excitations are similar. C1 and C2 are colors in a trichromatic color space. They are mapped to C1’ and C2’ in a protanomalous color space, where both are moved closer to the L=M line (while keeping the M-axis unchanged, since the M cone fundamental is unaffected). When C1’ and C2’ are sufficiently close, they become confusing.\n\n\n\nNot all colors on a dichromatic confusion line are confusing to the corresponding anomalous trichromatic individuals. From a modeling perspective, a typical approach is to restrict the confusing colors to a small segment on a confusion line (Flatla et al. 2015). Figure 4.19 illustrates a model to reason about this using Protanomaly as an example. C1 and C2 are two colors that differ only in the L cone response. For simplicity, Figure 4.19 shows only the L and M dimensions. Since the L and M cone fundamentals are very similar, the L and M cone responses are similar too (the extreme case being L = M). So C1 and C2 are mapped to C1’ and C2’ in a protanomalous color space, where both colors are pulled toward the L=M line. C1’ and C2’ are never going to exactly coincide, because the L and M cone fundamentals do not exactly overlap, but when C1’ and C2’ are sufficiently close, they can still be perceptually undiscriminable. This is not unique to CVD individuals: even for normal trichromats the color discrimination is not perfect, as has been demonstrated extensively through psychophysics Krauskopf and Karl (1992), which we will discuss in Section 5.5 soon.\n\n\nIsochromes\nWe know that all colors on the same confusion line are perceptually the same, but still we do not know, for all those colors on a confusion line, exactly what is the color a dichromate sees. The key to answering this question is the notion of isochromes, which are colors perceived correctly by a dichromate. The question is, how do we find the isochromes?\nIt is impossible to find isochromes by simply querying trichromats and dichromates. Imagine we have a normal trichromat and a Protanope looking at a color; even if they have the same color sensation, how would they communicate with each other about it? You might be tempted to find isochromes by asking a dichromate whether two colors appear the same.\nRemarkably, there is an exceedingly rare form of CVD called unilateral dichromacy, where an individual has one dichromatic eye and another trichromatic eye. Color matching between the two eyes by a unilateral dichromate would allow us to identify isochromes, assuming, of course, that the dichromatic eye and the trichromatic eye are similar to those of a “normal” dichromatic and trichromatic eye, respectively. This is a remarkable luck we get from nature; without unilateral dichromats, we might never be able to quantitatively study dichromats’ color vision. There are a handful of studies reported on unilaterial dichromates. Judd (1949) meticulously summarized data from prior studies, where only 8 had quantitative data that were useful. Sloan and Wollach (1948), Graham and Hsia (1958), and MacLeod and Lennie (1976) reported results for unilateral Protanopes/Deuteranopes, while Alpern, Kitahara, and Krantz (1983) reported results for a unilateral Tritanope.\nSuch studies show that monochromatic lights at 475 \\(\\text{nm}\\) and 575 \\(\\text{nm}\\) are isochromes for protanopes and deuteranopes (i.e., no significant difference between these two types when it comes to isochromes, but of course their confusion lines are different), and for tritanopes isochromes are found at 485 \\(\\text{nm}\\) and 660 \\(\\text{nm}\\).\n\n\n\n\n\n\nFigure 4.20: Left: isochrome planes and Protanopia confusion lines in LMS cone space; from Zhu (2022c). According to the Brettel, Viénot, and Mollon (1997) model, the intersection between the isochrome planes and a confusion line is the color a dichromat actually sees for all the colors on that confusion line. Right; the isochrome lines and Deuteranopia confusion lines in the xy-diagram; adapted from Curran919 (2022).\n\n\n\n\n\nBrettel et al. Model\nThere have been two main ways to build a model for dichromatic color vision: that of a phenomenological nature such as Brettel, Viénot, and Mollon (1997), Viénot, Brettel, and Mollon (1999), and Meyer and Greenberg (1988) and that based on first principles of the visual pathway, such as Jiang, Farrell, and Wandell (2016) and Rodriguez-Pardo and Sharma (2011). We will primarily focus on the phenomenological model described in Brettel, Viénot, and Mollon (1997). Like all other models, this model is not perfect. It makes assumptions that are not experimentally validated on unilateral dichromats, but it is a popular model that seems to work well in practice. Zhu (2022c) discusses cases where this model falls apart.\nBrettel, Viénot, and Mollon (1997) assumes that Equal-Energy White (EEW) is also an isochrome. In fact, they assume that the entire plane that contains EEW, 475 \\(\\text{nm}\\), 575 \\(\\text{nm}\\), and Black is an isochrome plane, on which all colors are isochromes for deuteranopes and protanopes. Similarly, the plane that contains EEW, 485 \\(\\text{nm}\\), 660 \\(\\text{nm}\\), and Black is an isochrome plane for tritanopes. Two caveats must be noted here. First, this is not validated against unilateral dichromats; it is just an assumption. Second, the isochrome “plane” is not an actual plane. It is more like two half-planes that share a border. For Protanopia and Deuteranopia, the two half-planes are almost parallel so that they look like part of one plane. The left panel in Figure 4.20 shows the two half planes (with distinct colors) for Protanopia.\nAssuming that the isochrome plane assumption by Brettel, Viénot, and Mollon (1997) is true, we can then reason about the colors that dichromats actually see: the intersection between the isochrome planes and a confusion line is the color a dichromat actually sees for all the colors on that confusion line. The left panel in Figure 4.20 visualizes this, where the trichromatic spectral locus is projected to the isochrome planes along the direction of the confusion lines. The resulting locus lies completely on the isochrome planes and represents how the spectral colors will actually be perceived by a Protanope.\nThe two isochrome half-planes become two line segments in the xy-diagram. The right panel in Figure 4.20 shows how the Brettel, Viénot, and Mollon (1997) model predicts a Deuteranope’s color perception in the xy-diagram.\nModeling anomalous trichromacy is “easy” as long as we know the new set of cone fundamentals. Assuming their subsequent neural processing is the same as that of normal trichromacy, we can then calculate the cone responses in an anomalous trichromatic color space given any light spectrum.\n\n\n\n4.6.2 CVD Assistive Technologies\nThere are many assistive techniques attempting to enhance the color vision of CVD individuals. Zhu et al. (2024, sec. 3) provides a good review.\n\nRe-Coloring Helps Color Discrimination\nBy far the most commonly used technique is called “re-coloring”; it is available on most Operating Systems in laptops, PCs, and smartphones. The idea is to apply a (usually linear) transformation to colors in an image (colloquially referred to as color filters) so that initially confusing colors become distinct (i.e., no longer on a confusion line).\nThe main limitation of re-coloring is that, while initially confusing colors might be distinguishable after a transformation, these colors will inevitably be confused with others. Fundamentally, a dichromat’s color vision is still two-dimensional, and the confusion lines are still there. Usually the transformation is designed so that a set of ecologically relevant, confusing colors (colors that we commonly encounter in everyday life and are important to discriminate, e.g., red flowers and green leaves) become distinct.\nRe-coloring can also be done optically. Many commercially available glasses for CVD individuals, such as those from EnChroma and VINO, use identical spectral “notch filters” for both eyes. The filter eliminates light from a narrow spectral band, where the human L and M cone sensitivities overlap the most. Recall that for Protanomalous and Deuteranomalous individuals, their L and M cone fundamentals are closer than normal, so the L and M cone excitations are very similar. The notch filters act to pull the two cone fundamentals apart and amplify the difference between L cone and M cone excitations. Therefore, in principle, the method can enhance color discrimination for anomalous trichromats but provides no benefit for strict dichromats. In comparative tests, the functional effectiveness of such glasses for anomalous trichromats is also not definitive (Gómez-Robledo et al. 2018; Patterson et al. 2022).\n\n\nColor Recognition By Reconstructing the Missing Dimension\nRe-coloring methods, thus, do not help with color recognition and naming (“pass me that pink marker” or “look at the person in a red shirt”), which is a common user complaint found in a user study (Geddes, Flatla, and Connelly 2023). For color recognition, we must restore a three-dimensional color space for dichromats. So the key is to somehow reconstruct the missing dimension.\nOne idea is to introduce binocular color disparity, where the stimuli are differentially altered for the two eyes. The idea was originated by Maxwell (1857) and then later revived by Cornsweet (1970). Maxwell conjectured that the disparity across the two eyes would essentially introduce a new dimension of perception, which would augment the existing 2D percept of a dichromat, providing 3D color perception. Knoblauch and McMahon (1995) shows that the improved color discrimination afforded by binocular filters might have limited use for Protanopes and Deuteranopes.\nZhu et al. (2024) introduces a smartphone App to reconstruct the missing dimension through temporally modulating colors. The idea is that as a user swipes their finger in the App, they apply a color-space transformation such that originally confusing colors undergo distinct color shifts. The combination of the initial 2D color precept with the induced temporal shifts reconstructs a new 3D space for the user. By spending time interacting with our system, users then learn to associate different color names in this new 3D space, thereby recognizing colors.\n\n\n\n4.6.3 Genetic Basis of CVD\nL and M cone genes are on the X chromosome. Because the L and M genes are created from a duplication, they are tandemly arrayed (spatially adjacent in a head-to-tail manner) and, thus, are subject to crossovers during recombination of meiosis, which might lead to deletion of a whole gene or hybrid genes in a X chromosome. This is the genetic basis of Protanopia, Deuteranopia, Protanomaly, and Deuteranomaly. Onishi et al. (2002) shows that only two in a sample of over 3000 macaque monkeys (an Old World monkey) have CVD, much lower than the CVD rate in humans, indicating that the crossovers might be recent (B. B. Lee 2008).\n\nIntergenic Crossovers Give Protanopia and Deuteranopia\nFigure 4.21 illustrates a crossover that can potentially lead to Deuteranopia. The two X chromosomes, due to an intergenic crossover, either lose or gain an M gene after recombination. One of the new X chromosomes has only the L cone gene, so an individual inheriting that X chromosome from the mother would get Deuteranopia. Interestingly, while the other X chromosome gets an additional M open gene, only the first two genes are sufficiently expressed. The fact that the L and M cone genes are in an X chromosome means that biological females are less vulnerable to Protanopia and Deuteranopia than biological males, simply because there are two X chromosomes in females. Even if one of the inherited X chromosomes has only, say, a L cone gene, the other inherited X chromosome, if normal, can still be sufficiently be expressed to give both L and M cones.\n\n\n\n\n\n\nFigure 4.21: An intergenic that can potentially give rise to Deuteranopia. Adapted from Rodieck (1998, p. 219–20).\n\n\n\n\n\nIntragenic Crossovers Give Protanomaly and Deuteranomaly\n\n\n\n\n\n\nFigure 4.22: An intragenic crossover that might give rise to Deuteranomaly and “Protanopia”. See text.\n\n\n\nIntragenic crossover might also occur during recombination, and this would lead to anomalous trichromacy. Figure 4.22 shows an intragenic crossover that might give rise to Deuteranomaly and “Protanopia”. The second X chromosome after recombination has a normal L opsin gene and another gene that is a mixture of the L cone gene and the M cone gene. The spectral sensitivity of such a hybrid pigment is in between that of an L cone and an M cone (Sharpe et al. 1999), as illustrated in the left panel in Figure 4.23. This is the source of anomalous trichromats.\n\n\n\n\n\n\nFigure 4.23: Left: the spectral sensitivities of the hybrid photopigments vary between those of the M- and L-cones depending on where the crossover occurs. Right: if two hybrid genes are sufficiently similar, the individual is effectively dichromatic. Slides credit: Andrew Stockman.\n\n\n\n\n\nIntergenic Crossovers Can Give Abnormal Protanopia and Deuteranopia\nIt is interesting to observe that intragenic crossovers can also give a dichromatic vision, although in a somewhat abnormal form. Observe in Figure 4.22 that the first X chromosome after recombination also has a hybrid gene. If the L cone gene dominates, that gene, when inherited, will be expressed in a pigment that has a spectral sensitivity that is closer to that of a L cone. If you want, you can still say that the inheriting individual has “Protanopia”, but its L cone sensitivity is different from that of a normal L cone.\nThe right panel in Figure 4.23 illustrates another scenario where dichromatic vision can arise from intragenic crossovers. If the two hybrid genes in the chromosome are sufficiently similar, they will be expressed in two pigments that are sufficiently similar, and the individual effectively has a dichromatic vision (Sharpe et al. 1999).\n\n\nDeuteranomaly is the Most Common CVD\nTritanopia and tritanomaly are much rarer than the other forms of CVDs. The gene for S cone opsin is in a nonsex chromosome (chromosome 7), which is not subject to L/M gene crossovers. Gene mutations causing changes in S-cone pigment are much more rare. Among CVDs that are caused by the crossovers, anomalous trichromacy is more common than strict dichromats, and Deuteranomaly is the most common (about 4.9% in caucasians) (Wyszecki and Stiles 1982, Table 1(5.4.2), p. 464), but the statistics certainly vary across ethnic groups (Birch 2012).\n\n\n\n\nAlpern, M, K Kitahara, and DH Krantz. 1983. “Perception of Colour in Unilateral Tritanopia.” The Journal of Physiology 335 (1): 683–97.\n\n\nBäuml, Karl-Heinz. 1993. “A Ratio Principle for a Red/Green and a Yellow/Blue Channel?” Perception & Psychophysics 53 (3): 338–44.\n\n\nBäuml, Karl-Heinz, and Brian A Wandell. 1996. “Color Appearance of Mixture Gratings.” Vision Research 36 (18): 2849–64.\n\n\nBaylor, Denis A, BJ Nunn, and JL Schnapf. 1987. “Spectral Sensitivity of Cones of the Monkey Macaca Fascicularis.” The Journal of Physiology 390 (1): 145–60.\n\n\nBhutajata. 2015. “Color temperature black body; CC BY-SA 4.0 license.” https://commons.wikimedia.org/wiki/File:Color_temperature_black_body_800-12200K.svg.\n\n\nBirch, Jennifer. 2012. “Worldwide Prevalence of Red-Green Color Deficiency.” JOSA A 29 (3): 313–20.\n\n\nBowmaker, James K. 1998. “Evolution of Colour Vision in Vertebrates.” Eye 12 (3): 541–47.\n\n\n———. 2008. “Evolution of Vertebrate Visual Pigments.” Vision Research 48 (20): 2022–41.\n\n\nBrainard, DH. 1996. “Cone Contrast and Opponent Modulation Color Spaces.” In Human Color Vision, 2nd ed., 563–79. Optical Society of America.\n\n\nBrettel, Hans, Françoise Viénot, and John D Mollon. 1997. “Computerized Simulation of Color Appearance for Dichromats.” Josa a 14 (10): 2647–55.\n\n\nBroadbent, Arthur D. 2004. “A Critical Review of the Development of the CIE1931 RGB Color-Matching Functions.” Color Research & Application 29 (4): 267–72.\n\n\n———. 2008. “Calculation from the Original Experimental Data of the CIE 1931 RGB Standard Observer Spectral Chromaticity Coordinates and Color Matching Functions.” Québec, Canada: Département de génie Chimique, Université de Sherbrooke, 1–17.\n\n\nBrown, Paul K, and George Wald. 1964. “Visual Pigments in Single Rods and Cones of the Human Retina.” Science 144 (3614): 45–52.\n\n\nCornsweet, Tom. 1970. Visual Perception. Academic press.\n\n\nCrawford, BH. 1949. “The Scotopic Visibility Function.” Proceedings of the Physical Society. Section B 62 (5): 321.\n\n\nCurran919. 2022. “Color Blind Confusion Lines; CC BY-SA 4.0 license.” https://commons.wikimedia.org/wiki/File:Color_Blind_Confusion_Lines.png.\n\n\nDacey, Dennis M, and Barry B Lee. 1994. “The’blue-on’opponent Pathway in Primate Retina Originates from a Distinct Bistratified Ganglion Cell Type.” Nature 367 (6465): 731–35.\n\n\nDartnall, Herbert JA, James K Bowmaker, and John Dixon Mollon. 1983. “Human Visual Pigments: Microspectrophotometric Results from the Eyes of Seven Persons.” Proceedings of the Royal Society of London. Series B. Biological Sciences 220 (1218): 115–30.\n\n\nDe Valois, RL, CJ Smith, ST Kitai, and AJ Karoly. 1958. “Response of Single Cells in Monkey Lateral Geniculate Nucleus to Monochromatic Light.” Science 127 (3292): 238–39.\n\n\nDe Valois, Russell L, Israel Abramov, and Gerald H Jacobs. 1966. “Analysis of Response Patterns of LGN Cells.” JOSA 56 (7): 966–77.\n\n\nDerrington, Andrew M, John Krauskopf, and Peter Lennie. 1984. “Chromatic Mechanisms in Lateral Geniculate Nucleus of Macaque.” The Journal of Physiology 357 (1): 241–65.\n\n\nDeValois, Russell L, and Karen K DeValois. 1990. “Spatial Vision.”\n\n\nDimmick, Forrest L, and Margaret R Hubbard. 1939a. “The Spectral Components of Psychologically Unique Red.” The American Journal of Psychology 52 (3): 348–53.\n\n\n———. 1939b. “The Spectral Location of Psychologically Unique Yellow, Green, and Blue.” The American Journal of Psychology 52 (2): 242–54.\n\n\nField, Greg D, Alexander Sher, Jeffrey L Gauthier, Martin Greschner, Jonathon Shlens, Alan M Litke, and EJ Chichilnisky. 2007. “Spatial Properties and Functional Organization of Small Bistratified Ganglion Cells in Primate Retina.” Journal of Neuroscience 27 (48): 13261–72.\n\n\nFlatla, David R, Alan R Andrade, Ross D Teviotdale, Dylan L Knowles, and Craig Stewart. 2015. “ColourID.” In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems. ACM.\n\n\nGeddes, Connor, David R Flatla, and Ciabhan L Connelly. 2023. “30 Years of Solving the Wrong Problem: How Recolouring Tool Design Fails Those with Colour Vision Deficiency.” In Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility, 1–13.\n\n\nGegenfurtner, Karl R. 2003. “Cortical Mechanisms of Colour Vision.” Nature Reviews Neuroscience 4 (7): 563–72.\n\n\nGlassner, Andrew S. 1995. Principles of Digital Image Synthesis. Elsevier.\n\n\nGómez-Robledo, Luis, EM Valero, R Huertas, MA Martı́nez-Domingo, and Javier Hernández-Andrés. 2018. “Do EnChroma Glasses Improve Color Vision for Colorblind Subjects?” Optics Express 26 (22): 28693–703.\n\n\nGouras, P, and E Zrenner. 1979. “Enhancement of Luminance Flicker by Color-Opponent Mechanisms.” Science 205 (4406): 587–89.\n\n\nGraham, Clarence Henry, and Yun Hsia. 1958. “Color Defect and Color Theory: Studies of Normal and Color-Blind Persons, Including a Subject Color-Blind in One Eye but Not in the Other.” Science 127 (3300): 675–82.\n\n\nGuild, John. 1931. “The Colorimetric Properties of the Spectrum.” Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character 230 (681-693): 149–87.\n\n\nHering, Ewald. 1878. Zur Lehre Vom Lichtsinne: Sechs Mittheilungen an Die Kaiser. Akad. Der Wissenschaften in Wien. C. Gerold’s Sohn.\n\n\n———. 1964. Outlines of a Theory of the Light Sense (Translation by Jameson and Hurvish; Originally Published in 1878). Harvard University Press.\n\n\nHunt, David M, Kanwaljit S Dulai, Jill A Cowing, Catherine Julliot, John D Mollon, James K Bowmaker, Wen-Hsiung Li, and David Hewett-Emmett. 1998. “Molecular Evolution of Trichromacy in Primates.” Vision Research 38 (21): 3299–3306.\n\n\nHurvich, Leo M, and Dorothea Jameson. 1955. “Some Quantitative Aspects of an Opponent-Colors Theory. II. Brightness, Saturation, and Hue in Normal and Dichromatic Vision.” JOSA 45 (8): 602–16.\n\n\n———. 1957. “An Opponent-Process Theory of Color Vision.” Psychological Review 64 (6p1): 384.\n\n\nJacobs, Gerald H. 2009. “Evolution of Colour Vision in Mammals.” Philosophical Transactions of the Royal Society B: Biological Sciences 364 (1531): 2957–67.\n\n\nJameson, Dorothea, and Leo M Hurvich. 1955. “Some Quantitative Aspects of an Opponent-Colors Theory. I. Chromatic Responses and Spectral Saturation.” JOSA 45 (7): 546–52.\n\n\nJiang, Haomiao, Joyce Farrell, and Brian Wandell. 2016. “A Spectral Estimation Theory for Color Appearance Matching.” Electronic Imaging 28:1–4.\n\n\nJohannes Vermeer. 1668. “The Art of Painting; released into the public domain.” https://en.wikipedia.org/wiki/File:Jan_Vermeer_-_The_Art_of_Painting_-_Google_Art_Project.jpg.\n\n\nJordan, Gabriele, Samir S Deeb, Jenny M Bosten, and John D Mollon. 2010. “The Dimensionality of Color Vision in Carriers of Anomalous Trichromacy.” Journal of Vision 10 (8): 12–12.\n\n\nJudd, Deane B. 1949. “The Color Perceptions of Deuteranopic and Protanopic Observers.” JOSA 39 (3): 252–56.\n\n\n———. 1951. “Report of US Secretariat Committee on Colorimetry and Artificial Daylight.” CIE Proceedings, 1951 1:11.\n\n\nJudd, Deane B, David L MacAdam, Günter Wyszecki, HW Budde, HR Condit, ST Henderson, and JL Simonds. 1964. “Spectral Distribution of Typical Daylight as a Function of Correlated Color Temperature.” Josa 54 (8): 1031–40.\n\n\nKnoblauch, Kenneth, and Matthew J McMahon. 1995. “Discrimination of Binocular Color Mixtures in Dichromacy: Evaluation of the Maxwell–Cornsweet Conjecture.” JOSA A 12 (10): 2219–29.\n\n\nKrauskopf, John, and Gegenfurtner Karl. 1992. “Color Discrimination and Adaptation.” Vision Research 32 (11): 2165–75.\n\n\nKries, Johannes von. 1905. “Übersicht Der Tatsachen, Ergebnisse für Die Theoretische Auffassung Des Sehorgans: Zonentheorie.” In Handbuch Der Physiologie Des Menschen, Dritter Band: Physiologie Der Sinne, edited by Willibald Nagel, 3rd ed., 269--274. Vieweg und Sohn, Braunschweig.\n\n\nLamb, Trevor D. 2013. “Evolution of Phototransduction, Vertebrate Photoreceptors and Retina.” Progress in Retinal and Eye Research 36:52–119.\n\n\n———. 2016. “Why Rods and Cones?” Eye 30 (2): 179–85.\n\n\n———. 2020. “Evolution of the Genes Mediating Phototransduction in Rod and Cone Photoreceptors.” Progress in Retinal and Eye Research 76:100823.\n\n\n———. 2022. “Photoreceptor Physiology and Evolution: Cellular and Molecular Basis of Rod and Cone Phototransduction.” The Journal of Physiology 600 (21): 4585–4601.\n\n\nLamb, Trevor D, Shaun P Collin, and Edward N Pugh. 2007. “Evolution of the Vertebrate Eye: Opsins, Photoreceptors, Retina and Eye Cup.” Nature Reviews Neuroscience 8 (12): 960–76.\n\n\nLarimer, James, Carol M Cicerone, et al. 1974. “Opponent-Process Additivity-i: Red/Green Equilibria.” Vision Research 14 (11): 1127–40.\n\n\nLarimer, James, David H Krantz, and Carol M Cicerone. 1975. “Opponent Process Additivity—II. Yellow/Blue Equilibria and Nonlinear Models.” Vision Research 15 (6): 723–31.\n\n\nLee, Barry B. 2008. “The Evolution of Concepts of Color Vision.” Neurociencias 4 (4): 209.\n\n\nLee, BB, PR Martin, and A Valberg. 1988. “The Physiological Basis of Heterochromatic Flicker Photometry Demonstrated in the Ganglion Cells of the Macaque Retina.” The Journal of Physiology 404 (1): 323–47.\n\n\nLennie, Peter, Joel Pokorny, and Vivianne C Smith. 1993. “Luminance.” JOSA A 10 (6): 1283–93.\n\n\nMacAdam, David L. 1942. “Visual Sensitivities to Color Differences in Daylight.” Josa 32 (5): 247–74.\n\n\n———. 1943. “Specification of Small Chromaticity Differences.” Josa 33 (1): 18–26.\n\n\nMacLeod, Donald IA, and Peter Lennie. 1976. “Red-Green Blindness Confined to One Eye.” Vision Research 16 (7): 691–702.\n\n\nMarco Polo. 2007. “CIE1931 RGB CMF; released into the public domain by the copyright holder.” https://commons.wikimedia.org/wiki/File:CIE1931_RGBCMF.svg.\n\n\nMarks, WB, William H Dobelle, and Edward F MacNichol Jr. 1964. “Visual Pigments of Single Primate Cones.” Science 143 (3611): 1181–83.\n\n\nMaxwell, James Clerk. 1857. “XVIII.—Experiments on Colour, as Perceived by the Eye, with Remarks on Colour-Blindness.” Earth and Environmental Science Transactions of the Royal Society of Edinburgh 21 (2): 275–98.\n\n\nMeyer, Gary W, and Donald P Greenberg. 1988. “Color-Defective Vision and Computer Graphics Displays.” IEEE Computer Graphics and Applications 8 (5): 28–40.\n\n\nMilić, Neda, Dragoljub Novaković, and Branko Milosavljević. 2015. “Enhancement of Image Content for Observers with Colour Vision Deficiencies.” Color Image and Video Enhancement, 315–43.\n\n\nMollon, John D. 2003. “Introduction: Thomas Young and the Trichromatic Theory of Colour Vision.” In Normal & Defective Colour Vision, edited by John D Mollon, Joel Pokorny, and Ken Knoblauch, 19--34. Oxford University Press.\n\n\nMorshedian, Ala, and Gordon L Fain. 2015. “Single-Photon Sensitivity of Lamprey Rods with Cone-Like Outer Segments.” Current Biology 25 (4): 484–87.\n\n\nNathans, Jeremy, Darcy Thomas, and David S Hogness. 1986. “Molecular Genetics of Human Color Vision: The Genes Encoding Blue, Green, and Red Pigments.” Science 232 (4747): 193–202.\n\n\nNewton, Isaac. 1704. Opticks, or, a Treatise of the Reflections, Refractions, Inflections & Colours of Light. London: Smith; Walford.\n\n\nOkano, Toshiyuki, DAISUKE KoJIMA, Yoshitaka Fukada, Yoshinori Shichida, and Toru Yoshizawa. 1992. “Primary Structures of Chicken Cone Visual Pigments: Vertebrate Rhodopsins Have Evolved Out of Cone Visual Pigments.” Proceedings of the National Academy of Sciences 89 (13): 5932–36.\n\n\nOkano, Toshiyuki, Toru Yoshizawa, and Yoshitaka Fukada. 1994. “Pinopsin Is a Chicken Pineal Photoreceptive Molecule.” Nature 372 (6501): 94–97.\n\n\nOnishi, Akishi, Satoshi Koike, Miki Ida-Hosonuma, Hiroo Imai, Yoshinori Shichida, Osamu Takenaka, Akitoshi Hanazawa, et al. 2002. “Variations in Long-and Middle-Wavelength-Sensitive Opsin Gene Loci in Crab-Eating Monkeys.” Vision Research 42 (3): 281–92.\n\n\nPatterson, EJ, RR Mastey, JA Kuchenbecker, Jessica Rowlan, Jay Neitz, Maureen Neitz, and Joseph Carroll. 2022. “Effects of Color-Enhancing Glasses on Color Vision in Congenital Red-Green Color Deficiencies.” Optics Express 30 (17): 31182–94.\n\n\nPoirson, Allen B, and Brian A Wandell. 1993. “Appearance of Colored Patterns: Pattern–Color Separability.” JOSA A 10 (12): 2458–70.\n\n\nRodieck, Robert W. 1998. The First Steps in Seeing. Sinauer Associates.\n\n\nRodriguez-Pardo, Carlos Eduardo, and Gaurav Sharma. 2011. “Dichromatic Color Perception in a Two Stage Model: Testing for Cone Replacement and Cone Loss Models.” In 2011 IEEE 10th IVMSP Workshop: Perception and Visual Signal Analysis, 12–17. IEEE.\n\n\nSato, Keita, Takahiro Yamashita, Keiichi Kojima, Kazumi Sakai, Yuki Matsutani, Masataka Yanagawa, Yumiko Yamano, et al. 2018. “Pinopsin Evolved as the Ancestral Dim-Light Visual Opsin in Vertebrates.” Communications Biology 1 (1): 156.\n\n\nSchnapf, JL, TW Kraft, and Denis A Baylor. 1987. “Spectral Sensitivity of Human Cone Photoreceptors.” Nature 325 (6103): 439–41.\n\n\nSchrödinger, Erwin. 1925. “Über Das Verhältnis Der Vierfarben-Zur Dreifarbentheorie.” Sitzungberichte Abt 2a, Mathematik, Astronomie, Physik, Meteorologie Und Mechanik Akademie Der Wissenschaften in Wien, Mathematisch-Naturwissenschaftliche Klasse 134:471–90.\n\n\n———. 1994. “On the Relationship of Four-Color Theory to Three-Color Theory (Translation by National Translation Center; Originally Published in 1925).” Color Research and Application 19 (1): 37.\n\n\nService, Phil. 2016. “The Wright – Guild Experiments and the Development of the CIE 1931 RGB and XYZ Color Spaces.”\n\n\nSharpe, Lindsay T, Andrew Stockman, Wolfgang Jagla, and Herbert Jägle. 2005. “A Luminous Efficiency Function, v*(\\(\\lambda\\)), for Daylight Adaptation.” Journal of Vision 5 (11): 3–3.\n\n\n———. 2011. “A Luminous Efficiency Function, VD65*(\\(\\lambda\\)), for Daylight Adaptation: A Correction.” Color Research & Application 36 (1): 42–46.\n\n\nSharpe, Lindsay T, Andrew Stockman, Herbert Jägle, and Jeremy Nathans. 1999. “Opsin Genes, Cone Photopigments, Color Vision, and Color Blindness.” Color Vision: From Genes to Perception 351:3–52.\n\n\nShevell, Steven K, and Paul R Martin. 2017. “Color Opponency: Tutorial.” JOSA A 34 (7): 1099–1108.\n\n\nShichida, Yoshinori, and Take Matsuyama. 2009. “Evolution of Opsins and Phototransduction.” Philosophical Transactions of the Royal Society B: Biological Sciences 364 (1531): 2881–95.\n\n\nSimunovic, MP. 2010. “Colour Vision Deficiency.” Eye 24 (5): 747–55.\n\n\nSloan, Louise L, and Lorraine Wollach. 1948. “A Case of Unilateral Deuteranopia.” JOSA 38 (6): 502–9.\n\n\nStiles, Walter Stanley, and Jennifer M Burch. 1959. “NPL Colour-Matching Investigation: Final Report (1958).” Optica Acta: International Journal of Optics 6 (1): 1–26.\n\n\nStiles, WS, and JM Burch. 1955. “Interim Report to the Commission Internationale de l’eclairage, Zurich, 1955, on the National Physical Laboratory’s Investigation of Colour-Matching (1955).” Optica Acta: International Journal of Optics 2 (4): 168–81.\n\n\nStockman, Andrew, and Lindsay T Sharpe. 2000. “The Spectral Sensitivities of the Middle-and Long-Wavelength-Sensitive Cones Derived from Measurements in Observers of Known Genotype.” Vision Research 40 (13): 1711–37.\n\n\nStockman, Andrew, Lindsay T Sharpe, and Clemens Fach. 1999. “The Spectral Sensitivity of the Human Short-Wavelength Sensitive Cones Derived from Thresholds and Color Matches.” Vision Research 39 (17): 2901–27.\n\n\nSvaetichin, G. 1953. “The Cone Action Potential.” Acta Physiol Scand 29 29:565–600.\n\n\nSvaetichin, G. 1956. “Spectral Response Curves from Single Cones.” Acta Physiol Scand 39:17–46.\n\n\nSvaetichin, G., and Edward F MacNichol Jr. 1958. “Retinal Mechanisms for Chromatic and Achromatic Vision.” Annals of the New York Academy of Sciences 74 (2): 385–404.\n\n\nTakanaka, Yoko, Toshiyuki Okano, Masayuki Iigo, and Yoshitaka Fukada. 1998. “Light-Dependent Expression of Pinopsin Gene in Chicken Pineal Gland.” Journal of Neurochemistry 70 (3): 908–13.\n\n\nViénot, Françoise, Hans Brettel, and John D Mollon. 1999. “Digital Video Colourmaps for Checking the Legibility of Displays by Dichromats.” Color Research & Application 24 (4): 243–52.\n\n\nVos, Johannes J. 1978. “Colorimetric and Photometric Properties of a 2 Fundamental Observer.” Color Research & Application 3 (3): 125–28.\n\n\nWald, George. 1945. “Human Vision and the Spectrum.” Science 101 (2635): 653–58.\n\n\nWandell, Brian A. 1995. Foundations of Vision. Sinauer Associates.\n\n\nWiesel, Torsten N, and David H Hubel. 1966. “Spatial and Chromatic Interactions in the Lateral Geniculate Body of the Rhesus Monkey.” Journal of Neurophysiology 29 (6): 1115–56.\n\n\nWright, WD. 1928. “A Trichromatic Colorimeter with Spectral Primaries.” Transactions of the Optical Society 29 (5): 225.\n\n\n———. 1930. “A Re-Determination of the Mixture Curves of the Spectrum.” Transactions of the Optical Society 31 (4): 201.\n\n\nWright, William David. 1929. “A Re-Determination of the Trichromatic Coefficients of the Spectral Colours.” Transactions of the Optical Society 30 (4): 141.\n\n\nWyszecki, Günther, and WS Stiles. 1982. Color Science: Concepts and Methods, Quantitative Data and Formulae. John wiley & sons.\n\n\nYoung, Thomas. 1802. “II. The Bakerian Lecture. On the Theory of Light and Colours.” Philosophical Transactions of the Royal Society of London, no. 92, 12–48.\n\n\nZhu, Yuhao. 2020. “How the CIE 1931 RGB Color Matching Functions Were Developed from the Initial Color Matching Experiments.” https://yuhaozhu.com/blog/cmf.html.\n\n\n———. 2022a. “Interative Tutorial: Building a Color Space From Cone Fundamentals.” https://horizon-lab.org/colorvis/cone2cmf.html.\n\n\n———. 2022b. “Interative Tutorial: CIE 1931 XYZ Color Space.” https://horizon-lab.org/colorvis/xyz.html.\n\n\n———. 2022c. “Interative Tutorial: Understanding and Modeling Color Blindness.” https://horizon-lab.org/colorvis/colorblind.html.\n\n\n———. 2022d. “Interative Tutorial: Visualizing Human Visual Gamut.” https://horizon-lab.org/colorvis/gamutvis.html.\n\n\nZhu, Yuhao, Ethan Chen, Colin Hascup, Yukang Yan, and Gaurav Sharma. 2024. “Computational Trichromacy Reconstruction: Empowering the Color-Vision Deficient to Recognize Colors Using Augmented Reality.” In Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology, 1–17.",
    "crumbs": [
      "Human Visual System",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Color Vision</span>"
    ]
  },
  {
    "objectID": "hvs-color.html#footnotes",
    "href": "hvs-color.html#footnotes",
    "title": "4  Color Vision",
    "section": "",
    "text": "After all, artificial lights are a very recent thing in the scale of evolution, so our HVS has not had a chance to adapt to non-daylight colors yet, if ever.↩︎\nOne subtlety is that Baylor, Nunn, and Schnapf (1987) used suction electrode to measure electrical responses (Section 3.2.2), so they obtained only the relative absorbance not the absolute absorption of the pigments. So what they actually ended up doing is to use the psychophysical CMFs to fit the peak axial absorption and calculate the cone fundamentals, and show that the regressed CMFs from the so-obtained cone fundamentals match that from psychophysics.↩︎\nsee translation in Hering (1964)↩︎\nsee translation in Schrödinger (1994)↩︎\nIn later research by Jameson and Hurvich, their white-black function was made equal to the CIE 1924 luminous efficiency function (Hurvich and Jameson 1955, p. 604), which is known to have severe flaws at low wavelengths and which is later corrected by Judd (1951) and Vos (1978). Compared to the Judd and Vos corrections, the function shown here has the advantage of being “physiologically relevant” in that the LEF is a linear combination of the cone fundamentals, whereas both the CIE 1924 LEF and its later corrections are not intentionally designed to be linear combinations of anything.↩︎\nnamed after the three authors; the L is Peter Lennie, who was twice on the faculty at University of Rochester and served as the Provost↩︎\nAgain, what is considered achromatic depends on the observer’s adaptation state; there is no single achromatic color.↩︎\nOf course, it is conceivable the result might vary in population and depend on the adaptation state (i.e., what is considered white/achromatic).↩︎",
    "crumbs": [
      "Human Visual System",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Color Vision</span>"
    ]
  },
  {
    "objectID": "hvs-colorimetry.html",
    "href": "hvs-colorimetry.html",
    "title": "5  Colorimetry",
    "section": "",
    "text": "5.1 CIE 1931 XYZ Space\nColorimetry is concerned with quantitatively studying color, a subjective experience. Not until we can put our experience into numbers can we rigorously study colors. In Section 4.2, we have seen two ways to geometrically interpret a color as a point in a three-dimensional space: the cone space and the CIE 1931 RGB space.\nThe main goal of this chapter is to introduce other commonly used color space to quantitatively analyze colors. Some of these color spaces are device-independent, just like the LMS cone space, so they permit us to analyze all human visible colors. Other color spaces are device-dependent; they are concerned with colors that can physically be captured (by an imaging device) or produceed (by a display device). Studying device-dependent spaces allows us to appreciate many subtle but important issues in real-world color workflows.\nClassic colorimetry is concerned only with color matching under the same viewing condition. It tells us if two objects or light sources have the same color when viewed under exactly the same conditions (e.g., ambient illumination). It does not tell us 1) how different two colors are and 2) the actual appearance of a color, which depends on the viewing condition. Color difference will be discussed in Section 5.5; Fairchild (2013) is a classic reference on color appearance modeling, which we will touch upon in Section 6.3.\nThere are two slight inconveniences with the CIE 1931 RGB color space. First, it depends on the exact primary colors (and reference white) you choose. Second, there are also inevitably going to be colors that can be “produced” only by using negative amounts of the primaries, no matter what primaries you choose. While mathematically and physically rigorous, it is not quite intuitive. So CIE in 1931 wanted to standardize a color space that 1) can be used as a “common language” (without having to laboriously specify what the primaries are used every time you say “the RGB color space”) and that 2) all the human-visible colors are produced by mixing non-negative amounts of the primaries. That color space is called the CIE 1931 XYZ color space, sometimes referred to simply as the XYZ color space.\nYou might be wondering: isn’t the LMS cone space already a color space that satisfies the two conditions above, and if so, why do we have to invent a new XYZ space? The cone space is tied intrinsically to the HVS, so it does not vary (significantly) in population. It is also a color space where all the colors are expressed using positive amounts of the primaries (cone responses). These are all true, but remember the cone fundamentals were not reliably available back in 1931 (Section 3.2).\nFairman, Brill, and Hemmendinger (1997), Brill (1998), and Service (2016, sec. 4) describe the process and the (sometimes rather arbitrary) design decisions that went into turning the CIE 1931 RGB space into the 1931 XYZ space. Zhu (2022c) is an interactive tutorial that walks through the math.\nThe bottom line is that the transformation from the CIE RGB to the XYZ space is constructed to be a linear transformation. Figure 5.1 shows how the spectral locus is transformed from the RGB to the XYZ space, governed by the matrix \\(\\mathbf{T}_{rgb2xyz}\\). We can see that in the RGB space the spectral locus enters negative octants, but it stays entirely within the all-positive, first octant in the XYZ space. The transformation also gives a new set of CMFs in the XYZ space. The Y CMF is intentionally designed to match the CIE 1924 Luminous Efficiency Function (LEF), so that by looking at the Y value of a color, we can tell what its relative luminance is (refer to Section 4.3.2 for the definition of the LEF and its various caveats).",
    "crumbs": [
      "Human Visual System",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Colorimetry</span>"
    ]
  },
  {
    "objectID": "hvs-colorimetry.html#sec-chpt-hvs-cori-xyz",
    "href": "hvs-colorimetry.html#sec-chpt-hvs-cori-xyz",
    "title": "5  Colorimetry",
    "section": "",
    "text": "Figure 5.1: The CIE 1931 XYZ color space (right) is constructed to be a linear transformation from the CIE 1931 RGB color space (left). Notice how a color, say, 600 \\(\\text{nm}\\) spectral light is represented differently in the two color spaces. This figure visualizes how the spectral locus and the CMFs are transformed. The exact coefficients of the transformation matrix \\(T_{rgb2xyz}\\) are omitted here but are widely available online. The CIE 1931 RGB CMFs figure is adapted from Marco Polo (2007), and the XYZ CMFs figure is adapted from Acdx (2009).",
    "crumbs": [
      "Human Visual System",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Colorimetry</span>"
    ]
  },
  {
    "objectID": "hvs-colorimetry.html#sec-chpt-hvs-cori-chro",
    "href": "hvs-colorimetry.html#sec-chpt-hvs-cori-chro",
    "title": "5  Colorimetry",
    "section": "5.2 Chromaticity Diagram",
    "text": "5.2 Chromaticity Diagram\nHow do a color that is mixed from 1:2:4 units of RGB primaries and a color that is mixed from 2:4:8 units of the primaries relate? The amount of a primary is directly proportional to the power of that primary, so the second color can be obtained by doubling the power of each primary in the first color. Similarly, halving the power of each primary in the second color gets us the first color. Intuitively, lights that have the same primary quantity ratio have the same “objective color quality” while differing in the intensity.\n\n5.2.1 Chromaticity is the Result of a Perspective Projection\nMore formally, we can calculate the primary ratio \\(r:g:b\\) of a color and then normalize the ratio such that \\(r + g + b = 1\\) (100%). The so-calculated \\(r\\), \\(g\\), \\(b\\) values of a color are called the (RGB) chromaticity values of that color. Mathematically, the chromaticity of a color defined in an RGB space is calculated from its absolute quantity by:\n\\[\n\\begin{aligned}\n    r = \\frac{R}{R+G+B}\\\\\n    g = \\frac{G}{R+G+B}\\\\\n    b = \\frac{B}{R+G+B}\n\\end{aligned}\n\\]\nGeometrically, going from the RGB values of a color to the rgb chromaticity is equivalent to a perspective projection, where we project an [R, G, B] point through the origin to the \\(r+g+b=1\\) plane. The left panel in Figure 5.2 visualizes this projection. Each line that goes through the origin is an “equi-chromaticity” line, in that all the colors on that line have the same chromaticity. The spectral locus is so projected to the \\(r+g+b=1\\) plane. Since there are only two degrees of freedom in chromaticity, we can visualize the chromaticity in a two-dimensional space, and usually the \\(r\\) and \\(g\\) coordinates are used. The right panel in Figure 5.2 shows the spectral locus in the rg-chromaticity diagram.\n\n\n\n\n\n\nFigure 5.2: Visualization of the CIE 1931 RGB space and its rg-chromaticity diagram. Left: the transformation from an [R, G, B] color to its [r, g, b] chromaticity is a perspective projection to the \\(r+g+b=1\\) plane. Each line that goes through the origin is an “equi-chromaticity” line, in that all the colors on that line have the same chromaticity. We use the CIE 1931 RGB color space for illustration here, but the same idea applies to other color spaces as well, e.g., the CIE 1931 XYZ space. From the interactive tutorial in Zhu (2022b). Right: visualization of the spectral locus in CIE 1931 RGB space; from Fairman, Brill, and Hemmendinger (1997, fig. 2).\n\n\n\n\n\n5.2.2 xy-Chromaticity Diagram and Its Interpretation\nOf course we can do the same if a color is defined in the XYZ space or the LMS cone space, and we omit the trivial math here. The left panel in Figure 5.3 shows the xy-chromaticity diagram. It is obtained by first converting from the XYZ space to the xyz space and then plotting only the x and y axes (z is implicit in that \\(x+y+z=1\\)). The horseshoe curve is the spectral locus. For the reference, we also show the three primary lights and the white point of the CIE 1931 RGB color space as well as the Planckian locus, which shows the chromaticities of the black-body radiation at different temperatures (Figure 4.5).\n\n\n\n\n\n\nFigure 5.3: Left: The gamut and spectral locus of the CIE 1931 RGB space visualized in the xy-chromaticity diagram; adapted from PAR (2012). The Planckian locus is shown for the reference too. A point outside the (convex) spectral locus is an imaginary color. Right: comparison of different color spaces in the xy-chromaticity diagram; from Myndex (2022). A color space’s chromaticity gamut is a triangle; a color outside the triangle cannot be physically produced in that color space.\n\n\n\nWe can make a few general observations. First, the triangle in the diagram represents the chromaticity values of all the colors that can be produced by mixing different amounts of the three colors whose chromaticities are the vertices of the triangle. That is, given three colors \\([R_1, G_1, B_1]\\), \\([R_2, G_2, B_2]\\), \\([R_3, G_3, B_3]\\) and their chromaticity coordinates \\(\\mathbf{c_1} = [\\frac{R_1}{R_1+G_1+B_1}]\\), \\(\\mathbf{c_2} = [\\frac{R_2}{R_2+G_2+B_2}]\\), and \\(\\mathbf{c_3} = [\\frac{R_3}{R_3+G_3+B_3}]\\), we can show if we mix these colors to form a color C, \\([\\alpha R_1 + \\beta R_2 + \\gamma R_3, \\alpha G_1 + \\beta G_2 + \\gamma G_3, \\alpha B_1 + \\beta B_2 + \\gamma B_3]\\) (\\(\\alpha, \\beta, \\gamma\\) are the contributions of the primary colors), C’s chromaticity is necessarily inside the triangle \\(\\bigtriangleup \\mathbf{c_1}\\mathbf{c_2}\\mathbf{c_3}\\). So the triangle \\(\\bigtriangleup \\mathbf{R}\\mathbf{G}\\mathbf{B}\\) represents the chromaticities that can be physically produced by the CIE 1931 RGB primary lights. We call that the chromaticity gamut of the color space, or sometimes simply the gamut of the color space, but we should keep in mind that the actual gamut of a color space is always a three-dimensional concept.\nSecond, we can extend from mixing three colors to mixing an arbitrary number of colors and show that the interior of the spectral locus represents the chromaticities of all the colors that humans can see, i.e., the gamut of the HVS. This is true because the shape of the spectral locus is convex, so connecting any two points (i.e., mixing two colors) on or inside the locus will never go beyond the locus. By extension, a positive linear combination of any points on or inside the locus will always stay inside the locus. A natural implication is that any point outside the spectral locus represents an imaginary color, since that point can never be constructed by a positive linear combination of points on or inside the spectral locus.\nThird, the right panel in Figure 5.3 shows the gamut of a few common color spaces. The sRGB color space is the most commonly used color space; virtually every single display supports it, and images, by default, are encoded in the sRGB format. We will have more to say about displays and image encoding later. Observe how small the sRGB gamut is: it covers about 35% of the HVS gamut. P3 is a more wider gamut that is supported in many new displays. Rec.2020 is an even wider gamut that is yet to be widely supported; it is 72% larger than the sRGB gamut and 37% larger than the P3 gamut. ProPhotoRGB contains colors that are beyond the HVS gamut, so to produce all the real colors in the ProPhotoRGB space we will need more than three primary lights. It is mostly used in Adobe Lightroom and Adobe Camera RAW software. They both deal with RAW images before they are encoded in a format that is displayable. We will talk about RAW imaging and processing later in Chapter 18.\nFinally, no display can produce all the colors that humans can see. No matter where you choose to place the primary colors in the chromaticity diagram and how many primaries you choose, the resulting gamut will never completely cover the entire HVS gamut as long as the primary colors are real colors (i.e., on or inside the spectral locus) and you have a finite number of them. This is again because the spectral locus is convex. For this reason, do not trust the colors in any xy-chromaticity diagram: the undisplayable colors are approximated by in-gamut, displayable colors. This is called gamut mapping, which we will discuss in Chapter 21.\n\n\n5.2.3 HVS Gamut\nWe can systematically sample the chromaticities in the chromaticity diagram to visualize how the HVS gamut looks like. Figure 5.4 visualizes the HVS gamut in both the XYZ space and the xy-chromaticity diagram. Comparing the two, you can see how a selected set of colors in the highlighted XYZ space map to a curve in the xy-chromaticity diagram.\n\n\n\n\n\n\nFigure 5.4: HVS gamut visualized in the XYZ space and in the xy-chromaticity diagram. We systematically sample the chromaticities in the chromaticity diagram using square pulses as the light SPDs (insets on the right). From the interactive tutorial in Zhu (2022d).\n\n\n\nThere are, of course, many ways you can sample the chromaticities to get good coverage of the HVS gamut, and Zhu (2022d) is an interactive tutorial that talks about this in detail (you can also see what the HVS gamut looks like in different color spaces). A common way seems to be to generate SPDs that are square pulses with equal peaks (see the insets on the right), which will guarantee that you do not repeatedly sample the same chromaticity point. This is what the popular Python package Colour (NumFOCUS n.d.) does, but nothing prevents you from using a different method, as explored in Zhu (2022d). Of course, the actual HVS gamut has no boundary: we can indefinitely grow the gamut by simply scaling up the light power.",
    "crumbs": [
      "Human Visual System",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Colorimetry</span>"
    ]
  },
  {
    "objectID": "hvs-colorimetry.html#sec-chpt-hvs-cori-cube",
    "href": "hvs-colorimetry.html#sec-chpt-hvs-cori-cube",
    "title": "5  Colorimetry",
    "section": "5.3 Color Cube",
    "text": "5.3 Color Cube\nThe various color spaces we have been discussing are great, but they do not seem to be the sort of color spaces we use in everyday software when specifying colors. By far the most common way in practical applications to specify colors is by using a color cube, where you can specify the primary values (usually R, G, and B) of a color, each an integer between 0 and 255. What exactly are the colors that can be represented by such a color cube? How is it related to the color gamut we have discussed, and how do we construct a color cube? These are questions explored in the interactive tutorial (Zhu 2022a), which you are invited to go through. Figure 5.5 illustrates the idea, and we will give a brief summary of the main steps.\n\n\n\n\n\n\nFigure 5.5: Pick the primary colors (which usually are termed R, G, and B, because they usually are red-ish, green-ish, and blue-ish) and the white point in the xy-chromaticity space (left panel) and then construct a color cube from them (right panel). Note how the spectral locus is now positioned in the constructed RGB space. From the interactive tutorial in Zhu (2022a), which we invite you to study, you can see that as you change the primary colors and/or the white point, the resulting color gamut and the color cube will change accordingly.\n\n\n\n\n5.3.1 Step 1: A Linear Transformation From the XYZ Space\n\nWe know that a color space is defined by its three primary colors and the white point, which you get to choose when building your own color cube. The left panel shows one such choice, which happens to be what is used by the sRGB color space.\nKnowing these four points uniquely defines the shape of a parallelepiped in the XYZ space (middle panel). The space inside the parallelepiped corresponds to actual colors that can be produced by using the primary colors.\n\nNote that at this point we know only the relative shape, but not the absolute scale, of the parallelepiped: we can uniformly scale the power of the primary colors and white point, which will not change their chromaticity values but will expand or shrink the parallelepiped. The convention is to set the Y value of white to be 1 and scale everything else accordingly, but of course the actual luminance of white (and any other color) depends on the actual device used.\n\nNow we turn the parallelepiped to a cube that is positioned between [0, 1] in all three directions (right panel). The white point in the XYZ space will be [1, 1, 1] in the color cube, signifying that white is produced from equal units of the three primary colors. This amounts to a linear transformation from the XYZ space.\n\nNote also how the spectral locus is now positioned in the RGB space: part of the locus (and by extension the HVS gamut) is now outside the RGB cube, showing that there exist real colors (i.e., inside the HVS gamut) that cannot be produced by the choice of the primary colors. This is consistent with our gamut interpretation in the chromaticity diagram (Figure 5.3).\nWhat we have done so far is to construct a linear transformation matrix, \\(T_{xyz2rgb}\\), which transforms the parallelepiped (middle panel in Figure 5.5) to a cube (right panel in Figure 5.5). This transformation matrix will change if we change any primary color or the white point of our color space (the interactive tutorial in Zhu (2022a) will allow you to do exactly that). Either way, the color cube we have built so far is luminance-linear: if we double the power of a light whose color is [R, G, B], we will get a color [2R, 2G, 2B]. This is because the XYZ space is luminance-linear, and the RGB cube we have so far is a linear transformation from the XYZ space.\n\n\n5.3.2 Step 2: Color Quantization and Gamma\nWe get a cube now, but we are not done yet. The cube is a continuous solid between [0, 0, 0] and [1, 1, 1], but the digital representation of a color is discrete and finite, so we have to quantize the solid. Assuming we have, say, 8 bits (i.e., 256 discrete levels) to represent the contribution of each primary color, the question is how to allocate the 256 levels to the [0, 1] range.\nSo far in our discussion, the contribution of a primary color is linearly correlated with the power of the primary: doubling the contribution of a primary requires doubling the power of the corresponding light. Therefore, a uniform allocation of the bits would mean uniformly quantizing the power range. This encoding strategy, however, is not ideal.\nAs we have seen in Section 3.6.1, the electrical response of a photoreceptor is not linearly proportional to the light power (even though the amount of photon absorption and pigment excitation are!); the response incrementally saturates as the light power increases. As a result, the perceptual brightness level also gradually saturates with the light power. Therefore, uniformly quantizing the power range would lead to a non-uniform quantization of the brightness range, leading to large perceptual quantization error when encoding low luminances.\nTo best use the limited bit budget, therefore, we would ideally want to uniformly quantize the brightness range, not the power range. A common method is to first model the brightness level (\\(B\\)) as a power-law function of the raw channel value (\\(v \\in [0, 1]\\)) by \\(B=v^{1/2.2}\\) and then quantize \\(B\\) uniformly. The constant factor \\(2.2\\) is called the gamma of the system. For instance, a red-channel value of 0.5 would translate to \\(\\lfloor 0.5^{1/2.2} \\times 255 \\rfloor = 186\\) in an 8-bit encoding. The relationship between \\(B\\) and \\(v\\) is called the Opto-Electronic Transfer Function (OETF). \nThe sRGB standard (Anderson et al. 1996; Stokes et al. 1996; IEC 1998) essentially uses this approach with one slight tweak to avoid numerical issues when \\(v\\) is small. In particular, the sRGB standard uses linear scaling when \\(v\\) is very small (below 0.0031308). This makes sense given our understanding in Section 3.6.3 that the receptor’s electrical response is approximately linear against the light luminance when the luminance is very low. The sRGB standard also adjusts the gamma to be 2.4 so that the overall quantization function approximates a uniform power-law function with a gamma of 2.2. As a result, the OETF used in sRGB is:\n\\[\nB =\n\\begin{cases}\n12.92 v, & v \\leq 0.0031308\\\\\n1.055 v^{1/2.4} - 0.055, & v &gt; 0.0031308\n\\end{cases}\n\\tag{5.1}\\]\nwhere \\(v \\in [0, 1]\\) is one of the three RGB channels. An 8-bit quantization is then applied to \\(B \\in [0, 1]\\) to bring each channel value to an integer between 0 and 255 assuming an 8-bit encoding.\nNote that the gamma-based OETF does not model the actual relationship between perceived brightness and light luminance, but it is a close engineering hack. The behavioral brightness perception is largely accounted for by the photoreceptor/RGC response to light intensity. As we discussed in Section 3.6.3, the relationship between the electrical response of a photoreceptor and the light intensity is usually modeled by a (generalized) Michaelis equation, which incrementally saturates and exhibits a diminishing return, the same characteristic that the power-law function also possess. \nThere are two caveats here. First, \\(v\\) is proportional to luminance \\(L\\), but is not exactly \\(L\\), so the same \\(v\\) will result in different \\(L\\)s on different displays that differ in their peak luminance. So encoding \\(B\\) as a power-law function of \\(v\\) does not mean the OETF actually models the correct relationship between \\(B\\) and \\(L\\). That is why the sRGB standard specifies the peak luminance of the display (white point) as 80 cd/m\\(^2\\). Presumably this means that at this particular luminance range (0 to 80 cd/m\\(^2\\)), the relationship between \\(B\\) and \\(L\\) roughly follows the power law. Second, light adaptation (Section 6.1) will also play a role, since the HVS responds to contrasts over the mean illuminance, rather than absolute illuminance, and the mean illuminance varies largely across viewing environments. The sRGB standard also specifies that the mean illuminance level of the viewing environment to be 64 lux. When actually viewing an sRGB image, both conditions are rarely met, so take all these with a huge grain of salt.\n\n\n5.3.3 RGB Color Spaces are Linearly Related in Luminance\nBy “RGB color space”, we mean a color space that is defined by its three primary colors (and critically also the white point), which we call R, G, and B for simplicity, but they certainly do not have to look like red-ish, green-ish, and blue-ish. Different RGB color spaces might use different gammas and quantization schemes. In the end, the discrete RGB values are usually not linearly related to luminance. We can go back to a luminance-linear space from the discrete RGB values by inverting the gamma encoding process described above. For instance, in sRGB space, 186 would translate to 0.5 in the luminance-linear sRGB space.\nOnce in a luminance-linear space, different color spaces are simply a linear transformation away from each other. The transformation matrix can be calculated based on the primary colors and the white point of the two color spaces. We will omit the math here, but to get an idea just go back to Figure 5.5. Two color spaces having different primary colors and white points will end up being two different parallelepipeds that are related by a transformation matrix. Another way to think about this is that each luminance-linear RGB color space is a linear transformation away from the XYZ space, so these RGB spaces must be linearly related too.",
    "crumbs": [
      "Human Visual System",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Colorimetry</span>"
    ]
  },
  {
    "objectID": "hvs-colorimetry.html#sec-chpt-hvs-cori-hsv",
    "href": "hvs-colorimetry.html#sec-chpt-hvs-cori-hsv",
    "title": "5  Colorimetry",
    "section": "5.4 HSB/HSL/HSV Space",
    "text": "5.4 HSB/HSL/HSV Space\nA color cube is one way to represent an RGB color space. Another common way to represent an RGB color space is to use a cylindrical-coordinate representation. There are two such representations, HSL (Hue, Saturation, and Lightness) and HSV (Hue, Saturation, and Value), which is also called HSB (B for Brightness). These are not new color spaces; their gamut is identical to that of the corresponding RGB color space. They are just different ways to organize colors in a color space; instead of using three-dimensional coordinates to represent a color as in a color cube, they use cylindrical coordinates.\n\n\n\n\n\n\nFigure 5.6: We can represent an RGB color cube (left) using cylindrical coordinates. One such representation is the HSL color space (right), where hue, saturation, and lightness have intuitive interpretations. Hue and saturation also have intuitive interpretations in the CIE 1931 xy-chromaticity diagram, which normalizes luminance so lightness information is absent. Left: from SharkD (2010b). Middle: adapted from SharkD (2010a). Right: from BenRG (2009).\n\n\n\nFigure 5.6 compares a typical color cube (left) and its HSL representation (right). We omit the transformation math here, but one can imagine how we turn the white point in a color cube to the top plane, the black point to the bottom plane, and expand everything else so that a cube surface morphs into a cylindrical surface. The three dimensions in an HLS space are hue, saturation, and lightness. Very informally, hue represents subjectively different colors (red, orange, yellow, etc.), saturation represents how much white a color has (a color with a higher saturation means it is more “pure”), and lightness represents the brightness. In this sense, hue and saturation also find their interpretations in the CIE 1931 xy-chromaticity diagram (right), where a color closer to the spectral locus has a higher saturation (and colors closer to white-ish colors are desaturated), and the spectral locus cycles through different hues. Lightness is not concerned with in the chromaticity diagram, which normalizes the color intensity.\nYou can imagine what the benefit of using an HSL/HSB color space is. It is more intuitive to pick colors in these color representations since the three dimensions have intuitive interpretations that better align with how we describe colors in our everyday language. So we can more easily reason about how a color will change if we vary a dimension. In contrast, it is sometimes hard to predict how a color will change when we, say, increase the red channel by 10. I almost exclusively use the HSL/HSB space when picking colors in graphing software.",
    "crumbs": [
      "Human Visual System",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Colorimetry</span>"
    ]
  },
  {
    "objectID": "hvs-colorimetry.html#sec-chpt-hvs-cori-diff",
    "href": "hvs-colorimetry.html#sec-chpt-hvs-cori-diff",
    "title": "5  Colorimetry",
    "section": "5.5 Color Discrimination and Color Difference",
    "text": "5.5 Color Discrimination and Color Difference\nIn many practical applications, we need to calculate color differences. For instance, an image synthesis algorithm might want to be minimize the color difference in the synthesized image and some form of “ground truth”; a display’s color reproduction might not be 100% accurate, so we want to quantitatively compare the quality of different displays by measuring the color difference (compared to the colors to be reproduced) each introduces.\nFortunately, once we put colors into a three-dimensional coordinate system, calculating color differences becomes natural: the Enclidan distance between two colors gives a measure of the difference between the two colors. However, for the Euclidean distance to be a good measure, we must be sure that the distance is proportional to the perceptual color difference. How do we quantify the perceptual color difference?\n\n5.5.1 Color Discrimination\nPractically there are not many cases where we need to quantify large color differences. What is more important is to quantify small color differences. For a given reference color, we can use a threshold-detection psychophysical paradigm such as the one described in Krauskopf and Karl (1992) to estimate the set of colors that can just barely be discriminated from the reference color. These experiments are called color discrimination tests.\nColor discrimination experiments typically use a n-alternative forced choice (nAFC) paradigm (Krauskopf and Karl 1992; Hansen, Pracejus, and Gegenfurtner 2009; Duinkharjav et al. 2022; Danilova and Mollon 2025; Hong et al. 2025). Figure 5.7 (a) shows a 4AFC variant from Duinkharjav et al. (2022). The visual field consists of an adapting background and four color patches. The background controls the light and chromatic adaptation state of the participant, which is shown to have significant impact on the color discrimination results (Krauskopf and Karl 1992). Among the four color patches, three have the same reference color whose discrimination contour we want to estimate. The other randomly placed color patch, the test patch, has a different color. A participant is instructed to identify which one of the 4 color disks appears different. The participant fixes their gaze on a crosshair at the center of the screen for the duration that the stimuli are shown; this makes sure that the four color patches are all placed at a given eccentricity.\n\n\n\n\n\n\nFigure 5.7: (a): A 4AFC color discrimination trial; adapted from Duinkharjav et al. (2022, fig. 2a). (b): iso-luminance discrimination contours plotted in the DKL space (Derrington, Krauskopf, and Lennie 1984), where all the colors have the same luminance (L+M response); from Krauskopf and Karl (1992, fig. 14). (c): MacAdam ellipses (measured at 2\\(^{\\circ}\\) eccentricity) plotted in the xy-chromaticity diagram (the ellipse sizes are magnified 10 times to be more visible); from Anonymous (2009). (d): iso-luminance discrimination contours in the DKL space similar to (b), but now the adapting field is the reference color itself; from Krauskopf and Karl (1992, fig. 8). (e): contours corresponding to a \\(\\Delta E_{00}\\) = 1.0 in the \\(a^*\\)-\\(b^*\\) plane in the CIELAB space; from Sharma (2003, fig. 1.18). (f): discrimination contours under two different eccentricities (10\\(^{\\circ}\\) and 25\\(^{\\circ}\\)); from Duinkharjav et al. (2022, fig. 4a).\n\n\n\nEach trial uses a 1-up-2-down staircase procedure (Cornsweet 1962; Leek 2001; Treutwein 1995) to adjust the color of the test patch: the test color is moved closer to the reference color (a harder trial) if the participant identifies the test patch correctly twice in a row, and moved farther away from the reference color (an easier trial) upon an incorrect response.\nFigure 5.7 (b) shows a set of discrimination contours obtained by Krauskopf and Karl (1992) plotted in the DKL space (Derrington, Krauskopf, and Lennie 1984), which we have discussed in Section 4.4.3. The individual discrimination thresholds of a reference color are fit with an ellipse to approximate the discrimination contour, where the reference color is placed at the center of the ellipse. In their experiments, the reference colors were all iso-luminant (i.e., identical L+M responses) and the test colors were all forced to be on the same iso-luminance plane. Of course, the actual discrimination contour would be 3D ellipsoid, and obtaining that data would be take a huge amount work: we would have to sample a 3D space for the reference colors and, for each reference color, sample another 3D space to obtain its discrimination thresholds. The results in Figure 5.7 (b) essentially simplifies the 6D sampling to 4D.\nWe can see, from Figure 5.7 (b), that the discrimination contours are quite regularly placed in the DKL space. A striking feature is that the thresholds on one dimension do not change as the reference color changes along the other direction. For instance, the threshold along the S-(L+M) axis, i.e., the Yellow-Blue dimension, is roughly constant for the three reference color that differ only along the L-M axis, i.e., the Red-Green dimension. This seems to suggest that color discrimination might be independently mediated by the two opponent processes.\nThe first set of color discrimination data was collected by David MacAdam in his seminar work MacAdam (1942) and MacAdam (1943)1. MacAdam did not use the 4AFC strategy above, but indirectly estimated the thresholds using variations in color matching experiments. A modern rendition of his results is shown in Figure 5.7 (c) in the CIE 1931 xy-chromaticity space; the ellipse sizes are magnified ten times to be visible. A practical interpretation of a dicrimination contour is that with an ellipse all the colors are non-discriminable with respect to the center, reference color.\n\n\n5.5.2 Adaptation and Eccentricity Dependence\nDiscrimination contours depend on both the adaptation state and eccentricity. Figure 5.7 (d), also from Krauskopf and Karl (1992) shows, the discrimination contours when the background, adapting light is the reference color itself and the participant is fully adapted to that color. We will study chromatic adaptation later in Section 6.3, but what this practically means is that, in this experiment, the background color is perceived as achromatic, even though it might not normally be considered so. We can see that the ellipses are wildy different from those in Figure 5.7 (b). In particular, it seems like the threshold along the L-M dimension does not change at all with the L-M response of the reference color.\nMacAdam’s original data were collected at 2\\(^{\\circ}\\) eccentricity. Given that the visual acuity reduces as the eccentricity increases, it is only natural that the discrimination contours expand in size as the eccentricity. Duinkharjav et al. (2022) measures the ellipses under different eccentricities. Figure 5.7 (e) compares the results between 10\\(^{\\circ}\\) and 25\\(^{\\circ}\\). Not surprisingly, the ellipses are larger in the latter. The qualitatively different contour shapes between Duinkharjav et al. (2022) and Krauskopf and Karl (1992) is perhaps due to the differences in the measurement methodology, which warrants further investigations.\n\n\n5.5.3 Color Difference and Perceptually Uniform Color Space\nThe difference between the reference color and a color on its discrimination contour is called the Just Noticeable Difference (JND). A color space is said to be “perceptually uniform” if the JND measure is a constant anywhere in the color space along any direction. In such a color space, the Euclidean distance would be a measure of color difference.\nUnfortunately, no perceptually uniform color space has even been identified. Consider the results of Krauskopf and Karl (1992) or MacAdam, the discrimination contours are ellipses (both in the DKL space and in the xy space), not circles, so the JND for a reference color varies angularly. Worse, the discrimination contour changes its shape as the reference color change, suggesting that the JND is also spatially varying.\nQuite a few attempts have been made to transform the XYZ space into a more perceptually uniform space. Among them, the two common ones are the CIE 1976 \\(Lu^*v^*\\) (CIELUV) space and the (more widely used) CIE 1976 \\(La^*b^*\\) (CIELAB) space, both of which are non-linear transformations from the XYZ space. The so-called CIE Delta E 1976 color difference metric (\\(\\Delta E_{ab}^*\\) ) is defined as the Euclidean distance in the CIELAB space. If CIELAB is truly perceptually uniform (as far as color discrimination is concerned), \\(\\Delta E_{ab}^*\\) being 1.0 would mean a JND. However, this is not true (Sharma 2003, fig. 1.18).\nCIE has since recommended a new, much more involved, and non-Euclidean measure in the CIELAB space, called the Delta E 2000 metric (\\(\\Delta E_{00}\\)), to achieve better perceptual uniformity (Sharma, Wu, and Dalal 2005). Figure 5.7 (f) shows iso-discrimination contours corresponding to \\(\\Delta E_{00}\\) = 1.0 in an \\(a^*\\)-\\(b^*\\) (iso-luminance) plane in the CIELAB space. If the \\(\\Delta E_{00}\\) is to be considered a perceptually uniform color difference metric, the CIELAB space itself must not be perceptually uniform given the varying contour shapes throughout the space. What if we construct a new space by transforming the CIELAB space using the \\(\\Delta E_{00}\\) metric — would that space be perceptually uniform? The answer is unlikely: the discrimination contours at the low \\(b^*\\) end become weirdly non-convex, which is not a reflection of the human discrimination data but, rather, of the limitation of the \\(\\Delta E_{00}\\) metric itself. This mean there is no simple, true color difference measure in that space either — if we insist on defining a perceptual uniform space to be one where a simple Euclidean distance is proportional to perceptual difference.\n\n\n5.5.4 Science vs. Engineering\nI always feel that color discrimination is a topic that exemplifies the similarities and differences between engineering and science.\nFrom an engineering perspective, we would like to have a practical tool that allows us to quantify perceptual color differences, which have huge implications on capturing, storing, computing, and displaying colors. We want to make sure our workflow preserves the color fidelity as much as possible, and a quantitative metric is, just like any other engineering problem, a pre-requisite. Since the linear LMS/XYZ spaces and Euclidean distances are insufficient, we turn to non-linear spaces like CIELAB and non-Euclidean measures like \\(\\Delta E_{00}\\). The goal is engineering convenience. It is fair to say that while they are not perfect, they have significantly improved color workflows in practice.\nVision scientists approach this problem with a different goal. The retina has access to only the cone reponses but the cone space itself is not perceptually uniform, so a natural question to ask is: how does color discrimination arise from cone reponses? It is of course of practical value to figure out the mechanisms that mediate our ability to discriminate between two colors; at the very least, we can design better color difference metrics for engineering applications. But fundamentally, understanding color discrimination might provide new insights of human color vision as a whole, and that is the scientific value of studying this problem.\n\n\n\n\nAcdx. 2009. “CIE 1931 XYZ Color Matching Functions; CC BY-SA 4.0.” https://commons.wikimedia.org/wiki/File:CIE_1931_XYZ_Color_Matching_Functions.svg.\n\n\nAnderson, Matthew, Ricardo Motta, Srinivasan Chandrasekar, and Michael Stokes. 1996. “Proposal for a Standard Default Color Space for the Internet—Srgb.” In Color and Imaging Conference, 4:238–45. Society of Imaging Science; Technology.\n\n\nAnonymous. 2009. “MacAdam Ellipses in the CIE1931 xy chromaticity diagram; CC BY-SA 3.0 license.” https://commons.wikimedia.org/wiki/File:CIExy1931_MacAdam.png.\n\n\nBenRG. 2009. “CIE1931 xy chromaticity plot; released into the public domain by the copyright holder.” https://commons.wikimedia.org/wiki/File:CIE1931xy_blank.svg.\n\n\nBrill, Michael H. 1998. “Erratum: How the CIE 1931 Color-Matching Functions Were Derived from Wright-Guild Data.” Color Research & Application 23 (4): 259–59.\n\n\nCornsweet, Tom N. 1962. “The Staircase-Method in Psychophysics.” The American Journal of Psychology 75 (3): 485–91.\n\n\nDanilova, MV, and JD Mollon. 2025. “Effect of Stimulus Size on Chromatic Discrimination.” Journal of the Optical Society of America A 42 (5): B167–77.\n\n\nDerrington, Andrew M, John Krauskopf, and Peter Lennie. 1984. “Chromatic Mechanisms in Lateral Geniculate Nucleus of Macaque.” The Journal of Physiology 357 (1): 241–65.\n\n\nDuinkharjav, Budmonde, Kenneth Chen, Abhishek Tyagi, Jiayi He, Yuhao Zhu, and Qi Sun. 2022. “Color-Perception-Guided Display Power Reduction for Virtual Reality.” ACM Transactions on Graphics (TOG) 41 (6): 1–16.\n\n\nFairchild, Mark D. 2013. Color Appearance Models. 3rd ed. John Wiley & Sons.\n\n\nFairman, Hugh S, Michael H Brill, and Henry Hemmendinger. 1997. “How the CIE 1931 Color-Matching Functions Were Derived from Wright-Guild Data.” Color Research & Application 22 (1): 11–23.\n\n\nHansen, Thorsten, Lars Pracejus, and Karl R Gegenfurtner. 2009. “Color Perception in the Intermediate Periphery of the Visual Field.” Journal of Vision 9 (4): 26–26.\n\n\nHong, Fangfang, Ruby Bouhassira, Jason Chow, Craig Sanders, Michael Shvartsman, Phillip Guan, Alex H Williams, and David H Brainard. 2025. “Comprehensive Characterization of Human Color Discrimination Thresholds.” bioRxiv, 2025–07.\n\n\nIEC. 1998. “IEC/4WD 61966-2-1: Colour Measurement and Management in Multimedia Systems and Equipment - Part 2-1: Default RGB Colour Space - sRGB.” https://web.archive.org/web/20141225172302/http://www2.units.it/ipl/students_area/imm2/files/Colore1/sRGB.pdf.\n\n\nKrauskopf, John, and Gegenfurtner Karl. 1992. “Color Discrimination and Adaptation.” Vision Research 32 (11): 2165–75.\n\n\nLeek, Marjorie R. 2001. “Adaptive Procedures in Psychophysical Research.” Perception & Psychophysics 63 (8): 1279–92.\n\n\nMacAdam, David L. 1942. “Visual Sensitivities to Color Differences in Daylight.” Josa 32 (5): 247–74.\n\n\n———. 1943. “Specification of Small Chromaticity Differences.” Josa 33 (1): 18–26.\n\n\nMarco Polo. 2007. “CIE1931 RGB CMF; released into the public domain by the copyright holder.” https://commons.wikimedia.org/wiki/File:CIE1931_RGBCMF.svg.\n\n\nMyndex. 2022. “A comparison of RGB gamuts of sRGB, P3, Rec2020, etc. using the CIE1931 chromaticity diagram; CC BY-SA 4.0 license.” https://commons.wikimedia.org/wiki/File:CIE1931xy_gamut_comparison_of_sRGB_P3_Rec2020.svg.\n\n\nNumFOCUS. n.d. “Colour: open-source Python package for colour science.” https://github.com/colour-science/colour.\n\n\nPAR. 2012. “Plankian locus; released into the public domain by the copyright holder.” https://commons.wikimedia.org/wiki/File:PlanckianLocus.png.\n\n\nService, Phil. 2016. “The Wright – Guild Experiments and the Development of the CIE 1931 RGB and XYZ Color Spaces.”\n\n\nSharkD. 2010a. “HSL color solid cylinder; CC BY-SA 3.0 license.” https://commons.wikimedia.org/wiki/File:HSL_color_solid_cylinder_saturation_gray.png.\n\n\n———. 2010b. “RGB Color Cube; CC BY-SA 3.0 license.” https://commons.wikimedia.org/wiki/File:RGB_Cube_Show_lowgamma_cutout_a.png.\n\n\nSharma, Gaurav. 2003. “Color Fundamentals for Digital Imaging.” In Digital Color Imaging Handbook, edited by Gaurav Sharma, 14–127. CRC Press.\n\n\nSharma, Gaurav, Wencheng Wu, and Edul N Dalal. 2005. “The CIEDE2000 Color-Difference Formula: Implementation Notes, Supplementary Test Data, and Mathematical Observations.” Color Research & Application 30 (1): 21–30.\n\n\nStokes, Anderson, Chandrasekar, and Motta. 1996. “A Standard Default Color Space for the Internet - sRGB.” https://www.w3.org/Graphics/Color/sRGB.\n\n\nTreutwein, Bernhard. 1995. “Adaptive Psychophysical Procedures.” Vision Research 35 (17): 2503–22.\n\n\nZhu, Yuhao. 2022a. “Interative Tutorial: Building a Color Cube.” https://horizon-lab.org/colorvis/colorcube.html.\n\n\n———. 2022b. “Interative Tutorial: Chromaticity, Gamut, and the Scary World of Imaginary Colors.” https://horizon-lab.org/colorvis/chromaticity.html.\n\n\n———. 2022c. “Interative Tutorial: CIE 1931 XYZ Color Space.” https://horizon-lab.org/colorvis/xyz.html.\n\n\n———. 2022d. “Interative Tutorial: Visualizing Human Visual Gamut.” https://horizon-lab.org/colorvis/gamutvis.html.",
    "crumbs": [
      "Human Visual System",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Colorimetry</span>"
    ]
  },
  {
    "objectID": "hvs-colorimetry.html#footnotes",
    "href": "hvs-colorimetry.html#footnotes",
    "title": "5  Colorimetry",
    "section": "",
    "text": "MacAdam did the work while working for Eastman Kodak at Rochester and he later was an adjunct professor at the Institute of Optics, University of Rochester.↩︎",
    "crumbs": [
      "Human Visual System",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Colorimetry</span>"
    ]
  },
  {
    "objectID": "hvs-adaptation.html",
    "href": "hvs-adaptation.html",
    "title": "6  Visual Adaptations and Constancy",
    "section": "",
    "text": "6.1 Light Adaptation\nHumans adapt to all sorts of sensory input, including heat, pain, smell. In all cases we care about two things: 1) the behavior after full adaptation and 2) the time course of adaptation. Before this chapter is fully developed, check out our lecture slides on adaptation.\nlight adaptation",
    "crumbs": [
      "Human Visual System",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Visual Adaptations and Constancy</span>"
    ]
  },
  {
    "objectID": "hvs-adaptation.html#sec-chpt-hvs-adaptations-dark",
    "href": "hvs-adaptation.html#sec-chpt-hvs-adaptations-dark",
    "title": "6  Visual Adaptations and Constancy",
    "section": "6.2 Dark Adaptation",
    "text": "6.2 Dark Adaptation\ndark adaptation",
    "crumbs": [
      "Human Visual System",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Visual Adaptations and Constancy</span>"
    ]
  },
  {
    "objectID": "hvs-adaptation.html#sec-chpt-hvs-adaptations-chroma",
    "href": "hvs-adaptation.html#sec-chpt-hvs-adaptations-chroma",
    "title": "6  Visual Adaptations and Constancy",
    "section": "6.3 Chromatic Adaptation",
    "text": "6.3 Chromatic Adaptation\nchromatic adaptation, which is concerned with sensitivity scaling and is a direct result of light adaptation.",
    "crumbs": [
      "Human Visual System",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Visual Adaptations and Constancy</span>"
    ]
  },
  {
    "objectID": "hvs-adaptation.html#sec-chpt-hvs-adaptations-cc",
    "href": "hvs-adaptation.html#sec-chpt-hvs-adaptations-cc",
    "title": "6  Visual Adaptations and Constancy",
    "section": "6.4 Color Constancy",
    "text": "6.4 Color Constancy\ncolor constancy goes beyond sensitivity scaling and involves higher-order visual functions, i.e., surface reflectance estimation.",
    "crumbs": [
      "Human Visual System",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Visual Adaptations and Constancy</span>"
    ]
  },
  {
    "objectID": "rendering.html",
    "href": "rendering.html",
    "title": "Rendering",
    "section": "",
    "text": "So far in the book, our focus has been on how our visual system encodes light spectrum coming from the scene into perception. We have, however, not cared much about how an object in the scene could produce light in the first place. An object could, of course, emit light itself. In the real world, however, the vast majority of objects have colors not because they emit light but because they interact with light that impinges upon them. The light-matter interaction modifies the energy spectrum of the incident light, and the modified light is scattered back to our eyes, giving rise to (color) vision.\nA great deal of computer graphics is concerned with rendering the color of objects, and the name of the game is to model the light-matter interactions in a physically accurate manner so that the colors in the generated imagery look real. This part of the book focuses on the physical principles that govern light-matter interactions insofar as they are used in rendering photorealistic color images. We will not cover implementation-specific topics such as how these principles are supported in modern graphics programming models (e.g., OpenGL, Vulkan, and OptiX); nor will we cover how these programming models are supported on modern GPU hardware correctly and efficiently.\nAny coverage of physics in rendering is necessarily an approximation — based on phenomenological models that abstract away unimportant details of the underlying physics while maintaining what is relevant for image synthesis. Deep learning and AI techniques push this kind of approximation to the extreme. With these techniques, rendering is re-branded as novel view synthesis, the prime example of which is the increasingly popular class of (neural) radiance-field rendering methods such as NeRF and 3DGS.\nThese methods are fundamentally image-based rendering: they sample, reconstruct, and re-sample the light field — using modern learning methods such as (stochastic) gradient descent. Interestingly, even though they do not exactly model the physics governing the light-matter interactions, their learning model is parameterized with physics-inspired formulations. By understanding the governing physics, we can better interpret these learning-based methods, understand their limits, and reason about potential opportunities for improvement.",
    "crumbs": [
      "Rendering"
    ]
  },
  {
    "objectID": "rendering-overview.html",
    "href": "rendering-overview.html",
    "title": "7  Overview",
    "section": "",
    "text": "7.1 Light-Matter Interactions\nThis chapter first lays the land for physically modeling light-matter interactions in rendering, discussing the different levels at which the modeling can occur (Section 7.1) and building intuitions using a toy example (Section 7.2). We will conclude the chapter by introducing an extremely high-level model that abstracts away almost all the underlying physics and models an object as its apparent reflection and transmission spectra (Section 7.3). This simple modeling is very commonly used in practice and serves as a curious teaser for the remaining rendering chapters: how and when can this simple model be a good approximation of the sophisticated light-matter interactions?\nBy and large, we will treat light as a collection of photons, each traveling in a particular direction and carrying a specific amount of energy. When a beam of photons hits a material surface, some of the photons will be scattered directly back to your eyes, and others will penetrate into the material. These surface phenomena are governed by surface scattering. We use the word “scattering” here to generally refer to lights coming back from the surface. Depending on the material, some of the scattered photons are along the perfect mirror-reflection directions, and others might be more diffuse. You might sometimes see the word “reflection” used. Reflection is sometimes used in the same way as scattering, which will be our use, but other times is reserved for the perfect, mirror-like reflection. Usually what the word means is self-evident given the context, but we will err on the side of verbosity when we want to mean a specific form of reflection.\nPhotons that penetrate the surface will further interact with particles in the material, which absorb, scatter, or might even emit photons. This is called subsurface scattering (SSS) in computer graphics. Even though we use the term “scattering”, you should know that the actual SSS processes involve not only photon scattering but also absorption and emission. We will, however, generally ignore emission in our discussion unless otherwise noted, but just note that emission does happen and is correlated with absorption, since emission is the result of absorbed photons having (e.g., chemical) reactions with the particles.\nIt turns out that the principles that govern SSS are exactly the same as those that govern the interactions between photons and particles in the so-called participating media, such as clouds, fogs, and smokes. In computer graphics, light-matter interactions in participating media are called volume scattering1. SSS is, instead, more commonly used to refer to light-matter interactions in solids (which have a clear definition of “surface”) where subsurface-scattered photons contribute to their observed colors, such as skin, jade, and wax.\nTo model SSS/volume scattering, we no longer consider the light-matter interaction as photons bouncing off of the surface; instead, we break a material down into small particles and model how photons interact with individual particles. Very importantly, the difference in the modeling methodology does not imply that there somehow is a fundamental difference between surface scattering and volume scattering.\nUltimately, both are caused by the light, an oscillating electromagnetic field, exciting discrete electric charges. The differences lie in how the charges are arranged in space and in relation to one another. In the classical regime, the laws that govern how photons interact with the charges are described by the electromagnetic theory. In fact, using the electromagnetic theory, we can show that surface reflection/refraction is nothing more than the coherent scattering of incident light waves by the surface particles. In the quantum regime, light-matter interactions are governed by the quantum electrodynamics (QED)2.\nSince there is no fundamental differences in the underlying physics, the only meaningful distinction is one between different phenomenological approximations, or “models”, of the same underlying physics. We totally could invoke the electromagnetic theories or QED, and if we did, we would have one single unified model that explains both surface scattering (reflection and refraction) and volume scattering. Doing so, however, is not only unnecessary (because many, not all, real-world material color phenomena could be modeled without them) and too computationally expensive, but also, perhaps more importantly, blinds us from the relatively simple intuitions in each scenario. Instead, each phenomenological model is based on a set of high-level guiding principles, which are approximations of the underlying physical process but are sufficient to quantitatively describe light-matter interactions in each scenario.\nJohnsen (2012) is a great reference, which has some equations but generally focuses on building intuitions and mostly uses the electromagnetic language rather than the quantum language. If you want to get to the nuts and bolts of the mathematical modeling, Bohren and Clothiaux (2006) is a phenomenal text whose models are also built in the electromagnetic land. Feynman (1985) has an accessible and breathtaking introduction to QED that I highly recommend. Dorsey, Rushmeier, and Sillion (2010) is a classic text on material appearance modeling in graphics that covers a range of topics, including modeling, measurements, and various implementation issues in practice. Johnston-Feller (2001) is specifically concerned with paintings; it has many interesting discussions of pigments and pigment mixtures and has many real-world data and insights that are rarely found elsewhere.",
    "crumbs": [
      "Rendering",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "rendering-overview.html#sec-chpt-mat-vs-ov",
    "href": "rendering-overview.html#sec-chpt-mat-vs-ov",
    "title": "7  Overview",
    "section": "7.2 Building Intuition",
    "text": "7.2 Building Intuition\nWe will use Figure 7.1 as a running example to discuss the life of photons when interacting with materials/media and develop some key intuitions along the way.\n\n\n\n\n\n\nFigure 7.1: At the Air-Material1 interface, photons are either reflected directly back (perfectly specular reflection here) or penetrate into the material through refraction. The refracted photons interact with the material particles through the volume scattering processes, where some photons are absorbed and others penetrate into Material2. For someone observing from the outside, a portion of the photons would eventually leave the material composite altogether and re-enter the air. Some of these leaving photons are called the back-scattered photons that contribute to the apparent surface reflectance; others transmit through the materials and contribute to the apparent transmittance of the material composite.\n\n\n\nPhotons come from above the surface of the first material. At the Air-Material1 interface, photons are either reflected directly back or penetrate into the material through refraction. The exact form of reflection is governed by surface scattering. Here we have assumed a perfectly specular reflection and refraction that obey the Snell’s law (Section 11.1), where, for instance, the direction of reflection is mirror-symmetric to the incident direction. As we will see in Chapter 11, general material surfaces are more complicated than this.\nOnce inside the material, a photon roams about until it meets a particle. The interactions between photons and particles are governed by the subsurface scattering (SSS) or volume scattering processes. Subsurface scattering is so termed to distinguish itself from surface scattering, but what is beneath the surface is nothing more than a volume of particles. In fact, what is above the surface is also a volume of particles. Looking at Figure 7.1, the air, Material1, and Material2 can all be thought of as participating media. We usually model the air as a vacuum so photons traverse in straight lines undisturbedly, but if we were to be exact, we would want to model the particles in the air, which becomes a participating medium. So “above-surface scattering” is as different from surface scattering as is subsurface scattering.\nWhen a photon meets a particle inside the material, the particle might absorb the photon or scatter it away. If absorbed, the photon is “dead” and can be removed from the discussion. If scattered, the photon might appear to change its direction and continue to travel on a straight line until it meets another particle, so in principle a photon can be scattered multiple times.\nA photon has a certain probability of being absorbed when it meets a particle, so the longer it travels, the more likely it will be absorbed. If a photon survives the absorption of all the particles in the media, it would re-emerge from Material1 back to the air or they might emerge to the air from the bottom of Material2. Let’s examine cases where a photon escapes the media.\n\nAfter multiple scattering events, some of the initial photons that enter Material1 from the air will reach the Material1-air boundary again, but this time from the material side. At that point, the photons necessarily go through another round of reflection-refraction governed by the surface scattering processes. The refracted photons will re-emerge from Material1.\nThis is called back-scattering, because these photons are scattered back to where they come. As a consequence, when we observe the material from the same side of the illumination, the lights that enter our eye come from two sources: the initial surface scattering and the back-scattering.\nSome photons might leave Material1 from the other side and enter Material2, in which photons go through the same volume scattering processes, where some are absorbed, some can be turned back to Material1, and others, critically, can hit the Air-Material2 interface. Just like what happens at the Air-Material1 interface, some of the photons will eventually emerge from Material2. When you observe the material from the opposite side of the illumination, it is these transmitted photons that dictate the color of the material.\n\nSometimes people will also say, “subsurface scattering is caused by photons exiting at a point different from the incident point”. It points to the fact that a photon can re-emerge anywhere from the material after SSS, whereas surface scattering is modeled to be taking place only at the incident point. But this is just a useful macroscopic abstraction or, rather, modeling strategy. In reality, surface scattering results from, as discussed above, the superposition of scattering of the incident wave by all particles in the surface region, which is intrinsically non-local since wave interactions extend beyond a single point.\n\n7.2.1 Material Types\nWe often hear materials being described as opaque, translucent, and transparent. We can now more scientifically approach these terms given the intuitions we have built so far.\n\nTransparent Materials\nTransparent materials either scatter light predominantly in forward directions or they scatter very little light (other than surface scattering). As a result, most photons traveling through the material are either absorbed or go through without changing much of their the directions. So if you hold a transparent material against a light source, you can clearly see through the material and see the light on the other side. This does not mean transparent materials always have the same color as the light source — absorption could be wavelength-selective. An example is aqueous/dye solutions where dye molecules are very small (\\(\\text{nm}\\) range) and, thus, scatter little light so they look transparent, but depending on the absorption spectrum (which depends on how the dye molecules interact with molecules in the solvent), most dye solutions are not colorless.\n\n\nOpaque Materials\nIn many materials, photons arriving at the material surface are either reflected right away at the surface or, for those that do penetrate into the materials, are all absorbed by the subsurface particles. Examples include conductors like metals or sufficiently thick dielectrics like bricks.\nThese materials are opaque in two senses. First, their transmittance is practically 0: because of strong absorption, no photon re-emerges at the other side of the material. If you hold, say, a brick (dielectric) against a light bulb, the brick would completely block the light. Second, their surface color is independent of the substrate or the material beneath them, so they completely hide the color of the substrate3. Using the example in Figure 7.1, if Material1 is sufficiently thick, few photons would reach Material2 and for those that do reach, they have little change of re-emerging from Material1. Painters know that if they want to cover a layer in their painting, they will need to apply a very thick layer of paint on top.\n\n\nTranslucent Materials\nTranslucent materials such as jade, wax, and skin are neither opaque nor transparent. If you hold a wax against a light bulb, the wax will not completely block the light, so you will see some light, but you will not be able to see clearly the other side through the wax, since photons from the light bulb are very much volume-scattered after passing through the wax. Clearly, modeling SSS is critical for accurately estimating the color of translucent materials. In fact, in graphics literature we sometimes see things like “modeling translucent material must consider subsurface scattering”. \nIt is not true that SSS is important only in modeling translucency. Modeling SSS can be important for opaque materials. Consider the wax case: what if we make the wax very thick? The thick wax will eventually become opaque in that it will completely hide the material behind it. But that does not mean volume scattering does not matter here; the back-scattered photons do contribute to the apparent color of a thick wax.\n\n\nOil Painting Example\nTo put things together, consider a painting. One way paintings are characterized is by how they were painted, and we might see things like “oil on canvas”. Oil means the paint is oil paint, where paint pigments are dispersed into (usually linseed) oil, which is usually called the binder or the vehicle. Canvas is the substrate, which is nothing more than another material that is right beneath the painting.\nThe oil itself is somewhat transparent, especially when you just apply a thin layer on the canvas. But with the paint pigments, the entire oil paint becomes a translucent material. When photons leave the oil paints, they immediately interact with the canvas. If the paint layer is thick enough, virtually no photon can ever reach the canvas. But if the paint is relatively thin, the property of the substrate will contribute to the overall color of the paint. For instance, if the canvas is white-ish, a good percentage of the photons will be reflected back. The same paint would look much darker if the canvas is black, which absorbs a lot of photons.\n\n\n\n7.2.2 Equilibrium\nWe can view the light-material interaction as a dynamical system under an equilibrium. To appreciate this, consider again Figure 7.1. Some photons entering Material1 are back-scattered and hit the Air-Material1 interface and some of those photons will re-enter Material1 through internal reflection. Those photons will then go through multiple scattering, and as a result some will be back-scattered again and hit the Air-Material1 interface. The cycle goes on. The secondary back-scattering is weaker in power than the first back-scattering, and the third-order back-scattering is even weaker, and so on. So eventually you can imagine that the total number of photons back-scattered at the surface will reach a constant.\nIn fact, this sort of dynamics takes place everywhere inside the material along every direction. If you pick a point \\(p\\) in the material (or at the surface) and a direction \\(\\omega\\) starting at the point, the radiance at (\\(p, \\omega\\)) is a constant under equilibrium. In other words, the spatial radiance distribution is not changing over time.\nThe equilibrium is reached almost instantaneously, since light propagates incredibly fast. So the equilibrium discussion is probably of no practical impact in modeling or actual measurement, but it is still important to keep this in mind. Our discussion will assume equilibrium. For instance, later when we model volume scattering, we will set up the differential equations that describe the energy transfer assuming equilibrium.",
    "crumbs": [
      "Rendering",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "rendering-overview.html#sec-chpt-mat-basics-model",
    "href": "rendering-overview.html#sec-chpt-mat-basics-model",
    "title": "7  Overview",
    "section": "7.3 Observed Reflection and Transmission",
    "text": "7.3 Observed Reflection and Transmission\nRegardless of the details of surface scattering and volume scattering, a material appears to have some color because some photons leaving the material enter our eye. If we observe the material from the same side of the light source, it is the lights reflected from the material that matter. If we observe the material from the other side of the light source, it is the light transmitted through the material that matter. At the highest level of abstraction, we can model the material color in the real world by modeling the observed reflection and transmission apparent to an outside observer: how much of the incident power is reflected/transmitted back to the eye?\nWe can quantify the observed reflection and transmission using the spectral reflectance function \\(r(\\lambda)\\) and the spectral transmittance function \\(t(\\lambda)\\), respectively. These two functions spare us the details of how lights interact with a material but describe, at each wavelength \\(\\lambda\\), the percentage of optical power that is reflected back to the eye or transmitted through the material and enters the eye, respectively.\n\n\n\n\n\n\nFigure 7.2: (a): apparent spectral reflectance modifies the illumination spectrum and dictates the observed color; adapted from The Astronomer by Johannes Vermeer (1668). (b): a photo of Acadia Redfish I took in the Ripley’s Aquarium of Canada. The fish ordinarily looks red-ish under a white-ish light, but appears colorless in the aquarium, which simulates the lighting environment in the deep sea where lights are predominately blue/violet. The spectral data are not accurate and for the illustration purpose only.\n\n\n\nFigure 7.2 (a) illustrates this modeling at work using the famous The Astronomer by Johannes Vermeer. We will proceed with our discussion using reflectance, but the case idea can be easily extended to transmittance. Vermeer paints an astronomer looking at a globe. Given the illumination coming from the window \\(\\Phi(\\lambda)\\) and the spectral reflectance of the point on the globe under gaze \\(r(\\lambda)\\), the light reflected toward the eye is then \\(\\Phi(\\lambda)r(\\lambda)\\). We can then calculate the color of these lights using the cone fundamentals or some set of CMFs, the same way as if the lights were directly emitted from the globe.\nAs another example, Figure 7.2 (b) is a photo of Acadia Redfish I took when visiting the Ripley’s Aquarium of Canada. The fish ordinarily looks red-ish under a white-ish light, which suggests that its spectral reflectance \\(r(\\lambda)\\) peaks at longer wavelengths: it scatters more long-wavelength, i.e., red-ish, lights than short-wavelength lights. But the fish appears colorless in the aquarium, which simulates the lighting environment in the deep sea where lights \\(\\Phi(\\lambda)\\) are predominately blue/violet4. As a result, the scattered lights have a rather uniform spectral power distribution, resulting in a gray-ish appearance.\nFigure 7.2 makes an important simplification: the reflectance of a point \\(p\\) on the material is simplified to only a single spectrum. In reality, the reflectance of a point \\(p\\) depends on both \\(\\oi\\), the direction of the light incident on \\(p\\), and \\(\\os\\), the outgoing direction (leaving \\(p\\)) through which one observes the material. In certain materials where SSS contributes to the material appearance (e.g., translucent materials like jade), the reflectance can also depend on light incident on other points of the material surface. So when we use a single reflectance spectrum to model material colors, what we have implicitly assumed is that the reflectance spectrum has been calculated in such a way that when you multiply it with the incident illumination, you get the scattered light power that is actually observed.\nHow such a reflectance spectrum can be obtained in measurement (to the extent that it is a useful high-level abstraction) will be discussed in Section 11.3. The reflectance is a “quick-and-dirty” abstraction that we often use to give a rough estimation/explanation of a material’s color, but it is so high-level that it hides lots of the low-level details: what exactly are the light-matter interactions that cause the surface and subsurface scattering behaviors that eventually give rise to the apparent reflectance and transmittance spectra? The remaining chapters in this part essentially answer this question.\n\n\n\n\nBohren, Craig F, and Eugene E Clothiaux. 2006. Fundamentals of Atmospheric Radiation: An Introduction with 400 Problems. John Wiley & Sons.\n\n\nDorsey, Julie, Holly Rushmeier, and François Sillion. 2010. Digital Modeling of Material Appearance. Elsevier.\n\n\nFeynman, R. 1985. QED: The Strange Theory of Light and Matter by Richard Feynman. Princeton University Press.\n\n\nJohannes Vermeer. 1668. “The Astronomer; released into the public domain.” https://en.wikipedia.org/wiki/File:Johannes_Vermeer_-_The_Astronomer_-_1668.jpg.\n\n\nJohnsen, Sönke. 2012. The Optics of Life: A Biologist’s Guide to Light in Nature. Princeton University Press.\n\n\nJohnston-Feller, Ruth. 2001. Color Science in the Examination of Museum Objects: Nondestructive Procedures. Getty Publications.",
    "crumbs": [
      "Rendering",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "rendering-overview.html#footnotes",
    "href": "rendering-overview.html#footnotes",
    "title": "7  Overview",
    "section": "",
    "text": "and again, even though we use the term “scattering”, absorption and emission are usually involved in the most general cases↩︎\nThe electromagnetic theory does not explain everything in light-matter interactions. Famously, they do not explain how the interference pattern in the double-slit experiment still arises even if the photons are delivered sequentially.↩︎\nTechnically speaking, having a zero transmittance requires the material to have a stronger absorption than hiding the substrate, because in the latter case photons have to make a round trip, so they have more opportunities to be absorbed.↩︎\nwhich results from a combination of water selectively absorbing medium-to-long wavelengths of light and increasing scattering of short wavelengths in the Rayleigh regime (Section 12.2.4).↩︎",
    "crumbs": [
      "Rendering",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "rendering-radiometry.html",
    "href": "rendering-radiometry.html",
    "title": "8  Radiometry and Photometry",
    "section": "",
    "text": "8.1 Key Radiometric Concepts\nTo rigorously model surface and volume scattering, we must first define a set of physical quantities that characterize light in space, time, angle, and spectrum. The study of these quantities is called radiometry, which operates entirely within the domain of geometric optics, treating light as a collection of photons, each traveling in a particular direction and carrying a specific amount of energy (Section 8.1). Radiometry allows us to model light-matter interactions using the framework of radiative transfer or light transport, which describes how energy is transferred between points in space and is the key tool we will use in rendering both surface and subsurface/volume scattering.\nThe counterpart of radiometry is photometry (Section 8.2), which deals with the measurement of radiometric quantities and thus is affected by the sensitivity of the measurement device itself (e.g., retina, a photodetector). Reinhard et al. (2008, chap. 6) and Bohren and Clothiaux (2006, chap. 4) are good references for radiometry and photometry.\nWe will start by defining a few key quantities in radiometry.",
    "crumbs": [
      "Rendering",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Radiometry and Photometry</span>"
    ]
  },
  {
    "objectID": "rendering-radiometry.html#sec-chpt-mat-basics-radiometry-concepts",
    "href": "rendering-radiometry.html#sec-chpt-mat-basics-radiometry-concepts",
    "title": "8  Radiometry and Photometry",
    "section": "",
    "text": "8.1.1 Energy and Power\nEach photon carries a certain amount of energy that is determined by its wavelength governed by:\n\\[\n    Q = \\frac{hc}{\\lambda},\n\\]\nwhere \\(c\\) is the speed of light, \\(\\lambda\\) is the photon wavelength, and \\(h\\) is the Planck’s constant.\nPower, or more formally in radiometry, radiant flux (or simply flux) is the total amount of energy passing through some surface in space per unit time. Or, taking a calculus perspective, power \\(\\Phi\\) is defined as:\n\\[\n    \\Phi = \\lim_{\\Delta t \\rightarrow 0}\\frac{\\Delta Q}{\\Delta t} = \\frac{\\text{d}Q}{\\text{d}t}.\n\\]\nThe way to think about this is that each photon carries a certain amount of energy so if you monitor photons passing across a surface over a period of time \\(\\Delta t\\), you can calculate the average power of that period by dividing the total energy passed by by \\(\\Delta t\\). As \\(\\Delta t\\) approaches 0, we get the instantaneous power.\nOf course, energy/power is a function of wavelength, so more rigorously we should be talking about spectral power \\(\\Phi(\\lambda)\\), which has a unit of \\(\\text{W}/\\text{nm}\\):\n\\[\n    \\Phi(\\lambda) = \\lim_{\\Delta \\lambda \\rightarrow 0}\\frac{\\Delta \\Phi}{\\Delta \\lambda} = \\frac{\\d \\Phi}{\\d \\lambda},\n\\]\nwhere \\(\\Delta \\Phi\\) is the total power within a wavelength interval \\(\\Delta \\lambda\\).\n\n\n8.1.2 Irradiance\nOur power calculation is done with respect to a surface area, but how about the power at each point on the surface area? You can imagine that some points get more photons and others get fewer, so it is useful to characterize the power at any given point. Technically, the answer to the question “how many photons hit a particular point” is zero, since the area of a single point is 01. The meaningful question is: what is the power density of a particular point \\(p\\)? Irradiance is such a quantity.\nImagine again that you are monitoring photons crossing a surface for a period \\(\\Delta t\\); you can calculate the average power received per unit area by dividing the average power by the surface area, and when you shrink the surface area to an infinitesimal point \\(p\\), we can calculate the power density, i.e., the irradiance, of \\(p\\) by:\n\\[\n    E(p) = \\lim_{\\Delta A \\rightarrow 0}\\frac{\\Delta \\Phi(p)}{\\Delta A} = \\frac{\\text{d}\\Phi(p)}{\\text{d}A}.\n\\tag{8.1}\\]\nIrradiance is a more primitive measure than power: in calculus terms, irradiance is a power density function, which means we can derive the power of a surface by integrating the irradiance over the surface area:\n\\[\n    \\Phi = \\int^{A}E(p)dA.\n\\]\nIrradiance has a unit of \\(\\text{W}/\\text{m}^\\text{2}\\), and spectral irradiance has a unit of \\(\\text{W}/(\\text{m}^\\text{2} \\cdot \\text{nm})\\).\n\n\n8.1.3 Solid Angle\nIrradiance is concerned with the power of all the photons incident on a point, but photons hit a point from all directions, so how do we quantify the amount of light coming from a direction?\nA direction is a vector, which is invariant to translational transformations, so the two parallel “arrows” \\(r_1\\) and \\(r_2\\) in Figure 8.1 (left) represent the same vector/direction. Therefore, conceptually it is easier if we translate all the arrows so that they start from the same origin when we want to reason about a collection of directions.\n\n\n\n\n\n\nFigure 8.1: (a): a solid angle is a measure of the size of a collection of directions in 3D. A direction is a vector, which is translationally invariant, so \\(r_1\\) and \\(r_2\\) refer to the same direction. (b): in spherical coordinate systems, a 3D direction can be parameterized by two angles, a polar angle \\(\\theta\\) and an azimuthal angle \\(\\phi\\). (c): radiance is an intrinsic property of the radiation field, but we can measure it differently.\n\n\n\nHow do we count the number of directions? In 2D, we use a planar angle to measure the amount of directions. Given an origin \\(O\\) and a vector, we rotate it to generate an arc. The angle subtended by the arc and \\(O\\) is a measure of the amount of directions we have just covered. The angle can also be mathematically given by the ratio \\(s/r\\), where \\(s\\) is the arc length and \\(r\\) is the radius of the circle. This matches our intuition that if we increase the radius of the circle, we would get a longer arc but the same angle. A full circle has a planar angle of \\(2\\pi\\).\nWe can similarly define the size of a set of directions in 3D. We draw a sphere around \\(O\\), and imagine that we have some area on the spherical surface. Connecting \\(O\\) to every point on that area represents a direction in 3D. So the spherical surface area is a measure of the amount of 3D directions. Like in the 2D case, we want the measure to be invariant to the spherical radius, so we define solid angle, a measure of the size of a set of 3D directions, as:\n\\[\n    \\Omega = \\frac{A}{r^2},\n\\tag{8.2}\\]\nwhere \\(A\\) is an area on a spherical surface and \\(r\\) is the radius. The unit of a solid angle is the steradian (\\(sr\\)), and the entire sphere subtends a solid angle of \\(4\\pi\\).\nSometimes we want to know the size of the set of directions from a point \\(O\\) to an arbitrary surface. We would project that surface to a sphere and get a projected spherical area \\(A\\), using which we can invoke Equation 8.2 to estimate the solid angle subtended by the surface. One useful trick that might help sometimes is to project the surface to the unit sphere (i.e., \\(r=1\\)), and the solid angle is mathematically equivalent to the projected area on the unit sphere. But the most useful intuition I use whenever I am confused about what a particular solid angle means is to always think of the set of directions/vectors that are represented by that solid angle.\n\n\n8.1.4 Radiance\nWe can now ask, what is the amount of flux received by a point from a particular direction? Photons travel in all sorts of directions. Let’s assume that we place an imaginary flux detector with an area \\(A\\) in the field. The detector is able to receive light from only one direction \\(\\omega\\), as illustrated in Figure 8.1 (c). We can then read out the total flux \\(\\Phi\\) received by the detector, from which we know that the power per unit area along the direction \\(\\omega\\) is simply \\(\\frac{\\Phi}{A}\\).\nNow imagine that we orient the detector so that its normal subtends an angle \\(\\theta\\) with respect to the light direction \\(\\omega\\). Figure 8.1 (b) explicitly illustrates this angle, where the tilted detector lies in the \\(xy\\)-plane, and the \\(z\\) direction is the normal \\(n\\). In a spherical coordinate system, a direction \\(\\omega\\) can be parameterized by two angles: a polar angle \\(\\theta\\) and an azimuthal angle \\(\\phi\\).\nThe total flux received by the detector has changed to \\(\\Phi\\cos\\theta\\), because the area that is available to receive photons is now \\(A\\cos\\theta\\). We call this the “effective area”. As a result, the power per area at the direction \\(\\omega\\) remains the same, i.e., \\(\\frac{\\Phi}{A}\\). This is not surprising, because we are not changing the radiation field, only how we measure it. When the effective area reaches 0 (i.e., the detector is completely parallel to the light direction), the detector collects no photons, but it certainly does not mean that there is no light in the field.\nIf we now want to measure light power coming from another direction, we would change the detector so that it receives light from only that direction. In reality, this is, of course, not possible. No detector can screen lights only from one direction. If we place a detector in a radiation field, it is going to receive photons from all sorts of directions. We can limit the directions of photons that the detector collects by placing a baffle that allows only certain directions to hit the detector.\n\n\n\n\n\n\nFigure 8.2: Left: the baffle limits the directions through which incident photons can be collected by the detector. As we reduce the solid angle of the baffle \\(\\Do\\) and the detector \\(\\D A\\), the average power per unit “effective area” per unit solid angle approaches \\(L(p, \\omega)\\), the radiance at position \\(p\\) along direction \\(\\omega\\). Right: intuitively we can think of a point (an infinitesimal area) receiving lights from a single direction (an infinitesimal solid angle) as just a tiny area intercepting a tiny cylinder.\n\n\n\nThis setup is illustrated in Figure 8.2 (left). The total flux collected by the detector is \\(\\Delta \\Phi\\), the detector size is \\(\\D A\\), and the solid angle subtended by the baffle is \\(\\D \\omega\\). The average power collected per unit “effective area” per unit direction by the detector is then:\n\\[\n    \\frac{\\D \\Phi}{\\D A \\cos\\theta \\D \\omega}.\n\\]\nThe baffle does a good job of rejecting many directions that are outside \\(\\D \\omega\\), but unless it is infinitely long, the detector will still collect some photons traveling through directions outside \\(\\D \\omega\\). But as we reduce the detector size and the baffle size, the baffle becomes a very thin cylinder over a very small detector, which collects light from a very small area along a very small solid angle, visualized in Figure 8.2 (right)2. In calculus terms, when we let the detector size and baffle’s solid angle approach 0, we obtain the quantity called radiance:\n\\[\n    L(p, \\omega) = \\lim_{\\D \\omega \\rightarrow 0} \\lim_{\\D A \\rightarrow 0} \\frac{\\D \\Phi}{\\D A \\cos\\theta \\D \\omega}\n    = \\frac{\\text{d}}{\\text{d}\\omega}\\frac{\\text{d}\\Phi(p)}{\\text{d}A \\cos\\theta} = \\frac{\\text{d}^2\\Phi(p)}{\\text{d}\\omega\\text{d}A \\cos\\theta}.\n\\tag{8.3}\\]\nEquation 8.3 is the definition of radiance, and it can be rewritten to Equation 8.4 given the definition of irradiance (see Equation 8.1).\n\\[\n    L(p, \\omega) = \\frac{\\text{d}E(p)}{\\text{d}\\omega\\cos\\theta}.\n\\tag{8.4}\\]\nRadiance is an intrinsic property of the radiation field, and the reason we have the \\(\\cos\\theta\\) term in the definition is merely due to the way we have chosen to measure the property (using a detector that is \\(\\theta\\)-oriented). Radiance has a unit of \\(\\text{W}/(\\text{m}^\\text{2}\\cdot \\text{sr})\\), and spectral radiance has a unit of \\(\\text{W}/(\\text{m}^\\text{2}\\cdot \\text{sr} \\cdot \\text{nm})\\).\nLooking at the effective area in Figure 8.2, if the irradiance at the infinitesimal area \\(p\\) is \\(\\d E(p)\\), the irradiance at the (infinitesimal) effective area (projected from \\(\\d A\\) along \\(\\omega\\)) is \\(\\frac{\\d E(p)}{\\cos\\theta}\\), which we denote \\(\\d E_\\bot(p)\\). Combining this with Equation 8.4, radiance \\(L(p, \\omega)\\) can also be defined as:\n\\[\n    L(p, \\omega) = \\frac{\\text{d}E_\\bot(p)}{\\text{d}\\omega}.\n\\tag{8.5}\\]\nEquation 8.5 and Equation 8.4 each corresponds to a concrete way of measuring the radiance. Equation 8.5 places the detector perpendicular to the direction of light that we care to measure, and the detector used by Equation 8.4 is \\(\\theta\\)-oriented with respect to the direction of interest. They give us an identical radiance result because radiance, again, is an inherent property of the radiation field invariant to how we measure it.\nRadiance is a density function: the density of power at a point along a direction. As with any density function, it is useful when it gets integrated to compute some other quantities. For instance, given the radiance \\(L(p, \\omega)\\), the irradiance at \\(p\\) is given by:\n\\[\n    E(p, \\Omega) = \\int^{\\Omega}L(p, \\omega)\\cos\\theta\\text{d}\\omega.\n\\tag{8.6}\\]\nHere we write the irradiance as \\(E(p, \\Omega)\\) to explicitly signify that the irradiance depends not only on the specific position \\(p\\) but also the solid angle \\(\\Omega\\) over which the lights are coming.\nUsing the interpretation of radiance in Equation 8.5, we can also give a more operational interpretation of Equation 8.6: we first calculate the infinitesimal irradiance \\(\\d E_\\bot(p) = L(p, \\omega)\\do\\) made by lights at the direction \\(\\omega\\), then “transfer” that to the infinitesimal irradiance at the detector surface through the \\(\\cos\\theta\\) factor, and then repeat this for all the directions to accumulate the contributions from all directions.\n\n\n8.1.5 Radiant Intensity and Lambert’s Cosine Law\nA Lambertian emitter or an ideal diffuse emitter is a flux-emitting point whose emitted radiance is constant regardless of the outgoing direction. A related concept is a Lambertian scatterer or an ideal diffuse surface, which is a surface point where the scattered radiance is independent of the scattering direction.\nIt might come as a surprise that the flux emitted by a Lambertian emitter through a fixed solid angle is different for different emission directions. Consider a setup where a Lambertian emitter has an infinitesimal area \\(\\d A\\). The power emitted by \\(\\d A\\) toward its normal direction in an infinitesimal solid angle of \\(\\do\\) is \\(\\d\\Phi_0 = L\\do\\d A\\), where \\(L\\) is the radiance. The power emitted toward an oblique direction \\(\\omega\\) through the same solid angle is \\(\\d\\Phi_\\theta = L \\do \\cos\\theta \\d A\\).\nIn radiometry, the ratio of infinitesimal power and infinitesimal solid angle is called the radiant intensity3, denoted \\(I\\):\n\\[\n    I(\\omega) = \\frac{\\d\\Phi}{\\do}.\n\\tag{8.7}\\]\n\\(I\\) is a meaningful measure only for a point source (e.g., our infinitesimal Lambertian emitter here). We can see that for a Lambertian emitter, the radiant intensity decays by a factor of \\(\\cos\\theta\\): \\(\\frac{\\d\\Phi_\\theta}{\\d\\omega} = \\frac{\\d\\Phi_0}{\\d\\omega}\\cos\\theta\\). This is usually called the Lambert’s cosine law, named after Johann Heinrich Lambert, from his Photometria (Lambert 1760). Similarly, if we have a Lambertian scatterer, its scattered radiant intensity will also decay by \\(\\cos\\theta\\) as the polar angle \\(\\theta\\) of the viewing direction \\(\\omega\\) increases.\n\n\n\n\n\n\nFigure 8.3: Comparison between the radiance distribution (constant w.r.t. viewing direction \\(\\omega\\)) and radiant intensity distribution (weakens by a factor of \\(\\cos\\theta\\)) of a Lambertian emitter/scatterer.\n\n\n\nFigure 8.3 compares the radiance distribution and radiant intensity distribution of a Lambertian emitter/scatterer. Both distributions are over the entire hemisphere, but we show only a cross section. The distributions are visualized as two lobes, and the distance of a point on the lobe to the origin is proportional to the value at that point. The radiance distribution is constant regardless of \\(\\omega\\) but the radiant intensity is proportional to \\(\\cos\\theta\\). This difference stems from the fact that intensity is defined with respect to the power at the detector/emission area (\\(\\d A\\)) while radiance is defined with respect to power at the effective area (\\(\\d A \\cos\\theta\\)).",
    "crumbs": [
      "Rendering",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Radiometry and Photometry</span>"
    ]
  },
  {
    "objectID": "rendering-radiometry.html#sec-chpt-mat-basics-radiometry-pm",
    "href": "rendering-radiometry.html#sec-chpt-mat-basics-radiometry-pm",
    "title": "8  Radiometry and Photometry",
    "section": "8.2 Photometric Quantities",
    "text": "8.2 Photometric Quantities\nSpectral radiant flux (power), irradiance, radiant intensity, and radiance are all radiometric quantities. They all have a photometric counterpart, which weighs the radiometric quantity by the luminous efficiency function (LEF). The LEF, as we have discussed in Section 4.3.2, at a particular wavelength is inversely proportional to the radiometric quantity at each wavelength needed to produce the same level of perceptual brightness.\nFor instance, given a spectral radiant flux \\(\\Phi(\\lambda)\\), the corresponding photometric counterpart is then:\n\\[\n    \\Phi_v(\\lambda) = K \\Phi(\\lambda) V(\\lambda),\n\\]\nwhere \\(\\Phi_v(\\lambda)\\) is the spectral luminous flux, \\(V(\\lambda)\\) is the LEF, and \\(K\\) is a constant that, for historical reasons, takes the value of 683.002. The total luminous flux is then:\n\\[\n    \\Phi_v = \\int_\\lambda K \\Phi(\\lambda) V(\\lambda) \\d \\lambda.\n\\]\nLuminous flux has a unit of lumen (\\(\\text{lm}\\)), so \\(K\\) has a unit of \\(\\text{lm}/\\text{W}\\). We can also weigh the radiant power by the scotopic LEF, in which case \\(\\text{K} = 1700\\) (\\(\\text{lm}/\\text{W}\\)).\nOther radiometric quantities can be similarly converted to the photometric counterparts. Specifically:\n\nthe photometric counterpart of irradiance is illumination, which has a unit of \\(\\text{lx} = \\text{lm}/\\text{m}^2\\), which is also called the lux;\nthe photometric counterpart of radiance intensity is luminous intensity, which has a unit of \\(\\text{cd} = \\text{lm}/\\text{sr}\\), which is called the candela;\nthe photometric counterpart of radiance is luminance, which has a unit of \\(\\text{lm}/(\\text{m}^2\\text{sr}) = \\text{cd}/(\\text{m}^2)\\), which is also called the nit.\n\nSometimes radiometric vs. photometric quantities are also called the radiant vs. luminous quantities. The way to interpret the photometric quantities is that they take into account the spectral sensitivity of a particular photodetector, which in our discussion above is the photoreceptors on the retina, so the spectral sensitivity function of the detector is the LEF. If we use other detectors, such as an image sensor, we will have a different spectral sensitivity, and the corresponding photometric measurements will be different. We will study the spectral sensitivity of image sensors in Section 16.5.\nA radiometer measures the absolute radiometric quantities, whereas a photometer reports photometric quantities. An image sensor and our retina can both be thought of as a photometer but the spectral sensitivities in the two cases are different, so the raw pixel readings and the photoreceptor responses are different even under an identical illumination.\n\n\n\n\nBohren, Craig F, and Eugene E Clothiaux. 2006. Fundamentals of Atmospheric Radiation: An Introduction with 400 Problems. John Wiley & Sons.\n\n\nLambert, Jean-Henri. 1760. Photometria Sive de Mensura Et Gradibus Luminis, Colorum Et Umbrae. Sumptibus viduae Eberhardi Klett, typis Christophori Petri Detleffsen.\n\n\nReinhard, Erik, Erum Arif Khan, Ahmet Oguz Akyuz, and Garrett Johnson. 2008. Color Imaging: Fundamentals and Applications. CRC Press.",
    "crumbs": [
      "Rendering",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Radiometry and Photometry</span>"
    ]
  },
  {
    "objectID": "rendering-radiometry.html#footnotes",
    "href": "rendering-radiometry.html#footnotes",
    "title": "8  Radiometry and Photometry",
    "section": "",
    "text": "A similar question is: imagine you are throwing a dart at a wall, what is the probability of hitting a particular point \\(p\\)? The answer is 0. The meaningful question to ask is: what is the probability density of hitting \\(p\\)?↩︎\nIt is just a visualization convention, but visualizing \\(\\do\\) as a cylinder rather than a cone makes it easier to imagine what \\(\\d A \\cos\\theta\\) is like.↩︎\nOr simply, the “intensity”, which is an extremely overloaded term, so we will be verbose and use “radiant intensity” when we mean it.↩︎",
    "crumbs": [
      "Rendering",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Radiometry and Photometry</span>"
    ]
  },
  {
    "objectID": "rendering-lightfield.html",
    "href": "rendering-lightfield.html",
    "title": "9  Light Field",
    "section": "",
    "text": "9.1 The Measurement Equation\nGiven that we have the basic understanding of radiometry, now seems like a good time to show how radiometry is of fundamental importance to computer graphics and imaging. We do so by building two important concepts using radiometry: the camera measurement equation (Section 9.1) and the light field (Section 9.2).\nFor simplicity, let’s just consider one single pixel with a setup illustrated in Figure 9.1.\nEach pixel is very small, but it has a finite area, say \\(A_p\\). Each pixel is constantly being bombarded by lights that enter the aperture, which has an area \\(V\\). The raw pixel value is roughly proportional to the energy it receives during the exposure time1. So using the basic radiometry, we can write the total energy received by a pixel during the exposure time \\(T\\) as:\n\\[\n    Q = \\int^{T} \\int^{A_p} \\int^{\\Omega(p, V)} L(p, \\omega) \\cos\\theta~\\text{d}\\omega~\\text{d}p~\\text{d}t,\n\\tag{9.1}\\]\nwhere \\(\\Omega(p, V)\\) explicitly expresses that a solid angle is determined by the aperture \\(V\\) and a point \\(p\\) on the pixel surface. Of course this quantity changes with \\(p\\). We sometimes omit \\(p\\) and \\(V\\) when it is clear what \\(\\Omega\\) refers to, but here, since the solid angle changes with the dummy variable \\(p\\) in the integral equation, we express it explicitly.  In graphics literature, this equation is sometimes called the measurement equation of an image sensor (Kolb, Mitchell, and Hanrahan 1995; Reinhard et al. 2008, chap. 6.8.1; Pharr, Jakob, and Humphreys 2023, chap. 5.4).\nThe inner integral in Equation 9.1 is expressed over the solid angle, which varies with \\(p\\). A more common, but equivalent, formulation of the measurement equation is to re-express the inner integral over the aperture area \\(V\\):\n\\[\n    Q = \\frac{1}{d^2} \\int^{T} \\int^{A_p} \\int^{V} L(p, \\omega) |\\cos^4\\theta|~\\text{d}p'~\\text{d}p~\\text{d}t,\n\\tag{9.2}\\]\nwhere \\(d\\) is the distance between the aperture plane and the sensor plane, and \\(p'\\) is a point on the aperture plane. The derivation is available in standard texts (Pharr, Jakob, and Humphreys 2023, chap. 5.4.1) and is omitted here.\nThe measurement equation is concerned with the radiance distribution inside a camera, but the only reason there is a radiance distribution inside the camera is because there is an external radiance distribution in the scene impinging upon the camera optics, which act as a transfer function that turns external radiance into internal radiance. The transfer function is determined by the material properties of the camera optics (e.g., lenses, filters, etc.), whose effects are nothing more than surface scattering and volume scattering, topics of the next two chapters.\nUsing Figure 9.1 as a concrete example, to know the radiance \\(L(p, \\omega)\\) inside the camera, we need to know \\(L(p', \\omega')\\), the corresponding radiance in the scene and how the latter is transferred to the former. If the camera is an ideal pinhole, we have \\(\\omega = \\omega'\\) and \\(L(p, \\omega) = L(p', \\omega')\\) (ignoring diffraction). If the camera uses an ideal convex lens, the relationship between the two rays is governed by the Gauss lens equation (Section 15.3.1) and, with some simplifications, \\(L(p, \\omega) = L(p', \\omega')\\) still holds (Section 15.3.5). The transfer function is more complicated when as the camera optics become more complicated. Imaging we replace the lenses with a duck tape — how would the radiance be transferred?\nThe measurement equation is important because it fundamentally allows us to, in theory, synthesize/render any image taken by any camera at any viewpoint — given that we know the radiance distribution of the scene. Using Figure 9.2 as an example, let us simulate a new camera where the sensor is moved closer to the lens. To calculate the pixel value \\(p_c\\) of this new camera imaging the scene, it requires nothing more than invoking the measurement equation Equation 9.1 at \\(p_c\\), integrating over all the incident rays, which is a portion of the overall radiance distribution. This is why having access to the underlying radiance field allows us to synthesize new images.\nCritically, observe that two of the rays that \\(p_c\\) needs are already captured by \\(p_a\\) and \\(p_b\\) in the current camera. So it is only natural to ask: can we synthesize new images from images taken from the same scene? How do we systematically reason about this? Read on.",
    "crumbs": [
      "Rendering",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Light Field</span>"
    ]
  },
  {
    "objectID": "rendering-lightfield.html#sec-chpt-mat-basics-radiometry-cam",
    "href": "rendering-lightfield.html#sec-chpt-mat-basics-radiometry-cam",
    "title": "9  Light Field",
    "section": "",
    "text": "Figure 9.1: Geometric setting for the camera measurement equation. Calculating the pixel value requires integrating the energy of all the rays hitting the pixel area, which requires knowing the light field inside the camera, which, in turn, requires knowing the light field in the scene and how the camera optics transfer the external light field to the internal light field.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9.2: The light field is described by the plenoptic function, which describes the radiance of any ray, i.e., the energy at any position, along any ray direction, at any wavelength, and at any time. In free space, the plenoptic function is invariant to traversal along the ray propagation direction (\\(P_1\\) and \\(P_2\\) share the same radiance but not with \\(P_3\\)). Having access to the entire light field allows us to synthesize any image taken by any camera (e.g., moving the sensor closer to the lens). A lens-based camera, however, is a poor device to capture the light field, since each pixel necessarily integrates many rays.",
    "crumbs": [
      "Rendering",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Light Field</span>"
    ]
  },
  {
    "objectID": "rendering-lightfield.html#sec-chpt-mat-basics-radiometry-lf",
    "href": "rendering-lightfield.html#sec-chpt-mat-basics-radiometry-lf",
    "title": "9  Light Field",
    "section": "9.2 Light Field",
    "text": "9.2 Light Field\nThere is a name for the distribution of the radiance in the space — it is called the light field, which refers to the complete set of all the possible radiances flowing through every possible direction. The light field is thus a function \\(L(p, \\omega, \\lambda, t)\\), describing the energy of a ray passing the position \\(p\\), along the direction \\(\\omega\\), at time \\(t\\) and wavelength \\(\\lambda\\). This function is also called the plenoptic function (Bergen and Adelson 1991; Gortler et al. 1996; Levoy and Hanrahan 1996). Figure 9.2 shows a tiny portion of the light field — six rays in fact; three inside the camera and three outside the camera.\n\n9.2.1 Light-Field Imaging\nThe field of light-field imaging is concerned with measuring the light field of a scene, which is a task impossible — we cannot possibly measure the radiance of every single ray. There are some simplifications we can make. For instance, we can assume that a ray’s energy does not change in free space during propagation, so the plenoptic function is invariant along the ray traversal direction; we can also assume that the light field is time-invariant during the period of interest. But still, the task of measuring the entire field is a daunting one.\nThe next best thing is to sample the light field. A lens-based camera does a poor job of sampling the light field. The pixel \\(p_a\\) integrates a bundle of rays, two of which are shown. Even assuming that the ray’s radiance remains unchanged as it passes through the lens, the inherent integration by the pixel (i.e., the measurement equation in Equation 9.1) still means from the pixel value itself we could not decouple the radiance of the incident rays. Therefore, the ray that \\(p_c\\) wants cannot be easily extracted from \\(p_a\\). Using an ideal pinhole helps, but pinhole imaging comes with its own limitations that make it infeasible in practice (Section 15.2).\n\n\n\n\n\n\nFigure 9.3: A pixel in a conventional camera (e.g., \\(q\\)) integrates over a large portion of the light field. By placing a microlens array (here at where the sensor plane would have been in lieu of the microlens array), each pixel (e.g., \\(p\\)) now integrates over a small portion of the light field.\n\n\n\nA vast literature exists in effective light-field sampling (Lam 2015, sec. 3). A good trade-off in practice is to insert a lenticular array or a microlens array between the main imaging lens and the sensor plane (Ng 2006; Adelson and Wang 1992). The idea was first conceptualized by Gabriel Lippmann (Lippmann 1908)2 Figure 9.3 shows one such example. Without the microlens array, a pixel (e.g., \\(q\\)) would integrate over all the rays that are subtended by the main lens, which is relatively large. Now we insert a microlens array and move the sensor plane a little farther back; each pixel (e.g., \\(q\\)) now integrates over a much smaller portion of the light field (rays subtended by a microlens), providing a higher angular resolution in light-field measurement.\n\n\n9.2.2 Light-Field Rendering\nThe main reason we want to measure the light field is so that we can render new images. Light-field rendering is concerned with rendering a new image at a novel perspective (or by a novel camera configuration) given a set of images from other perspectives/configurations. It is a form of image-based rendering. In this sense, many familiar tasks such as interpolating between video frames, panoramic photography, and (stereoscopic) 360\\(^\\circ\\) video rendering are all light-field rendering in disguise.\nGiven that each image is a sample of a portion of the light field followed by a low-pass filter (i.e., the integration in Equation 9.1), rendering an image at a new perspective is nothing more than estimating another sample of the light field. As with any signal re-sampling task, the ideal solution to light-field rendering is to first reconstruct the underlying light field from a set of samples and then re-sample the light field given the new perspective. Signal filtering is necessary for both signal reconstruction and anti-aliasing, and the name of the game is to design good filters that are practically useful and computationally tractable (Pharr, Jakob, and Humphreys 2023, chap. 8.8).\nOf course, modern image-based rendering, known under the name (neural) radiance-field rendering (Mildenhall et al. 2021; Kerbl et al. 2023), approaches the whole problem through machine learning and learns to reconstruct from massive amounts of data. To be precise, these methods do not reconstruct the light field; they reconstruct the radiance field.\n\n\n9.2.3 Radiance-Field Rendering\nRadiance field, popularized by Mildenhall et al. (2021), applies a simplification and an addition to a light field. A radiance field is described by a function \\(R(p, \\omega, r, g, b, \\sigma)\\), describing the \\((r, g, b)\\) color and the density \\(\\sigma\\) of a ray passing through a position \\(p\\) along the direction \\(\\omega\\). Compare that with the plenoptic function, we can see that the radiance field function simplifies the the energy spectrum into just the tristimulus color values and assumes that the energy is time-invariant.\nImportantly, the radiance field incorporates a new quantity, density, that is absent in the light field. Density has nothing to do with the energy of a ray; rather, it is/models an intrinsic property of the material (at position \\(p\\) along ray direction \\(\\omega\\)). Materials are important for imaging and rendering, because they change the light field of the scene — through surface scattering and volume scattering. After all, rendering is a process of simulating the light-matter interactions.\nIn essence, a radiance field combines both a (simplified) light field, a property of the light, and a density field, a property of the materials. This simple extension from light to materials allows radiance-field methods to model (in fact, learn) material properties, which in turn enables more effective light-field rendering. Conventional light-field rendering, in contrast, does not attempt to decouple the light field from the material properties.\nRadiance-field methods learn, from offline captured images (hence image-based rendering), to predict the tristimulus color values and density of a given point along a given direction:\n\\[\nf: (p, \\omega) \\mapsto r, g, b, \\sigma.\n\\]\nThe function \\(f\\) can be parameterized in many ways. Two of most popular parameterizations are to use either a neural network (Mildenhall et al. 2021) or a mixture of Gaussians (Kerbl et al. 2023). With \\(f\\), we can then synthesize/rendering any image — by co-opting the classic volume rendering. We will study density and radiance field in much greater detail in Chapter 13.\n\n\n9.2.4 Light-Field Display\nLight-field display is a 3D display technology that attempts to reproduce the light field of a scene (Jones et al. 2007; Wetzstein et al. 2012; Lanman and Luebke 2013). Reproducing the light field provides the depth information of a scene that is missing in conventional 2D displays and can, thus, accurately drive the accommodation of eye lens in immersive (AR/VR) environment (Wann, Rushton, and Mon-Williams 1995; Hoffman et al. 2008). Other technologies include varifocal displays, multi-focal displays, and holographic displays.\n\n\n\n\n\n\nFigure 9.4: We first capture the light field (here using a pinhole array) and then reproduce the light field (by placing the displays on the other side of the pinhole array), offering depth cues.\n\n\n\nFigure 9.4 shows a usual two-stage process of displaying a light-field. The first step is to capture the light field using some form of light-field imaging technique discussed in Section 9.2.1; here we use a pinhole array placed in front of the sensor plane. Each pinhole covers a small group of the pixels on the sensor; the image captured by the group of pixels under each pinhole is called an elemental image. Once we have recorded the light field, we can then reproduce it. This is done by displaying the elemental images, each with a display placed on the other side of the pinhole array. Note that the relative positions of the display pixels are reversed from that of the the image pixels during light-field recording.\n\n\n\n\nAdelson, Edward H, and John YA Wang. 1992. “Single Lens Stereo with a Plenoptic Camera.” IEEE Transactions on Pattern Analysis and Machine Intelligence 14 (2): 99–106.\n\n\nBergen, James R, and Edward H Adelson. 1991. “The Plenoptic Function and the Elements of Early Vision.” Computational Models of Visual Processing 1 (8): 3.\n\n\nGortler, Steven J, Radek Grzeszczuk, Richard Szeliski, and Michael F Cohen. 1996. “The Lumigraph.” In ACM Transactions on Graphics (ToG), 43–54. ACM New York, NY, USA.\n\n\nHoffman, David M, Ahna R Girshick, Kurt Akeley, and Martin S Banks. 2008. “Vergence–Accommodation Conflicts Hinder Visual Performance and Cause Visual Fatigue.” Journal of Vision 8 (3): 33–33.\n\n\nJones, Andrew, Ian McDowall, Hideshi Yamada, Mark Bolas, and Paul Debevec. 2007. “Rendering for an Interactive 360 Light Field Display.” In ACM SIGGRAPH 2007 Papers, 40–es.\n\n\nKerbl, Bernhard, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 2023. “3d Gaussian Splatting for Real-Time Radiance Field Rendering.” ACM Trans. Graph. 42 (4): 139–31.\n\n\nKolb, Craig, Don Mitchell, and Pat Hanrahan. 1995. “A Realistic Camera Model for Computer Graphics.” In Proceedings of the 22nd Annual Conference on Computer Graphics and Interactive Techniques, 317–24.\n\n\nLam, Edmund Y. 2015. “Computational Photography with Plenoptic Camera and Light Field Capture: Tutorial.” Journal of the Optical Society of America A 32 (11): 2021–32.\n\n\nLanman, Douglas, and David Luebke. 2013. “Near-Eye Light Field Displays.” ACM Transactions on Graphics (TOG) 32 (6): 1–10.\n\n\nLevoy, Marc, and Pat Hanrahan. 1996. “Light Field Rendering.” In ACM Transactions on Graphics (ToG), 31–42. ACM New York, NY, USA.\n\n\nLippmann, Gabriel. 1908. “Epreuves Reversibles Donnant La Sensation Du Relief.” J. Phys. Theor. Appl. 7 (1): 821–25.\n\n\nMildenhall, Ben, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. 2021. “Nerf: Representing Scenes as Neural Radiance Fields for View Synthesis.” Communications of the ACM 65 (1): 99–106.\n\n\nNg, Ren. 2006. “Digital Light Field Photography.” PhD thesis, Stanford University.\n\n\nPharr, Matt, Wenzel Jakob, and Greg Humphreys. 2023. Physically Based Rendering: From Theory to Implementation. 4th ed. MIT Press.\n\n\nReinhard, Erik, Erum Arif Khan, Ahmet Oguz Akyuz, and Garrett Johnson. 2008. Color Imaging: Fundamentals and Applications. CRC Press.\n\n\nWann, John P, Simon Rushton, and Mark Mon-Williams. 1995. “Natural Problems for Stereoscopic Depth Perception in Virtual Environments.” Vision Research 35 (19): 2731–36.\n\n\nWetzstein, Gordon, Douglas R Lanman, Matthew Waggener Hirsch, and Ramesh Raskar. 2012. “Tensor Displays: Compressive Light Field Synthesis Using Multilayer Displays with Directional Backlighting.”",
    "crumbs": [
      "Rendering",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Light Field</span>"
    ]
  },
  {
    "objectID": "rendering-lightfield.html#footnotes",
    "href": "rendering-lightfield.html#footnotes",
    "title": "9  Light Field",
    "section": "",
    "text": "Assuming there is no noise and there is no quantization error in converting analog signals to digital signals.↩︎\nLippmann did not get to implement the idea. He won the Nobel Prize in Physics in 1908 for inventing, for the first time, a method for color photography.↩︎",
    "crumbs": [
      "Rendering",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Light Field</span>"
    ]
  },
  {
    "objectID": "rendering-re.html",
    "href": "rendering-re.html",
    "title": "10  Rendering Surface Scattering",
    "section": "",
    "text": "10.1 BRDF\nWhen a group of photons arrives at a material surface, some of the photons might be immediately turned away (i.e., reflected) while others might penetrate into the material (i.e., refracted). The refracted portion is scattered further inside the material, giving rise to subsurface scattering. Let’s focus first on surface scattering, ignoring the contribution of surface scattering to an object’s appearance.\nThere are two properties we care about in surface scattering: the directions of the reflection and the (spectral) energy along each direction. The direction is important because, as far as rendering, human vision, or camera imaging are concerned, only the photons that will eventually captured by a detector matter. The (spectral) energy is important because it dictates the perceived color. These two properties are captured by what is known as the BRDF (Section 10.1). We will then derive a few very useful equations and properties based on BRDF (Section 10.2). Given the BRDF, we will introduce the rendering equation, the theoretical tool for rendering surface scattering (Section 10.3).\nGenerally, the energy distribution of the surface scattering is captured by the Bidirectional Reflectance Distribution Function (BRDF) (Nicodemus et al. 1977). Informally, it tells us how the incident energy from a particular direction is distributed to different exiting directions. The BRDF is parameterized by three parameters: a surface point \\(p\\), the direction of light incident on \\(p\\), denoted \\(\\oi\\), and the direction of light leaving \\(p\\), denoted \\(\\os\\). So the BRDF is usually written as \\(f_r(p, \\os, \\oi)\\).\nThe way to understand BRDF \\(f_r(p, \\os, \\oi)\\) is to consider the following. \\(L(p, \\os)\\), i.e., the radiance leaving \\(p\\) toward \\(\\os\\), is dependent on the light incident on \\(p\\). When the incident light on \\(p\\) comes from only the direction \\(\\oi\\), the irradiance at \\(p\\) is zero, since the solid angle of a single direction \\(\\oi\\) is zero, so naturally \\(L(p, \\os)\\) is 0 (assuming there is no other light hitting \\(p\\)). When \\(p\\) receives light from a non-zero solid angle of directions \\(\\Delta\\oi\\) (centered around \\(\\oi\\)), the irradiance of \\(p\\) is increased by \\(\\Delta E(p, \\oi)\\). At the same time, due to this increase in incident light, \\(L(p, \\os)\\) is no longer zero; the increase in the radiance leaving \\(p\\) over \\(\\os\\) is denoted \\(\\Delta L(p, \\os)\\).\nAs we increase \\(\\Delta \\oi\\), both \\(\\Delta E(p, \\oi)\\) and \\(\\Delta L(p, \\os)\\) increase. BRDF is defined as the ratio of the two increments when \\(\\Delta \\oi\\) approaches 0 (when the radiance along all directions in \\(\\Delta \\oi\\) can be thought of as a constant):\n\\[\n    f_r(p, \\os, \\oi) = \\lim_{\\Delta \\oi \\rightarrow 0}\\frac{\\Delta L(p, \\os)}{\\Delta E(p, \\oi)} = \\frac{\\text{d}L(p, \\os)}{\\text{d}E(p, \\oi)} = \\frac{\\text{d}L(p, \\os)}{L(p, \\oi)\\cos\\theta_i \\text{d}\\oi}.\n\\tag{10.1}\\]",
    "crumbs": [
      "Rendering",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Rendering Surface Scattering</span>"
    ]
  },
  {
    "objectID": "rendering-re.html#sec-chpt-mat-ss-brdf",
    "href": "rendering-re.html#sec-chpt-mat-ss-brdf",
    "title": "10  Rendering Surface Scattering",
    "section": "",
    "text": "10.1.1 A Useful Approximation\nNow assume that we illuminate \\(p\\) through a finite, but small, solid angle \\(\\Oi\\). Turning the differential equation into an integral equation, we get:\n\\[\n    L(p, \\os) = \\int^{\\Oi} f_r(p, \\os, \\oi) \\text{d}E(p, \\oi).\n\\tag{10.2}\\]\nUsing the definition of radiance in Equation 8.4, we get: \\[\n    L(p, \\os) = \\int^{\\Oi} f_r(p, \\os, \\oi) L(p, \\oi)\\cos\\theta_i \\text{d}\\oi.\n\\tag{10.3}\\]\nIf we assume that the BRDF is a constant over all the directions in \\(\\Oi\\), Equation 10.3 is simplified to:\n\\[\n    L(p, \\os) \\approx f_r(p, \\os, \\oi) \\int^{\\Oi} L(p, \\oi)\\cos\\theta_i \\text{d}\\oi.\n\\tag{10.4}\\]\nThe integration in Equation 10.4 has no analytical solution, since we do not know the analytical form of \\(L(p, \\oi)\\), but we know the integration is just another way of expressing the total irradiance incident upon \\(p\\) over \\(\\Oi\\), which is denoted as \\(E(p, \\Oi)\\)1. This gets us Equation 10.5:\n\\[\n    L(p, \\os) \\approx f_r(p, \\os, \\oi) E(p, \\Oi).\n\\tag{10.5}\\]\nThus, we can approximate the BRDF as: \\[\n    f_r(p, \\os, \\oi) \\approx \\frac{L(p, \\os)}{E(p, \\Oi)}.\n\\tag{10.6}\\]\nUltimately, we can see from Equation 10.6 that the BRDF \\(f_r(p, \\os, \\oi)\\) can also be calculated as the ratio between the absolute radiance \\(L(p, \\oi)\\) and the absolute irradiance \\(E(p, \\Oi)\\) illuminated from a very small, but finite solid angle \\(\\Oi\\). Another way to interpret this is that the so-calculated BRDF is the average BRDF over \\(\\Oi\\). This derivation is useful for actually measuring a BRDF, which we will discuss in Section 11.3.2, where we will have no choice but to use a non-zero solid angle for illumination, because physically we just cannot illuminate a point through an infinitesimal solid angle \\(\\doi\\).\n\n\n10.1.2 Isotropic Material\nA 3D direction \\(\\omega\\) expressed in the Cartesian coordinate system can also be expressed by two 2D planar angles in the spherical coordinate system: the polar angle \\(\\theta\\) and the azimuthal angle \\(\\phi\\). So BRDF can also be parameterized as \\(f_r(p, \\theta_s, \\phi_s, \\theta_i, \\phi_i)\\). A material is isotropic if its BRDF satisfies \\(f_r(p, \\theta_s, \\phi_s, \\theta_i, \\phi_i) = f_r(p, \\theta_s, \\phi_s+x, \\theta_i, \\phi_i+x)\\) for any \\(x\\). An intuitive way to think of an isotropic material is this: if you pick a point \\(p\\) and rotate the material about the normal vector at \\(p\\), the color of \\(p\\) does not change. This is because rotation about the normal vector keeps \\(\\theta_i\\) and \\(\\theta_s\\) unchanged and varies \\(\\phi_i\\) and \\(\\phi_s\\) by the same amount.\nThe nice thing about an isotropic BRDF is it can be parameterized with one fewer degree of freedom: \\(f_r(p, \\theta_s, \\phi_s - \\phi_i, \\theta_i)\\). This is because it is \\((\\phi_i - \\phi_s)\\) rather than the specific values of \\(\\phi_s\\) or \\(\\phi_i\\) that matter.",
    "crumbs": [
      "Rendering",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Rendering Surface Scattering</span>"
    ]
  },
  {
    "objectID": "rendering-re.html#sec-chpt-mat-ss-reflectance",
    "href": "rendering-re.html#sec-chpt-mat-ss-reflectance",
    "title": "10  Rendering Surface Scattering",
    "section": "10.2 Reflectance and Albedo",
    "text": "10.2 Reflectance and Albedo\nThe BRDF does not have to be a value between 0 and 1. Let’s say that there is 100 J of energy incident on a point coming from a solid angle \\(\\Delta \\oi\\). That amount of energy is distributed across all the outgoing directions in the hemisphere, which forms a solid angle of \\(4\\pi/2 = 2\\pi\\). So on average the energy exiting per direction is \\(\\frac{100}{2\\pi}J\\), which clearly is greater than 1. This is not surprising, since BRDF is ultimately a density measure, a distribution, which is most meaningful when it is integrated to calculate some quantity. Integrating the BRDF gives a percentage/fraction measure between 0 and 1, i.e., reflectance, which we will discuss next.\n\n10.2.1 Directional-Hemispherical Reflectance\nFor the energy to be conserved, the total outgoing energy at any point must not exceed that of the incident energy received by that point. Assume that a point \\(p\\) receives an irradiance \\(\\d E_i\\) from a direction \\(\\oi\\) over an infinitesimal solid angle \\(\\doi\\), and the outgoing radiance along the direction \\(\\os\\) due to that irradiance is \\(f_r(p, \\os, \\oi)\\d E_i\\). Then the outgoing irradiance leaving \\(p\\) over an infinitesimal solid angle \\(\\dos\\) around \\(\\os\\) would be \\(f_r(p, \\os, \\oi)\\d E_i\\cos\\theta_s\\dos\\). If we integrate all the outgoing directions \\(\\Omega\\), we get the total outgoing irradiance \\(\\d E_o\\), which must not exceed the incident irradiance \\(\\d E_i\\):\n\\[\n    \\d E_o = \\int^{\\Omega} \\d E_i f_r(p, \\os, \\oi) \\cos\\theta_s\\text{d}\\os.\n\\tag{10.7}\\]\n\\(\\d E_i\\) is independent of \\(\\os\\), so it can be hoisted out of the integration. Therefore, we have the following equation, which holds for any arbitrary incident direction \\(\\oi\\):\n\\[\n    \\int^{\\Omega}f_r(p, \\os, \\oi)\\cos\\theta_s\\text{d}\\os = \\frac{\\d E_o}{\\d E_i} = \\rho_{dh}(p, \\oi) \\leq 1.\n\\tag{10.8}\\]\n\\(\\rho_{dh}\\) is defined as the ratio between \\(\\d E_o\\) and \\(\\d E_i\\). When \\(\\Omega\\) is the hemisphere, \\(\\rho_{dh}\\) is called the directional-hemispherical reflectance in the computer vision and graphics literature, and is interpreted as the percentage of energy scattered by a point over the entire hemisphere given the incident light from a particular direction. Clearly, \\(\\rho_{dh}\\) is a function of both \\(p\\) and \\(\\oi\\) and takes a value between 0 and 1.\n\n\n10.2.2 Hemispherical-Directional Reflectance\nSince we are dealing with geometric optics, the Helmholtz reciprocity holds:\n\\[\n    f_r(p, \\os, \\oi) = f_r(p, \\oi, \\os),\n\\]\nwhich means the energy conservation can also be expressed as:\n\\[\n    \\int^{\\Omega}f_r(p, \\os, \\oi)\\cos\\theta_i\\text{d}\\oi = \\rho_{hd}(p, \\os) \\leq 1,\n\\tag{10.9}\\]\nwhere \\(\\rho_{hd}\\) is called the hemispherical-directional reflectance when \\(\\Omega\\) is the hemisphere. \\(\\rho_{hd}\\), a function of \\(p\\) and \\(\\os\\), is interpreted as the percentage of energy reflected toward a particular direction \\(\\os\\) given the incident energy over the entire hemisphere.\nEquation 10.9 can be derived by first rewriting Equation 10.8 as \\(\\int^{\\Omega}f_r(p, \\oi, \\os)\\cos\\theta_s\\text{d}\\os \\leq 1\\) (using the reciprocity) followed by switching \\(\\os\\) and \\(\\oi\\) (simply a change of notation). This derivation suggests that \\(\\rho_{hd}(p, \\oi) = \\rho_{dh}(p, \\os)\\), a natural consequence of the reciprocity2.\n\n\n10.2.3 Albedo and Hemispherical-Hemispherical Reflectance\nWe can also describe the relationship between all the outgoing irradiance \\(E_o\\) of a point over a solid angle \\(\\Os\\) due to all the incident irradiance \\(E_i\\) over a solid angle \\(\\Oi\\):\n\\[\n\\begin{aligned}\n    E_o &= \\int^{\\Omega_s} \\Big(\\int^{\\Omega_i} f_r(p, \\os, \\oi) L(p, \\oi) \\cos\\theta_i\\text{d}\\oi\\Big) \\cos\\theta_s \\dos, \\\\\n    E_i &= \\int^{\\Omega_i} L(p, \\oi) \\cos\\theta_i\\text{d}\\oi\n\\end{aligned}\n\\tag{10.10}\\]\nDue to energy conservation, we have:\n\\[\n    \\rho_{hh}(p) = \\frac{E_o}{E_i} \\leq 1.\n\\tag{10.11}\\]\nEquation 10.11 defines \\(\\rho_{hh}\\), which is called the hemispherical-hemispherical reflectance when both \\(\\Omega_i\\) and \\(\\Omega_s\\) are hemispheres. \\(\\rho_{hh}\\) has another name: albedo.\nWhen \\(f_r(p, \\os, \\oi)\\) is independent of (invariant to) \\(\\oi\\) and \\(\\os\\), i.e., when \\(p\\) is an ideal Lambertian surface (see Section 11.1), Equation 10.10 can be re-written as:\n\\[\n    E_o = \\int^{\\Omega_s} f_r(p, \\os, \\oi) (\\int^{\\Omega_i} L(p, \\oi) \\cos\\theta_i\\text{d}\\oi) \\cos\\theta_s \\dos.\n\\tag{10.12}\\]\nPlugging in the definition of \\(E_i\\) from Equation 10.10, we have: \\[\n    E_o = \\int^{\\Omega_s} f_r(p, \\os, \\oi) E_i \\cos\\theta_s \\dos.\n\\tag{10.13}\\]\nSince \\(E_i\\) is independent of \\(\\os\\), we have:\n\\[\n    E_o = E_i  \\int^{\\Omega_s} f_r(p, \\os, \\oi) \\cos\\theta_s \\dos.\n\\tag{10.14}\\]\nUsing the definition of \\(\\rho_{dh}\\) in Equation 10.8, we have:\n\\[\n    E_o = E_i \\rho_{dh}(p, \\oi).\n\\tag{10.15}\\]\nComparing Equation 10.15 and Equation 10.11, we can see that for a Lambertian surface the albedo (\\(\\rho_{hh}\\)) is equivalent to \\(\\rho_{dh}\\) and \\(\\rho_{hd}\\), but this relationship is not true in general.\nWe can also show that for a Lambertian surface, the BRDF is the constant \\(\\frac{\\rho_{hh}}{\\pi}\\). Starting from Equation 10.14 and using the assumption that \\(f_r(p, \\os, \\oi)\\) is independent of \\(\\os\\):\n\\[\n\\begin{aligned}\n    E_o = E_i \\rho_{hh} &= E_i  \\int^{\\Omega_s} f_r(p, \\os, \\oi) \\cos\\theta_s \\dos \\\\\n    &= E_i f_r(p, \\os, \\oi) \\int^{\\Omega_s} \\cos\\theta_s \\dos \\\\\n    &= E_i f_r(p, \\os, \\oi) \\pi.\n\\end{aligned}\n\\tag{10.16}\\]\nThus: \\[\n    f_r(p, \\os, \\oi) = \\frac{\\rho_{hh}}{\\pi}.\n\\tag{10.17}\\]\nThe last step in Equation 10.16 uses the integral results that:\n\\[\n\\begin{aligned}\n    \\d\\omega &= \\sin\\theta\\d\\theta\\d\\phi, \\\\\n    \\int^{\\Omega=2\\pi} \\cos\\theta \\d\\omega &= \\int^{2\\pi}_0 \\int^{\\pi/2}_{0} \\cos\\theta \\sin\\theta\\d\\theta\\d\\phi \\nonumber\\\\\n    &= 2\\pi\\int^{\\pi/2}_{0} \\cos\\theta \\sin\\theta\\d\\theta \\nonumber \\\\\n    &= \\pi,\n\\end{aligned}\n\\]\nwhen \\(\\Omega\\) is the hemisphere.\nA fun exercise you can entertain yourself with is to show that if \\(f_r\\) is independent of \\(\\oi\\), it must also be independent of \\(\\os\\). An informal way to do so is the following. Since \\(f_r(p, \\os, \\oi)\\) is independent of \\(\\oi\\), let’s rewrite it as \\(g(p, \\os)\\). Now we invoke the reciprocity and rewrite \\(f_r(p, \\os, \\oi)\\) as \\(g(p, \\oi)\\). The only way for \\(g(p, \\os) = g(p, \\oi)\\) is for \\(g\\) to be dependent only on \\(p\\).\nFinally, one can also define the directional-directional reflectance, which is naturally a function of both the incident direction and outgoing direction and can be defined as the ratio between the incident irradiance and the outgoing irradiance when both the incident and outgoing solid angles approach 0.\nBRDF and directional-directional reflectance are both sensitive to both the incident and outgoing directions. But the former is a density measure, whereas the latter is a fraction/percentage measure (all other reflectance quantities are fraction measures too). Integrating BRDF over a finite set of directions gives us some measure reflectance. This is why the BRDF is defined as the radiance/ irradiance ratio rather than radiance/radiance or irradiance/irradiance ratio; it is to reflect the fact that the energy of a small cone of incident directions is distributed over all the directions over the hemisphere, and what we care to characterize is the distribution of the incident energy over all outgoing directions.",
    "crumbs": [
      "Rendering",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Rendering Surface Scattering</span>"
    ]
  },
  {
    "objectID": "rendering-re.html#sec-chpt-mat-ss-re",
    "href": "rendering-re.html#sec-chpt-mat-ss-re",
    "title": "10  Rendering Surface Scattering",
    "section": "10.3 The Rendering Equation",
    "text": "10.3 The Rendering Equation\nGiven the BRDF, we can estimate the outgoing radiance of a point given its illumination using the well-known Rendering Equation.\nThe setup is that we have a surface on which there is a point \\(p\\) that is receiving light from a solid angle \\(\\Omega\\). We are interested in calculating the exiting radiance leaving \\(p\\) toward an arbitrary direction \\(\\os\\). The rendering equation formulates this calculation by:\n\\[\n    L(p, \\os) = \\int^{\\Omega} f_r(p, \\os, \\oi) L(p, \\oi) \\cos\\theta_i \\text{d}\\oi,\n\\tag{10.18}\\]\nwhere \\(L(p, \\os)\\) is the outgoing radiance from \\(p\\) toward the direction \\(\\os\\); \\(\\Omega\\) is usually a hemisphere in surface scattering, since lights hitting a surface point can come from anywhere in the hemisphere, in which case Equation 10.18 is also called the reflection equation, indicating the fact that the equation governs surface reflection/scattering. The rendering equation was first introduced to computer graphics by Kajiya (1986) and Immel, Cohen, and Greenberg (1986) (albeit with slightly different formulations and the former being more general than the latter).\nThe rendering equation is exactly the same equation in Equation 10.3, so there is nothing more profound about the rendering equation than the definition of the BRDF: we are simply following the BRDF’s definition and turning the differential equation into an integral one. Intuitively, the way to understand this equation is that every ray that hits \\(p\\) makes some contribution toward the outgoing radiance \\(L(p, \\os)\\), and the integration just accumulates all the contributions. In particular:\n\n\\(L(p, \\oi) \\text{d}\\oi\\) is the incident irradiance of a differential solid angle \\(\\text{d}\\oi\\); note that the irradiance calculated here is defined with respect to a surface perpendicular to the direction of \\(\\oi\\).\n\\(L(p, \\oi) \\cos\\theta \\text{d}\\oi\\) applies the Lambert’s cosine law and calculates the irradiance at the surface where \\(p\\) lies.\n\\(f_r(p, \\os, \\oi)L(p, \\oi) \\cos\\theta \\text{d}\\oi\\) “transfers” the differential incident irradiance to the differential outgoing radiance toward \\(\\os\\) through the BRDF function.\nThe integration over all the incident directions calculates the total outgoing radiance given all the incident lights.\n\nThe rendering equation in theory allows us to calculate the entire light field, i.e., the radiance distribution in space, given an arbitrary \\(p\\) and \\(\\os\\). Why is knowing the light field important? Recall Equation 9.1: knowing the light field allows us to synthesize any image or calculate the color of any object from any perspective.\nIt is, of course, much easier said than done when it comes to solving the rendering equation, which itself is worth multiple chapters in a computer graphics textbook. We will not get into it here; let’s just consider the following challenges. First, the integrand in Equation 10.18 generally has no analytical form, so we will not be able to get an analytical solution to the integral equation. A common method is Monte-Carlo integration, which samples the integrand at different points and estimates the integral from the samples.\nSecond, in a realistic environment, we need to solve the rendering equation recursively. Note how the radiance function shows up on both sides of the equation. Put it in another way, when using Monte-Carlo integration to solve Equation 10.18 we need to sample the value of \\(L(p, \\oi)\\) for a specific \\(\\oi\\) — how? We evaluate Equation 10.18 again, but this time treating \\(\\oi\\) as the \\(\\os\\), which means we invoke Monte Carlo integration again. You can see how this can quickly blow up the computation: the number of rays whose radiances we need to calculate exponentially increases as long as we need to sample more than one ray at each point. A big chunk of physically-based graphics is devoted to addressing this issue; the most commonly used strategy is called path tracing, for which Pharr, Jakob, and Humphreys (2023, chap. 13) is a great reference.\nAnother way to think of this is that there are infinitely many paths through which light can propagate and be incident on a point. A global illumination method for rendering would attempt to track all these paths (e.g., through Monte Carlo methods). In contrast, a local illumination method is concerned with only a small subset of these paths, in which case we might be able to evaluate the rendering equation as a single-pass integration while avoiding recursion. For instance, we might consider lights only from direct light sources. We will see the counterpart of this exact situation in surface scattering/volume rendering in Chapter 13. For this reason, the rendering equation is sometimes called the light transport equation (LTE), because it in principle captures how light is transported in space3.\nAn interesting, and approximate, global illumination method that avoids path tracing is the idea of environment map (Ramamoorthi 2009, chap. 3). It assumes that the light sources are so distant from the objects in the scene that all points in the scene receive the same incident radiance distribution. That is, \\(L(p, \\oi)\\) in Equation 10.18 is a function of only \\(\\oi\\) but not \\(p\\). We can then pre-compute (through path tracing for instance) or directly measure \\(L(\\oi)\\) offline and store them in a data structure. For instance, we can use the equirectangular projection to store a discretized form of \\(L(\\oi)\\), or use spherical harmonics to (approximately) store a parameterized form of \\(L(\\oi)\\). Either way, the data structure that stores pre-computed \\(L(\\oi)\\) is called an environment map, which we can load at rendering time, plug it into the rendering equation, and calculate the outgoing radiance by simply evaluating the integral.\nFinally, we also need to somehow know the BRDF of the material. There are generally two methods of going about it. We can, of course, measure it, but we have no realistic way of measuring the complete BRDF for a material, because we would have to measure infinitely many points and, for each point, infinitely many incident and outgoing directions. We can only sample the BRDF using something called a goniospectroreflectometer or a goniospectrophotometer (Judd and Wyszecki 1975, p. 402–10), but there is still a massive amount of samples we need to take and to store. Lots of prior work goes into efficiently sampling, measuring, and deriving BRDFs (Marschner et al. 2000; Matusik 2003; Pharr, Jakob, and Humphreys 2023, chap. 9.8).\nAnother approach is to parameterize the BRDF so that we can evaluate the BRDF on demand rather than storing all the BRDF data, and this is what we will study next.\n\n\n\n\nImmel, David S, Michael F Cohen, and Donald P Greenberg. 1986. “A Radiosity Method for Non-Diffuse Environments.” Acm Siggraph Computer Graphics 20 (4): 133–42.\n\n\nJudd, Deane B, and Günter Wyszecki. 1975. Color in Business, Science, and Industry. 3rd ed. John Wiley & Sons.\n\n\nKajiya, James T. 1986. “The Rendering Equation.” In Proceedings of the 13th Annual Conference on Computer Graphics and Interactive Techniques, 143–50.\n\n\nMarschner, Stephen R, Stephen H Westin, Eric PF Lafortune, and Kenneth E Torrance. 2000. “Image-Based Bidirectional Reflectance Distribution Function Measurement.” Applied Optics 39 (16): 2592–2600.\n\n\nMatusik, Wojciech. 2003. “A Data-Driven Reflectance Model.” PhD thesis, Massachusetts Institute of Technology.\n\n\nNicodemus, FE, JC Richmond, JJ Hsia, IW Ginsberg, and T Limperis. 1977. Geometrical Considerations and Nomenclature for Reflectance. Vol. 160. US Department of Commerce, National Bureau of Standards Washington, DC, USA.\n\n\nPharr, Matt, Wenzel Jakob, and Greg Humphreys. 2023. Physically Based Rendering: From Theory to Implementation. 4th ed. MIT Press.\n\n\nRamamoorthi, Ravi. 2009. “Precomputation-Based Rendering.” Foundations and Trends in Computer Graphics and Vision 3 (4): 281–369.",
    "crumbs": [
      "Rendering",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Rendering Surface Scattering</span>"
    ]
  },
  {
    "objectID": "rendering-re.html#footnotes",
    "href": "rendering-re.html#footnotes",
    "title": "10  Rendering Surface Scattering",
    "section": "",
    "text": "To be more rigorous, the integration in Equation 10.4 evaluates to \\(E(p, \\Oi) + C\\), where \\(C\\) is a constant. Given the boundary condition that \\(L(p, \\os) = 0\\) when \\(E(p, \\Oi) = 0\\), we know \\(C=0\\), so \\(C\\) is omitted.↩︎\nFor instance, if \\(\\rho_{hd}(p, \\oi) = \\frac{1}{1+\\oi^2}\\), then \\(\\rho_{dh}(p, \\os)\\) must take the form \\(\\rho_{dh}(p, \\os) = \\frac{1}{1+\\os^2}\\)↩︎\nTo be exact, the LTE sometimes has an emission term at the right-hand side to denote the spontaneous emission from a surface point.↩︎",
    "crumbs": [
      "Rendering",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Rendering Surface Scattering</span>"
    ]
  },
  {
    "objectID": "rendering-surface.html",
    "href": "rendering-surface.html",
    "title": "11  Modeling Material Surface",
    "section": "",
    "text": "11.1 Types of Material Surface\nWe have established the rendering equation for modeling surface scattering. To apply this equation, we need the BRDF of the surfaces being rendered. In practice, the material BRDF is modeled in two ways: analytical parameterization (Section 11.1 and Section 11.2) and directly measurement (Section 11.3).\nIn everyday life, material surfaces are usually classified as being diffuse, specular, or glossy. Figure 11.1 shows examples of the three materials. We can now give a more rigorous treatment of these material types using BRDF, which will, in turn, give us some inspiration for parameterizing the BRDF.",
    "crumbs": [
      "Rendering",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Modeling Material Surface</span>"
    ]
  },
  {
    "objectID": "rendering-surface.html#sec-chpt-mat-ss-mat",
    "href": "rendering-surface.html#sec-chpt-mat-ss-mat",
    "title": "11  Modeling Material Surface",
    "section": "",
    "text": "Figure 11.1: (a): a diffuse material and its BRDF. (b): a specular material and its BRDF. (c): a glossy material and its BRDF. From Prabhu B Doss (2007), Daderot (2012), Steve Fareham (2007), VonHaarberg (2018c), VonHaarberg (2018a), and VonHaarberg (2018b).\n\n\n\n\n11.1.1 Diffuse Material\nWhen the surface is rough, the energy of surface reflection deviates away from the perfect mirror-like reflection and, instead, distributes across the hemisphere. When the surface becomes rough enough, the distribution of outgoing energy can become uniform across all outgoing directions over the entire hemisphere. Such a surface is called a diffuse or an ideal Lambertian surface. The perfect Lambertian surface does not exist, but many things in the real world come close, such as paper, marble, or wood.\nThe BRDF is a Lambertian surface is a uniform function. As we have seen in Equation 10.17, \\(f_r(p, \\os, \\oi) = \\frac{\\rho_{hh}}{\\pi}\\) when \\(\\rho_{hh}\\) is the surface albedo and is between 0 and 1. It is easy to see that diffuse materials are always isotropic.\n\n\n11.1.2 Perfectly Specular Material\nIf a surface is perfectly smooth, like a mirror, it is called a perfectly specular material. Such materials follow the Snell’s law, which governs the angles of reflection and refraction, and the Fresnel equations, which govern the energy of reflection and refraction.\nIn the plane of incidence (the plane uniquely determined by the incident direction and the surface normal), the reflection direction is symmetric about the surface normal as the incident direction. More precisely, if the incident direction is \\(\\oi\\) (parameterized by the polar angle \\(\\theta_i\\) and azimuthal angle \\(\\phi_i\\)) and the reflection direction is \\(\\os\\) (\\(\\theta_s, \\phi_s\\)), we have:\n\\[\n\\begin{aligned}\n    \\theta_s &= \\theta_i, \\\\\n    \\phi_s &= \\phi_i + \\pi.\n\\end{aligned}\n\\tag{11.1}\\]\nThe refraction/transmitted direction \\(\\omega_t\\) (\\(\\theta_t, \\phi_t\\)) follows:\n\\[\n\\begin{aligned}\n    & n_1 \\sin\\theta_i = n_2 \\sin\\theta_t, \\\\\n    & \\phi_t = \\phi_i + \\pi,\n\\end{aligned}\n\\tag{11.2}\\]\nwhere \\(n_1\\) is the refractive index of the medium where light comes from and \\(n_2\\) is that of the medium that reflects/refracts the lights.\nThe energy of the reflected and refracted light is governed by the Fresnel equations. We will spare you the details, but it suffices to say that the fractions of reflected/refracted light are dependent on the incident angle, refractive indices of the two interface media, and the polarization states of the light. If you work out the math and assume that the incident light is unpolarized, the percentage of reflected energy \\(F_r(\\oi)\\) for an incident direction \\(\\oi\\) is given by:\n\\[\n\\begin{aligned}\n    F_r(\\oi) &= \\frac{r_a+r_e}{2}, \\\\\n    r_a &= (\\frac{n_2 \\cos\\theta_i - n_1 \\cos\\theta_t}{n_2 \\cos\\theta_i + n_1 \\cos\\theta_t})^2, \\\\\n    r_e &= (\\frac{n_1 \\cos\\theta_i - n_2 \\cos\\theta_t}{n_1 \\cos\\theta_i + n_2 \\cos\\theta_t})^2.\n\\end{aligned}\n\\tag{11.3}\\]\nWe call \\(F_r(\\oi)\\) the specular reflectance, which not only varies with \\(\\oi\\) but also is also a spectral term; we omit the wavelength for simplicity. Assuming no loss of energy, the specular transmittance, i.e., the fraction of the transmitted energy, is given by \\(1-F_r\\).\nFresnel’s equations are best understood in the context of the electromagnetic theory and are derived by treating light as waves in an electric field (the fact that we need to consider polarization states of a light is a giveaway). While \\(F_r\\) cannot be derived from radiometry, it is fundamentally about the energy transfer of surface scattering, which radiometry is also concerned with. So \\(F_r\\) can be integrated into the radiometry framework. One good example is to express the BRDF of a specular material using \\(F_r\\):\n\\[\n    f_r(p, \\os, \\oi) = F_r(\\oi)\\frac{\\delta(\\theta_s-\\theta_i)\\delta(\\phi_s-\\phi_i-\\pi)}{\\cos\\theta_i},\n\\tag{11.4}\\]\nwhere \\(\\delta(x)\\) is the Dirac delta function, which is 0 everywhere except when \\(x=0\\) and has the property \\(\\int\\delta(x)\\d x = 1\\).\nWe can verify that this BRDF makes sense. First, the BRDF is non-zero only when Equation 11.1 holds because of the double-delta term. Second, the energy conservation is followed. For instance, if we calculate the directional-hemispherical reflectance by plugging the BRDF into Equation 10.8 and assuming \\(\\Omega\\) is a hemisphere, we get:\n\\[\n    \\frac{E_o}{E_i} = \\rho_{dh}(p, \\oi) = \\int^{\\Omega} F_r(\\oi)\\frac{\\delta(\\theta_s-\\theta_i)\\delta(\\phi_s-\\phi_i-\\pi)}{\\cos\\theta_i} \\cos\\theta_s\\text{d}\\os.\n\\tag{11.5}\\]\nSince \\(F_r(\\oi)\\) is independent of \\(\\os\\), Equation 11.5 evaluates to Equation 11.6. The integration in Equation 11.6 evaluates to 1. This is because, informally, the integrand is non-zero only when Equation 11.1 holds, at which point \\(\\theta_s = \\theta_i\\), so the cosine terms cancel out. So the integration is just sort of a hugely complicated way of writing \\(\\int \\delta(x)\\d x\\), which is 1.  \\[\n    \\frac{E_o}{E_i} = F_r(\\oi) \\int^{\\Omega} \\frac{\\delta(\\theta_s-\\theta_i)\\delta(\\phi_s-\\phi_i-\\pi)}{\\cos\\theta_i} \\cos\\theta_s\\text{d}\\os = F_r(\\oi).\n\\tag{11.6}\\]\n\nWe can see that the specular reflectance \\(F_r\\) is equivalent to \\(\\rho_{dh}\\), the directional-hemispherical reflectance. This makes sense, because in specular materials the scattering is directional if the incident light is directional. So the directional-hemispherical reflectance reduces to the “directional-directional” reflectance, which is essentially the specular reflectance.\nThe specular reflectance is also equivalent to the hemispherical-directional reflectance \\(\\rho_{hd}\\). We can show this either by simply invoking the reciprocity that \\(\\rho_{hd} = \\rho_{dh}\\) or by plugging the specular BRDF Equation 11.4 into Equation 10.9 and obtaining (assuming \\(\\Omega\\) is hemisphere):\n\\[\n\\begin{aligned}\n    \\rho_{hd}(p, \\os) &= \\int^{\\Omega} F_r(\\oi)\\frac{\\delta(\\theta_s-\\theta_i)\\delta(\\phi_s-\\phi_i-\\pi)}{\\cos\\theta_i} \\cos\\theta_i\\text{d}\\oi \\\\\n    &= F_r(\\hat\\os) = F_r(\\os),\n\\end{aligned}\n\\]\nwhere \\(\\hat\\os(\\theta_s, \\phi_s-\\pi)\\) is the mirror-reflection direction of \\(\\os(\\theta_s, \\phi_s)\\). The integral evaluates to \\(F_r(\\hat\\os)\\) because, informally, the integrand is non-zero only when Equation 11.1 holds, at which point \\(\\oi = \\hat\\os\\) so \\(F_r(\\oi) = F_r(\\hat\\os)\\); the integral is a complicated way of writing \\(\\int F_r(\\oi)\\delta(\\hat\\os-\\oi)\\doi\\), which evaluates to \\(F_r(\\hat\\os)\\). The result has an intuitive explanation: for a specular surface, the scattered energy along \\(\\os\\) given a hemispherical illumination is the same as when the illumination comes only from \\(\\hat\\os\\). We can then show that \\(F_r(\\hat\\os) = F_r(\\os)\\), which is not surprising given reciprocity; you can also verify it by going through the equations in Equation 11.3.\nInterestingly, the specular reflectance \\(F_r\\) in general is not equivalent to the hemispherical-hemispherical reflectance \\(\\rho_{hh}\\). To see this, plug the specular BRDF into Equation 10.10 (assuming \\(\\Omega_i\\) and \\(\\Omega_s\\) are hemispheres):\n\\[\n\\begin{aligned}\n    E_o &= \\int^{\\Omega_s} (\\int^{\\Omega_i} f_r(p, \\os, \\oi) L(p, \\oi) \\cos\\theta_i\\text{d}\\oi) \\cos\\theta_s \\dos, \\\\\n    &= \\int^{\\Omega_s} \\big(F_r(\\os) L(p, \\os)\\big) \\cos\\theta_s \\dos, \\\\\n    &= \\int^{\\Omega_i} F_r(\\oi) L(p, \\oi) \\cos\\theta_i \\doi , \\\\\n    E_i &= \\int^{\\Omega_i} L(p, \\oi) \\cos\\theta_i \\doi.\n\\end{aligned}\n\\]\nWe can see that only when \\(F_r(\\oi)\\) is a constant do we get \\(F_r(\\oi) = \\frac{E_o}{E_i} = \\rho_{hh}\\). This is consistent with our early result in Section 10.2.3 that \\(\\rho_{dh} = \\rho_{hh}\\) only when the material is Lambertian, and a specular material is obviously not Lambertian.\nWhen \\(F_r(\\oi)\\) is constant, the specular material is isotropic (can you prove it?). Since \\(F_r(\\oi)\\) does not have to be a constant, specular materials could be anisotropic. That is, it is theoretically possible that a material always reflects specularly, but the reflected energy depends on the incident direction. \n\n\n11.1.3 Glossy Material\nThe surface scattering in most materials is in-between being perfectly specular and perfectly diffuse. These materials scatter light to a small cone of directions, usually centered around the direction of a perfect reflection. These materials are usually called glossy or sometimes, confusingly, “specular”, too. The energy distribution of a glossy material is neither a Delta function (as in the perfectly specular case) nor a uniform function (as in the diffuse case). It is usually a function that peaks at the mirror-reflection direction and gradually decays as we move away from that direction.\nThe bottom figures in Figure 11.1 illustrate an example of the BRDF for each of the three surface types under a given incident direction. An actual BRDF (for a given surface point and a given incident direction) would be a 3D shape, and what we are showing here is the cross section. The shape of the locus is drawn to be proportional to the magnitude of the BRDF; the locus in graphics literature is sometimes called the specular lobe.\nThe spectral-lobe visualization gives us a hint: we can parameterize a BRDF by mathematically describing the shape of the specular lobe. In fact, the BRDFs for the Lambertian surface (Equation 10.17) and for the specular materials (Equation 11.4) are two such examples. A glossy BRDF is more difficult to parameterize. Many BRDF parameterizations have been proposed; some are empirical, while others attempt to be physically plausible. The most popular and widely used is based on the microfacet model, which we will discuss next.",
    "crumbs": [
      "Rendering",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Modeling Material Surface</span>"
    ]
  },
  {
    "objectID": "rendering-surface.html#sec-chpt-mat-ss-para",
    "href": "rendering-surface.html#sec-chpt-mat-ss-para",
    "title": "11  Modeling Material Surface",
    "section": "11.2 BRDF Parameterization with Microfacet Models",
    "text": "11.2 BRDF Parameterization with Microfacet Models\nThe assumption of the microfacet model is that the surface scattering behavior of a point depends on its local roughness: the rougher the surface, the more diffuse the surface scattering becomes. To model the roughness, the surface is modeled as a collection of small microfacets, each of which acts like a perfect mirror. A specular surface is one where all the microfacets have the exact same orientation. As the surface becomes rougher, the mirrors become more randomly oriented. When the mirrors are completely randomly oriented, the resulting surface scattering becomes diffuse.\nTo derive a microfacet model, we need to first define the orientation of each microfacet. Given a beam of incident lights from a particular direction, we can then trace, following the laws governing specular reflection, how the lights are scattered by the collection of the microfacets given their orientations. In the end, we obtain the collection of outgoing directions, from which we can derive the BRDF.\nThere are many variants of the microfacet model. They have one thing in common: they do not explicitly model the scattering of each ray at each microfacet but, rather, model the scattering of the microfacets statistically given the distribution of the microfacet orientations. In the end, they can either have an analytical form of the BRDF (Lambertian surface being an extreme example), have a close approximation of the analytical form, or can numerically estimate the BRDF efficiently (mostly through sampling).\nWithout going into the details, we will refer you to Pharr, Jakob, and Humphreys (2023, chap. 9.6) for a mathematical treatment of the general idea and to Torrance and Sparrow (1967), Cook and Torrance (1982), Ward (1992), Oren and Nayar (1995), and Walter et al. (2007) for the classical models.\n\n\n11.2.1 Nature of Microfacets Models\nIf the microfacet theory does not sound weird to you, it should! In a microfacet model, we are still modeling surface scattering using discrete objects (microfacets) and events (perfect mirror-like reflection on each microfacet). Is it surprising that we can use the discrete microfacet model to reason about the behavior of a continuous surface? Given any point \\(p\\) on a surface, wouldn’t \\(p\\) correspond to one single microfacet, and the behavior of \\(p\\) simply be the result of a perfect mirror reflection there? If so, how can the microfacet model describe non-specular surface scattering of glossy and diffuse materials?\nAn intermediate answer is that the microfacet theory is just a modeling methodology. We use a set of discrete microfacets to derive the surface-scattering statistics of that set of microfacets, but then simply assume that the so-derived statistics apply anywhere on a continuous surface of interest. Still, does this methodology reflect the physical reality?\n\n\n\n\n\n\nFigure 11.2: Triphasic profile of object property. Object property at both the macroscopic scale and at the atomic/molecular scale fluctuates wildly, but at there is a scale where the property does not very much. Models based on radiometry operate at this scale. This scale is sufficiently small (smaller than the spatial resolution of human vision and typical cameras) so our calculus machinery can be applied, but still larger than individual molecules and atoms so that we do not have to worry about the wild fluctuations at that scale.\n\n\n\nWell, the physical world is fundamentally not continuous; when we break down the surface into finer and finer scales, we eventually get to molecules and atoms, so the surface property undergoes wild fluctuations depending on whether a small area contains molecules or not. If that is the level of detail you want to get into, you have to model things at the molecular and atomic levels (or even lower). Figure 11.2 illustrates this idea.\nFortunately for many real-world use-cases, we do not have to go there. Our eyes have a resolution limit, so we cannot resolve the details of a tiny surface area anyway; image sensors also have a resolution limit. The just-resolvable area \\(\\delta A\\), set by the spatial resolution limit of our visual system, is more than large enough that it contains many microfacets, so the aggregated behavior of those microfacets can effectively model the observed scattering of \\(\\delta A\\), which is all that matters to our vision (and to computer graphics and imaging, which is concerned only with satisfying human vision). So effectively what the microfacet theory does is to assume that the small \\(\\delta A\\) (which contains a distribution of microfacets) is just within the range where the surface scattering property is stable. When the microfacet theory says something about a particular point \\(p\\), it is really saying something about \\(\\delta A\\).\nThis way of modeling and thinking is pervasive in radiometry, which uses differential and integral equations and thus has inherently assumed that the radiation field under modeling is continuous. That is not true. Take irradiance as an example. The average irradiance of a surface changes dramatically at the microscopic level when we initially reduce the surface area, because the photon distribution over a large area is likely very non-uniform. When the surface area is sufficiently small, the number of photons hitting the surface will change proportionally with the surface area, because at that scale the photon distribution is roughly uniform. This is the scale at which irradiance is defined. But if we keep reducing the area smaller and smaller, the amount of photons hitting a tiny area will, again, undergo wild fluctuations depending on whether there are photons in the area of not — photons are discrete packets of energy. We will see another example shortly in volume scattering, where we use a small volume of discrete particles to build a model for radiative energy transfer, which we then apply to any given point in a continuous volume.\n\nOrthogonal to the discussion above is the limitation that microfacet models do not account for the surface roughness on the scale of the light wavelength. In the regime where the length of each microfacet is comparable with the light wavelength, diffraction takes place. As a result, reflection does not follow the Snell’s law and is wavelength dependent. In fact, this is how we get iridescence; in engineering, people make diffraction gratings that take advantage of the wavelength dependency to disperse lights of different wavelengths.",
    "crumbs": [
      "Rendering",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Modeling Material Surface</span>"
    ]
  },
  {
    "objectID": "rendering-surface.html#sec-chpt-mat-measurement",
    "href": "rendering-surface.html#sec-chpt-mat-measurement",
    "title": "11  Modeling Material Surface",
    "section": "11.3 Measuring Spectral Reflectance and BRDF",
    "text": "11.3 Measuring Spectral Reflectance and BRDF\nThis section discuss the principles and practices of measuring the spectral reflectance or spectral BRDF. It is absolutely important to note that the measured reflectance is not necessarily attributed only to surface scattering, because the measurement setup does not care what the material being measured is. If SSS plays a role (e.g., translucent materials), the resulting reflectance data would include the contribution from volume scattering, too.\nWorse, for these materials not all the SSS influences are captured by this measurement geometry, since some back-scattered photons will exit at other surface points, which will not be captured by the detector. So the measurement is neither complete nor sound for materials where back-scattered photons contribute to their reflectance.\n\n11.3.1 Measuring Spectral Reflectance\nHow do we know the spectral reflectance (transmittance) of a material? We measure it. This is easier said than done. We will focus on the reflectance measurement here, but transmittance is measured similarly, except you are not measuring from the same side of the illuminant but from the other side. Sharma (2003, chap. 1.11.4), Trussell and Vrhel (2008, chap. 8.7), and Reinhard et al. (2008, chap 6.8) have overviews of various measurement devices that might be helpful.\n\nThe Importance of Measurement Geometry\nConsider Figure 7.2 (a) again. The illuminant emits lights everywhere, but what matters is the light incident on the point \\(p\\) the viewer is currently gazing at; of course, the incident lights could come from everywhere else in the space, not just a particular illuminant. Similarly, \\(p\\) could potentially scatter lights everywhere over the hemisphere (through surface scattering and/or SSS), but it is the small beam of light that enters the viewer’s eye that matters. In order to measure the reflectance that is relevant to this particular illumination-viewing geometry, we need to 1) measure all the illuminating power that hits \\(p\\) and 2) measure the scattered light from \\(p\\) only along the viewing direction.\nYou can imagine that if we change the illumination to be, say, a diffuse lighting where there is an equal amount of light hitting \\(p\\) from all directions, the reflectance would be different, and it would be a perfectly relevant reflectance measure to report. If you have not, next time when you visit an art museum, pay attention to how the lighting system is carefully set up to bring out the best viewing experience (while also considering conservation); you ideally want the reflectance measurement of an artifact to simulate the viewing lighting.\n\n\nSingle Reflectance Measurement\n\n\n\n\n\n\nFigure 11.3: (a): Four different illumination-viewing geometries to measure the reflectance of a material; from Judd and Wyszecki (1975, fig. 2.11). (b): A spectrophotometer, which takes two spectroradiometric measurements of the standard material with a known reflectance and a test material to calculate the spectral reflectance of the test material; from Sharma (2003, fig. 1.33). (c): A spectroradiometer design, which measures the spectral power distribution of a light source (self-luminous or scattering) using a prism; from Judd and Wyszecki (1975, fig. 2.1). (d): Another way to implement the spectroradiometer that uses diffraction grating to disperse incident light; from Reinhard et al. (2008, fig. 6.22).\n\n\n\nIn general, there really is no single reflectance number we can associate with a material. There are two ways to approach this. A common approach is to set up the measurement geometry so that it is close to an actual viewing experience. Figure 11.3 (a) shows four common settings. Some might illuminate the material from 0\\(^{\\circ}\\) (assuming the direction of the surface normal has an angle of 0\\(^{\\circ}\\)) and then measure the scattered lights at 45\\(^{\\circ}\\); others can illuminate the material using diffuse illumination and measure the reflectance at 0\\(^{\\circ}\\) (Judd and Wyszecki 1975, p. 122–25; Reinhard et al. 2008, chap. 6.8.2; Li 2003, chap. 2.2.2).\n\nTo get a reflectance spectrum, we need to know the reflectance at each sampled wavelength. There are multiple ways to go about measuring the spectral information. For instance, we can place a monochromator or a set of optical filters between the illuminant and the material so that we can control the wavelength of the light that is incident on the material.\nAlternatively, we can change the detector to measure spectral information. We can use a dispersive medium such as a prism, shown in Figure 11.3 (c), or a diffraction grating, shown in Figure 11.3 (d), to separate the scattered light into different wavelengths and measure them individually. A detector that is capable of measuring the spectral radiometric quantities (e.g., the spectral power distribution) is called a spectroradiometer.\nThe raw detector readings of a spectroradiometer are usually not the absolute radiometric quantity of interest. The raw recording is, instead, roughly proportional to radiometric quantity up to a constant scaling factor \\(SSF(\\lambda)\\), which is usually called the detector’s spectral sensitivity funciton or the responsivity function, which we will study carefully in Section 16.5. \\(SSF(\\lambda)\\) can be calibrated offline, and that allows us to turn a detector’s raw recording into the corresponding absolute radiometric quantity.\nWe take a spectroradiometric measurement of the illumination hitting the material and that of the scattered light of interest; the ratio is the spectral reflectance \\(\\rho(\\lambda)\\):\n\\[\n    \\rho(\\lambda) = \\frac{\\Phi_s(\\lambda)SSF(\\lambda)}{\\Phi_i(\\lambda)SSF(\\lambda)} = \\frac{\\Phi_s(\\lambda)}{\\Phi_i(\\lambda)}.\n\\]\nWe can see that for reflectance measurement, the exact values of \\(SSF(\\lambda)\\) are immaterial. A curious question is that, while the detector can measure \\(\\Phi_s(\\lambda)\\), what measures \\(\\Phi_i(\\lambda)\\)? One strategy is to, offline, place the same detector where the material is and directly measure \\(\\Phi_i(\\lambda)\\) there.\nAnother, perhaps much more common and standard, way to measure spectral reflectance is to use something called a spectrophotometer. This method does not need to know \\(\\Phi_i(\\lambda)\\), but it requires a reference sample with a known spectral reflectance. This is shown in Figure 11.3 (b). It takes two spectroradiometric measurements under the identical illumination: one for the test material and the other for the standard/reference sample. The spectral reflectance of the test material \\(\\rho_t(\\lambda)\\) is given by:\n\\[\n    \\rho_t(\\lambda) = \\frac{m_t(\\lambda)}{m_s(\\lambda)}\\rho_s(\\lambda),\n\\]\nwhere \\(\\rho_s(\\lambda)\\) is the known spectral reflectance of the standard/reference sample, \\(m_s(\\lambda)\\) and \\(m_t(\\lambda)\\) refer to the raw detector readings of the standard and the test material at wavelength \\(\\lambda\\), respectively. We can see that the spectrum of the illumination does not matter1. Sometimes \\(\\frac{m_t(\\lambda)}{m_s(\\lambda)}\\) is called the spectral reflectance factor of the test material if the reference material is perfectly diffuse (Judd and Wyszecki 1975, p. 93).\nIn practice, the reference measurement can be done separately rather than simultaneously with the test material to reduce the device form factor, and the reference measurement data can be tabulated to save measurement time.\nOne note on terminology: while a spectroradiometer is used to measure the spectral radiometric quantities (e.g., spectral radiance), a spectrophotometer does not measure the spectral photometric quantities (e.g., spectral luminance); instead, it measures the spectral reflectance. This is standardized in American Society for Testing and Materials (ASTM) E284-13b (ASTM International 2013) (along with other terminologies related to material properties and measurement instruments).\n\nThe nice thing about the approach described so far is that you get a single reflectance spectrum, but be very careful under what measurement geometry is the spectrum obtained. There is no guarantee that a particular measurement geometry corresponds to the illumination/observation geometry of an actual viewing experience, so use the reported reflectance data with that caveat in mind.\n\n\nGoniometric Measurements\nA more general approach is to measure the reflectance at every illumination-viewing direction combination. For that we need what is called a goniospectrophotometer2. There are also gonioradiometers, which measure the spectral radiometric quantities from different viewing directions.]. Figure 11.4 shows one such setup. The illuminant/light source incident on the material comes through the small aperture \\(I\\), and the scattered light from the material is captured by a detector (e.g., a photodiode or, essentially, a single-pixel image sensor) through another aperture \\(V\\). Transmittance can be similarly measured by placing the detector at the other side of the material.\n\n\n\n\n\n\nFigure 11.4: A setup for measuring goniometric reflectance and BRDF. Both the illuminant (source) and the detector (photometer) can vary in two degrees of freedom, \\((\\theta_i, \\phi_i)\\) for the source and \\((\\theta_s, \\phi_s)\\) for the detector, covering different illuminant-scattering combinations. Adapted from Judd and Wyszecki (1975, fig. 3.4).\n\n\n\nThe idea is to simultaneously sample, say, \\(N\\) illumination directions (parameterized by the azimuth \\(\\phi_i\\) and polar angle \\(\\theta_i\\)) and \\(M\\) scattering directions (parameterized by the azimuth \\(\\phi_s\\) and polar angle \\(\\theta_s\\)), and obtain \\(M \\times N\\) measurements, each of which corresponds to one particular combination of the illuminant and scattering directions. For convenience, commercial goniometric measurements usually use a beam splitter to simultaneously measure the illumination and scattering flux (Lanevski, Manoocheri, and Ikonen 2022; Rabal et al. 2012).\nDenote the area on the material being measured \\(A_r\\). The size of the area is dictated by the illumination aperture \\(I\\). Assuming the power received by \\(A_r\\) from the illuminant through \\(I\\) is \\(\\Phi_i(\\lambda, A_r, I)\\), and the power scattered by \\(A_r\\) and collected by the detector through the aperture \\(V\\) is \\(\\Phi_s(\\lambda, A_r, V)\\), the reflectance of the small area \\(A_r\\) is simply given by:\n\\[\n    \\rho(\\lambda, A_r) = \\frac{\\Phi_s(\\lambda, A_r, V)}{\\Phi_i(\\lambda, A_r, I)}.\n\\]\nAs the two apertures become very small, \\(A_r\\) becomes very small, and the incident and outgoing solid angles become very small, too. The resulting reflectance measurement can be thought of as estimating the directional-directional reflectance (Section 10.2). But in general you can see how the reflectance number can easily change when we slightly vary the hardware setup. For instance, if we increase the detector aperture \\(V\\), the detected power will increase, and that would increase the resulting reflectance. If we increase the illumination aperture \\(I\\), the resulting reflectance would be for a larger material area \\(A_r\\).\nOne can also use a reference material (with known reflectance spectra at the same measurement geometries) to avoid measuring \\(\\Phi_i(\\lambda, A_r, I)\\), similar to how a spectrophotometer is operated.\n\n\n\n11.3.2 Measuring BRDF\nReflectance is integrated from the BRDF, which suggests that the latter is a more fundamental measure of material property. The same setup shown in Figure 11.4 can also be used to measure the BRDF, in which case the setup is called a goniospectroreflectometer. We will take the same measurements, but with a bit more calculation we can estimate the BRDF of the material, rather than just the (goniometric) reflectance spectra.\nLet us be precise about the setup (omitting the \\(\\lambda\\) term in all relevant quantities).\n\nWe are illuminating a small area \\(A_r\\) through the illumination aperture \\(I\\).\nThe center of \\(A_r\\) is an infinitesimal point \\(p\\), which along with \\(I\\) subtends a solid angle \\(\\Oi(p, I)\\).\n\\(\\oi\\) is the direction between \\(p\\) and the center of \\(I\\).\n\\(A_r\\) scatters lights toward the detector through the detector aperture \\(V\\), which subtends a solid angle of \\(\\Os (p, V)\\) with \\(p\\).\n\\(\\os\\) is the direction between \\(p\\) and the center of \\(V\\).\nThe power incident on \\(A_r\\) is \\(\\Phi_i(A_r, I)\\), and the portion of the power scattered by \\(A_r\\) and collected by the detector is \\(\\Phi_s(A_r, V)\\).\nWe are interested in calculating the BRDF \\(f_r(p, \\omega_s, \\omega_i)\\).\n\nRecall that \\(f_r(p, \\omega_s, \\omega_i)\\) is defined as the ratio of the difference in radiance leaving \\(p\\) toward \\(\\os\\) over the difference in irradiance incident on \\(p\\) due to the lights coming from an infinitesimal solid angle \\(\\doi\\) (omitting \\(\\lambda\\) in all equations for simplicity):\n\\[\nf_r(p, \\omega_s, \\omega_i) = \\frac{\\text{d}L_s(p, \\omega_s)}{\\text{d}E_i(p, \\omega_i)} \\approx \\frac{L_s(p, \\omega_s)}{E_i(p, \\Oi(p, I))}.\n\\tag{11.7}\\]\nThere is no way we can illuminate a point \\(p\\) through an infinitesimal solid angle \\(\\doi\\); all we could do is to illuminate a small cone of directions \\(\\Oi(p, I)\\). We can then calculate the average BRDF of all the incident directions in \\(\\Oi(p, I)\\) (i.e., assuming the BRDF is the same for all the outgoing directions in \\(\\Oi(p, I)\\)) using the approximation in Equation 11.7, which we have derived in Section 10.1.1.\nHow do we calculate \\(E_i(p, \\Oi(p, I))\\)? There is no way we can illuminate and measure the irradiance of an infinitesimal point \\(p\\); all we can do is to illuminate a small area \\(A_r\\) and assume that the irradiance received is constant anywhere inside \\(A_r\\), so we have:\n\\[\n    E_i(p, \\Oi(p, I)) \\approx \\frac{\\Phi_i(A_r, I)}{A_r}.\n\\tag{11.8}\\]\nNow how do we get \\(L_s(p, \\omega_s)\\)? For this we turn to the detector side. Using basic radiometry, \\(\\Phi_s(A_r, V)\\) is expressed in Equation 11.9, where \\(p'\\) and \\(\\os'\\) are dummy variables, \\(\\theta_s'\\) is associated with \\(\\os'\\), and \\(\\Oi(p', V)\\) is associated with \\(p'\\) (c.f., \\(p\\) refers to a specific point on \\(A_r\\), and \\(\\os\\) and \\(\\Os(p, V)\\) refer to physical quantities associated specifically with \\(p\\)):\n\\[\n    \\Phi_s(A_r, V) = \\int^{A_r} \\int^{\\Os(p', V)} L_s(p', \\omega_s') \\cos{\\theta_s'} \\text{d}\\os' \\text{d}p'.\n\\tag{11.9}\\]\nWe assume that the radiance of any ray between \\(A_r\\) and the detector aperture \\(V\\) is constant and takes the value of \\(L_s(p, \\os)\\); this gets us Equation 11.10:\n\\[\n    \\Phi_s(A_r, V) \\approx \\int^{A_r} \\int^{\\Os(p, V)} L_s(p, \\omega_s) \\cos{\\theta_s} \\text{d}\\os' \\text{d}p'.\n\\tag{11.10}\\]\nSince \\(L_s(p, \\os)\\) and \\(\\cos\\theta_s\\) are invariant to \\(\\os'\\) and \\(p'\\), they can be taken out of the two integrations, and this gives us Equation 11.11: \\[\n    \\Phi_s(A_r, V) \\approx L_s(p, \\omega_s) \\cos{\\theta_s} \\int^{A_r} \\int^{\\Os(p, V)} \\text{d}\\os' \\text{d}p'.\n\\tag{11.11}\\]\nCalculating the two integrals in Equation 11.11 gives us Equation 11.12, where \\(C_1\\) and \\(C_2\\) are constant. Given the boundary condition that \\(\\Phi_s(\\cdot)\\) has to be 0 when \\(\\Os(\\cdot)\\) or \\(A_r\\) is 0 (if the detector aperture is closed or the illumination area vanishes, no scattered light will be detected), we know \\(C_1=C_2=0\\).\n\\[\n    \\Phi_s(A_r, V) \\approx L_s(p, \\omega_s) \\cos{\\theta_s} (A_r (\\Os(p, V) + C_1) + C_2).\n\\tag{11.12}\\]\nPlugging Equation 11.7 we get:\n\\[\n    \\Phi_s(A_r, V) \\approx f_r(p, \\omega_s, \\omega_i)E_i(p, \\Oi(p, I)) \\cos{\\theta_s} A_r \\Os(p, V).\n\\tag{11.13}\\]\nPlugging Equation 11.8 we get:\n\\[\n    \\Phi_s(A_r, V) \\approx f_r(p, \\omega_s, \\omega_i) \\frac{\\Phi_i(A_r, I)}{A_r} \\cos{\\theta_s} A_r \\Os(p, V).\n\\tag{11.14}\\]\nTherefore, the final BRDF is given by:\n\\[\n    f_r(p, \\omega_s, \\omega_i) = \\frac{\\Phi_s(A_r, V)}{\\Phi_i(A_r, I) \\cos{\\theta_s} \\Os(p, V)}.\n\\tag{11.15}\\]\nRearranging the terms, we get a seemingly more complex expression:\n\\[\n    f_r(p, \\omega_s, \\omega_i) = \\frac{[\\Phi_s(A_r, V)/(A_r \\cos\\theta_s)]/\\Os(p, V)}{\\Phi_i(A_r, I)/A_r}.\n\\tag{11.16}\\]\nEquation 11.16 actually gives a simple interpretation. The denominator is the average irradiance incident on \\(p\\) through a small solid angle \\(\\Oi(p, I)\\) (see Equation 11.8), and the numerator is the average radiance leaving \\(p\\)3. Taking the ratio of the two matches our intuition of the average BRDF: radiance over irradiance (received over a small solid angle).\nIf we assume the surface to be Lambertian, the BRDF is then \\(1/\\pi\\) for any \\(\\os\\) (under a given \\(p\\) and \\(\\oi\\); see Equation 10.17) assuming no loss of energy. This means:\n\\[\n    \\Phi_s(A_r, V) \\propto \\cos\\theta_s.\n\\]\nThat is, the flux reading weakens as the incident direction \\(\\theta\\) by a factor of \\(\\cos\\theta\\). Is this surprising? It should not be if you recall our discussion of radiant intensity (Equation 8.7). If we assume that every point on \\(A_r\\) emits the same amount flux to the same solid angle (through the aperture \\(V\\)), the radiant intensity of \\(p\\) toward \\(\\os\\) is to \\(\\frac{\\Phi_s(A_r, V)}{A_r \\Os(p, V)}\\) and, thus, proportional to \\(\\cos\\theta\\), which matches our earlier conclusion of how the radiant intensity of a Lambertian emitter/scatterer decays with \\(\\theta\\).\nAnytime you measure something, the measurement is subject to noise and uncertainty. For instance, in the case of gonioreflectometer measurement, the angular positioning of the illuminant and detector might not be accurate, the detector itself is subject to all sorts of measurement noise (which we will study in the image sensor lecture), and there might be stray lights that enter the detector. Quantifying the sources of uncertainty and, even better, correcting for them is an important part of reflectance/BRDF measurement (Lanevski, Manoocheri, and Ikonen 2022; Rabal et al. 2012).\n\n\n\n\nASTM International. 2013. “ASTM E284-13b Standard Terminology of Appearance.” https://webstore.ansi.org/standards/astm/astme28413b.\n\n\nCook, Robert L, and Kenneth E. Torrance. 1982. “A Reflectance Model for Computer Graphics.” ACM Transactions on Graphics (ToG) 1 (1): 7–24.\n\n\nDaderot. 2012. “Kongens Have, Copenhagen, Denmark; CC0 1.0 license.” https://commons.wikimedia.org/wiki/File:Marble_ball_-_Kongens_Have_-_Copenhagen_-_DSC07898.JPG.\n\n\nJudd, Deane B, and Günter Wyszecki. 1975. Color in Business, Science, and Industry. 3rd ed. John Wiley & Sons.\n\n\nLanevski, Dmitri, Farshid Manoocheri, and Erkki Ikonen. 2022. “Gonioreflectometer for Measuring 3D Spectral BRDF of Horizontally Aligned Samples with Traceability to SI.” Metrologia 59 (2): 025006.\n\n\nLi, Yang. 2003. “Ink-Paper Interaction-a Study in Ink-Jet Color Reproduction.” Institute of Technology-Linköpings University. Norrköping, Sweden: UniTryck.\n\n\nOren, Michael, and Shree K Nayar. 1995. “Generalization of the Lambertian Model and Implications for Machine Vision.” International Journal of Computer Vision 14:227–51.\n\n\nPharr, Matt, Wenzel Jakob, and Greg Humphreys. 2023. Physically Based Rendering: From Theory to Implementation. 4th ed. MIT Press.\n\n\nPrabhu B Doss. 2007. “Tso Kiagar Lake Ladakh; CC BY 2.0 license.” https://commons.wikimedia.org/wiki/File:Tso_Kiagar_Lake_Ladakh.jpg.\n\n\nRabal, Ana M, Alejandro Ferrero, Joaquın Campos, José Luis Fontecha, Alicia Pons, Antonio Manuel Rubiño, and Antonio Corróns. 2012. “Automatic Gonio-Spectrophotometer for the Absolute Measurement of the Spectral BRDF at in-and Out-of-Plane and Retroreflection Geometries.” Metrologia 49 (3): 213.\n\n\nReinhard, Erik, Erum Arif Khan, Ahmet Oguz Akyuz, and Garrett Johnson. 2008. Color Imaging: Fundamentals and Applications. CRC Press.\n\n\nSharma, Gaurav. 2003. “Color Fundamentals for Digital Imaging.” In Digital Color Imaging Handbook, edited by Gaurav Sharma, 14–127. CRC Press.\n\n\nSteve Fareham. 2007. “Heart of the City water feature Sheffield; CC BY-SA 2.0 license.” https://commons.wikimedia.org/wiki/File:Heart_of_the_City_water_feature_Sheffield_-_geograph.org.uk_-_618552.jpg.\n\n\nTorrance, Kenneth E, and Ephraim M Sparrow. 1967. “Theory for Off-Specular Reflection from Roughened Surfaces.” Josa 57 (9): 1105–14.\n\n\nTrussell, H Joel, and Michael J Vrhel. 2008. Fundamentals of Digital Imaging. Cambridge University Press.\n\n\nVonHaarberg. 2018a. “Illustration of a diffuse BRDF; CC0 1.0 license.” https://commons.wikimedia.org/wiki/File:BRDF_diffuse.svg.\n\n\n———. 2018b. “Illustration of a glossy BRDF; CC0 1.0 license.” https://commons.wikimedia.org/wiki/File:BRDF_glossy.svg.\n\n\n———. 2018c. “Illustration of a mirror BRDF; CC0 1.0 license.” https://commons.wikimedia.org/wiki/File:BRDF_mirror.svg.\n\n\nWalter, Bruce, Stephen R Marschner, Hongsong Li, and Kenneth E Torrance. 2007. “Microfacet Models for Refraction Through Rough Surfaces.” Rendering Techniques 2007:18th.\n\n\nWard, Gregory J. 1992. “Measuring and Modeling Anisotropic Reflection.” In Proceedings of the 19th Annual Conference on Computer Graphics and Interactive Techniques, 265–72.",
    "crumbs": [
      "Rendering",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Modeling Material Surface</span>"
    ]
  },
  {
    "objectID": "rendering-surface.html#footnotes",
    "href": "rendering-surface.html#footnotes",
    "title": "11  Modeling Material Surface",
    "section": "",
    "text": "An alternative, and mathematically equivalent, method is that we measure 1) the spectrum of the illumination (e.g., using a spectroradiometer) and 2) the camera SSF; then given the detector reading we can calculate the spectral reflectance (raw reading = illumination \\(\\times\\) reflectance \\(\\times\\) SSF). While we do not have to measure the reflectance of the reference sample, this method comes with the extra work of measuring the illumination and calibrating the camera SSF, so it is less preferred.↩︎\n“gonio-”” comes from the Greek word \\(\\gamma\\omega\\nu\\iota\\alpha\\) (g={o}n'{i}a), which means angle.↩︎\n\\(\\Phi_s(A_r, V)/(A_r \\cos\\theta_s)\\) in the numerator gives us the average irradiance leaving \\(p\\) (note that this radiance is defined at the surface perpendicular to \\(A_r\\), hence the \\(\\cos\\theta_s\\) term), which is divided by \\(\\Os(p, V)\\) to give us the average radiance leaving \\(p\\).↩︎",
    "crumbs": [
      "Rendering",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Modeling Material Surface</span>"
    ]
  },
  {
    "objectID": "rendering-sss.html",
    "href": "rendering-sss.html",
    "title": "12  Volume and Subsurface Scattering Processes",
    "section": "",
    "text": "12.1 Absorption\nNow we turn our attention to subsurface scattering and volume scattering. While superficially different, they involve the same forms of light-matter interaction and are modeled in the same way. This chapter is concerned with local events — absorbing (Section 12.1) and scattering (Section 12.2) photons by particles at a particular point in a medium. Understanding the local behaviors will then allow us to build a general framework in the next chapter to reason about subsurface and volume scattering globally.\nWe start by modeling absorption in this chapter, and the way we build the models is fundamental to how scattering will be dealt with later.",
    "crumbs": [
      "Rendering",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Volume and Subsurface Scattering Processes</span>"
    ]
  },
  {
    "objectID": "rendering-sss.html#sec-chpt-mat-vs-abs",
    "href": "rendering-sss.html#sec-chpt-mat-vs-abs",
    "title": "12  Volume and Subsurface Scattering Processes",
    "section": "",
    "text": "12.1.1 A Simple Case: Collimated Illumination on Uniform Medium\nImagine that a beam of light hits a volume of particles. The light is collimated in that all photons travel along the same direction. We take a slice of the material perpendicular to the incident direction. The slice is so thin that no particles in that material cover each other from the direction of the incident light. This is shown in Figure 12.1 (a). We also, for now, assume that the medium is uniform in that the number concentration \\(c\\) (i.e., the number of particles per unit volume) of each slice is exactly the same.\n\n\n\n\n\n\nFigure 12.1: Conceptual model to help reason about photon absorption. (a): the setting for calculating the radiance reduction over a very thin slice. (b): the radiance reduction over a finite length is calculated by accumulating the radiance reduction over infinitely many thin slices.\n\n\n\nSay the slice has a depth of \\(\\D s\\) and a geometrical cross-sectional area of \\(E\\). All the particles have the same geometrical cross-sectional area of \\(\\epsilon_g\\). In the simplest model, a photon is absorbed whenever it hits a particle. In reality, the chance of absorption can be higher or lower. The effective area available for absorption is:\n\\[\n    \\epsilon = \\epsilon_g Q_a,\n\\]\nwhere \\(Q_a\\) is called the absorption efficiency and is usually smaller than 1 for molecules (which have small \\(\\epsilon_g\\)) and greater than 1 for large particles (whose \\(\\epsilon_g\\) can be large). \\(Q_a\\) is wavelength dependent, so we should have written it as \\(Q_a(\\lambda)\\), but we will omit the wavelength in our notations for simplicity’s sake. In physics, \\(\\epsilon\\) is called the absorption cross section of the particle; it characterizes the intrinsic capability of a particle to absorb photons. Mind the subtle but important difference between the geometrical cross-sectional area and the cross section of a particle.\nThe question we are interested in is, if the incident radiance is \\(L\\), what is the radiance leaving the slice \\(L+\\D L\\)? By convention, \\(\\D L\\) is defined as the exitant radiance minus the incident radiance and, in this case, has to be negative. The percentage of photons that are absorbed by this slice of particles (\\(-\\frac{\\D L}{L}\\)) is equivalent to the cross-sectional area of the slice that is covered by the total cross sections of the particles:\n\\[\n    -\\frac{\\D L}{L} = \\frac{cE\\D s\\epsilon}{E},\n\\tag{12.1}\\]\nwhere \\(c\\) is the particle concentration of the slice, and \\(E\\D s\\) is the total volume of the slice. So \\(cE\\D s\\) is the number of particles in this thin slice, and \\(cE\\D s\\epsilon\\) is the total cross section of all the particles. Given the assumption that no particles are covering each other, \\(\\frac{cE\\D l\\epsilon}{E}\\) is then the percentage of the thin slice’s cross-sectional area that is available for photon absorption and, thus, the percentage of the incident photons that are absorbed. The negative sign on the left-hand side of Equation 12.1 signals the fact that \\(\\D L\\) is negative.\nWe rewrite Equation 12.1 as Equation 12.2: \\[\n    \\frac{\\D L}{\\D s} = -c\\epsilon L = -\\sigma_a L,\n\\tag{12.2}\\]\nwhich shows that the amount of photon absorption per unit length (\\(\\frac{\\D L}{\\D s}\\)) is proportional to the current amount of photons up to a scaling factor \\(c\\epsilon\\). In the computer graphics literature, \\(c\\epsilon\\) is called the absorption coefficient, denoted \\(\\sigma_a\\).\n\nBouguer-Beer-Lambert’s Law\nWhen \\(\\D s\\) approaches infinity, we can rewrite Equation 12.2 as a differential equation:\n\\[\n    \\frac{\\d L}{\\d s} = \\lim_{\\D s \\rightarrow 0}\\frac{\\D L}{\\D s} = -\\sigma_a L\n\\tag{12.3}\\]\nThis equation is a classic case of exponential decay, and its solution is given by:\n\\[\n    L(s) = L_0 e^{-\\sigma_a s}.\n\\tag{12.4}\\]\nwhere \\(L_0 = L(0)\\) is the initial radiance of the light before interacting with the particles, as visualized in Figure 12.1 (b), \\(L(s)\\) denotes the radiance at a particular length \\(s\\).\nEquation 12.4 allows us to calculate the remaining radiance after the light travels a length \\(s\\). Equation 12.4 is called the Bouguer-Beer-Lambert’s law (BBL), which is a geometrical optics’ simplification of the electromagnetic theory of light-matter interaction where the matter is purely absorptive (Mayerhöfer, Pahlow, and Popp 2020).\n\n\nAn Alternative Derivation\nAn equivalent way of deriving the BBL law is the following. We divide the entire volume (with a total length of \\(s\\)) into \\(N\\) thin slices, each with a length of \\(\\D s\\). After the first slice, the surviving portion of the initial radiance is \\(L = L_0(1-\\sigma_a \\D s)\\), so after going through all the \\(N\\) slices, the remaining radiance is given by:\n\\[\n    L_N = L_0(1-\\sigma_a \\D s)^N = L_0(1-\\sigma_a \\frac{s}{N})^N.\n\\tag{12.5}\\]\nNow when \\(\\D s\\) becomes infinitesimally small, \\(N\\) approaches infinity, so the limit of the remaining radiance as a function of the total length \\(s\\) is given in Equation 12.6, which is the same as Equation 12.4.\n\\[\n    L(s) = \\lim_{N \\rightarrow \\infty}L_0(1-\\sigma_a \\frac{s}{N})^N = L_0 e^{-\\sigma_a s}.\n\\tag{12.6}\\]\n\n\n\n12.1.2 Absorption Coefficient\nThe absorption coefficient is an important measure of the medium’s ability to absorb photons. It has a unit of \\(\\text{m}^\\text{-1}\\), which means it is not bound by 0 and 1. One way to interpret the absorption coefficient is to observe that \\(\\sigma_a \\d s = \\d L/L\\), which is the fraction of the radiance absorbed or the probability of light absorption by an infinitesimal slice. So \\(\\sigma_a = (\\d L/L)/\\d s\\) can be interpreted as the probability density of photon absorption, i.e., the probability of absorption per unit length traveled:\n\\[\n    \\sigma_a = \\lim_{\\D s \\rightarrow 0} \\frac{\\D L}{L}/\\D s = \\frac{\\d L}{L \\d s}.\n\\]\nLike any density measure, absorption coefficient is most useful when it is integrated: when we integrate \\(\\sigma_a\\) over the length that light travels, we get the fraction/percentage of the light absorbed. One can also show that \\(1/\\sigma_a\\) is the expected value of the distance a photon can travel before being absorbed (Bohren and Clothiaux 2006, chap. 5.1.3); this quantity is given the name mean free path (l). To derive \\(l\\), observe that the probability that a photon is absorbed after traveling a distance \\(s\\) is \\(1-e^{-\\sigma_as}\\). So the probability density of absorption as a function of the distance \\(s\\) is:\n\\[\n    f(s) = \\frac{\\text{d}(1-e^{-\\sigma_a s})}{\\text{d}s} = \\sigma_ae^{-\\sigma_a s}.\n\\]\nSo the expected value of \\(s\\), which we can interpret as the distance a photon can travel on average before being absorbed, is:\n\\[\n    l = \\int_0^\\infty sf(s)\\text{d}s = 1/\\sigma_a.\n\\tag{12.7}\\]\n\n\n12.1.3 A Few Important Quantities\nWe can now define a few other commonly used quantities (omitting the wavelength dependence for simplicity). The transmittance \\(T\\) of a volume with a total thickness of \\(s\\) is defined as the percentage of the transmitted/unabsorbed photons after traveling the length of \\(s\\) (Equation 12.8): \\[\n    T = \\frac{L(s)}{L_0} = e^{-\\sigma_a s}.\n\\tag{12.8}\\]\nThe absorbance \\(A\\) is the product of \\(\\sigma_a s\\): \\[\n    A = -\\ln T = \\ln\\frac{L(s)}{L_0} = \\sigma_a s.\n\\tag{12.9}\\]\nThe absorptance \\(a\\) of a volume is defined as the percentage of the absorbed photons by the volume, which relates to \\(T\\) and \\(A\\) by Equation 12.9: \\[\n    a = 1 - T = 1 - e^{-A}.\n\\tag{12.10}\\]\nWe have seen these definitions in Section 3.2.1. One very nice thing about the absorbance \\(A\\) is that it is approximately equivalent to absorptance \\(a\\) when \\(A\\) is small (which would be true when, e.g., the length \\(s\\) is very small, as is the case when discussing how a photoreceptor absorbs photons when illuminated transversely).\nAnother nice thing about absorbance is that absorbances add, because absorption coefficients add. Imagine you have \\(n\\) kinds of particles mixed up in a medium, each with a different absorption coefficient \\(\\sigma_a^i\\). The overall absorbance of the medium is the sum of the individual absorbance \\(A^i\\) derived as if the medium is made up of only one kind of particles. That is: \\[\n    A = \\sum_i^n A^i = s\\sum_i^n\\sigma_a^i = s\\sum_i^n c^i\\epsilon^i,\n\\tag{12.11}\\]\nwhere \\(c^i\\) and \\(\\epsilon^i\\) are the concentration and absorption cross section of the \\(i^{th}\\) particles. Specifically, \\(c^i\\) is defined as: \\[\n    c^i = \\frac{n_i}{V},\n\\]\nwhere \\(n_i\\) is the number of the \\(i^{th}\\) kind of particles in the material, and \\(V\\) is the material volume.\nThis is not a surprising result. As long as particles in a thin slice of this new heterogeneous medium do not cover each other, we can easily extend Equation 12.1 and the rest of the derivation to consider multiple kinds of particles; eventually Equation 12.11 would be a natural conclusion. We will omit the derivation here for simplicity sake.\nEquation 12.11 is a nice conclusion to have, because usually we are dealing with hybrid media. For instance, paint is a mixture of binder particles and pigment particles, and a mist is a mixture of water droplets and air particles.  If we do not want to model individual matters, we can use a single absorption coefficient to describe the aggregate behavior of the mixture. That absorption coefficient does have a physical meaning: it is the concentration-weighted sum of the individual absorption coefficients.\nThere are a bunch of other quantities defined in the literature. The state of the definitions is a bit of a mess, largely because different communities use different definitions.\n\nIn visual neuroscience people sometimes use a quantity called specific absorbance (see, e.g., Bowmaker and Dartnall (1980)), which is the absorbance per unit length \\(\\frac{A}{s}\\). Whenever you see a quantity that starts with the word “specific”, chances are that the quantity is defined per unit length. You can see that specific absorbance is actually just our absorption coefficient.\nIn scientific communities, especially chemistry and spectroscopy, people define \\(\\epsilon\\), rather than \\(c\\epsilon\\), to be the absorption coefficient. You can see the appeal of doing that — \\(\\epsilon\\) is a more fundamental measure of a medium’s ability to absorb photons, independent of the particle concentration \\(c\\) (and certainly independent of the traversal length \\(s\\)).\nThe absorbance defined in Equation 12.9 is technically called the Naperian absorbance, because we take the natural logarithm of \\(T\\). Sometimes people also use the decadic absorbance, which is defined as \\(-\\log T\\). This quantity is also called the optical density.\nFinally, the number concentration \\(c\\) here is defined in terms of the absolute quantity per unit volume, but sometimes people want to define \\(c\\) as the molar concentration, which is the number of moles per unit volume. If so, all other derived quantities are then prefixed with “molar”. Next time when you see something like the molar decadic absorption coefficient, you know what it is!\n\nThe annoying thing is that people do not always tell you which definition they use. The plea I have to you is to be specific about which definition you use in your writing and tell me when I am being vague!\n\n\n12.1.4 General Case\nSo far we have assumed that the absorption coefficient \\(\\sigma_a = c\\epsilon\\) is a constant regardless of the position \\(p\\) in the medium and along any direction \\(\\omega\\). The former property assumes that the medium is uniform, and the latter property is called isotropic1 in that the medium’s ability to absorb photons is independent of the light direction.\nBoth assumptions are problematic in practice. The concentration can change spatially and should be denoted \\(c(p)\\), where \\(p\\) is an arbitrary position in space. \\(\\epsilon\\) can also change with \\(p\\) and, more importantly, change with the direction of light incidence \\(\\omega\\). For instance, the particles might not be spherical, so their geometrical cross-sectional area and, thus, the cross section \\(\\epsilon\\) available for absorbing photons can depend on \\(\\omega\\). As a result, the absorption coefficient should generally be denoted \\(\\sigma_a(p, \\omega)\\).\n\n\n\n\n\n\nFigure 12.2: A conceptual model to help reason about photon absorption in the general case, where the absorption coefficient can vary spatially and directionally. The medium is divided into many tiny elemental volumes, each of which is so small that particles do not cover each from any direction.\n\n\n\nEffectively, our conceptual model, shown in Figure 12.2, has to be changed to one where the entire body of particles is divided into many equally-sized volumes (with a length \\(\\D s\\) and an area \\(\\D A\\)), each of which is so small that particles do not cover each other from any direction. The radiance reduction per unit length in a small volume is then expressed as:\n\\[\n    \\frac{\\D L(p, \\omega)}{\\D s} = -\\sigma_a(p, \\omega)L.\n\\tag{12.12}\\]\nGiven this model, we can calculate the exitant radiance after light travels a length \\(s\\) through the medium:\n\\[\n    L(p+s\\omega, \\omega) = L(p, \\omega) e^{-\\int_0^s \\sigma_a(p+t\\omega, \\omega) \\d t},\n\\tag{12.13}\\]\nwhere \\(\\omega\\) is the (unit) direction of the incident radiance, \\(L(p, \\omega)\\) is the incident radiance, and \\(L(p+s\\omega, \\omega)\\) is the exitance radiance (radiance toward \\(\\omega\\) leaving the entire medium after traveling \\(s\\)).\nYou would notice that for a beam with an oblique incident direction, the distance traveled, say \\(\\D s'\\), can be different (longer or shorter than) from \\(\\D s\\). Our model can account for this by folding the factor \\(\\D s'/\\D s\\) specific to a particular direction \\(\\omega'\\) into the absorption coefficient \\(\\sigma_a(p, \\omega')\\). Note that the \\(\\D s'/\\D s\\) factor should be the average for all the incident photons with the same direction \\(\\omega'\\) across the entire \\(\\D A\\).\n\n\n12.1.5 Nature and Applicability of the Model\nThe absorption model (the BBL law) derived before (Equation 12.4 and Equation 12.13) is a continuous one, but it is derived based on modeling discrete particles and events. It is another example of the modeling methodology discussed on Section 11.2.1.\nEquation 12.13 seems to suggest that absorption coefficient \\(\\sigma_a(p, \\omega)\\) is continuously defined at any position \\(p\\) in the medium along any direction \\(\\omega\\). It is not true. For starters, concentration \\(c\\) is not continuous. Rather, it exhibits the triphasic profile shown in Figure 11.2. As we keep shrinking the size of the volume to the molecular scale, eventually the concentration depends on whether the tiny volume contains any molecules or not, so it becomes wildly discontinuous, not to mention the headache of dealing with a partial molecule in a volume — should it be counted or not? In general, the absorption coefficient can be an arbitrary discontinuous function that is not integrable.\nWhat about Equation 12.4 where the absorption coefficient is uniform so we do not have to take the integral? Well, that is a lie too: concentration is not continuous, so it cannot be uniform everywhere, and, by extension, the absorption coefficient cannot be a constant everywhere either. So Equation 12.3 is technically wrong when we let \\(\\D s \\rightarrow 0\\) (i.e., \\(N \\rightarrow \\infty\\)), which is necessary for us to construct the differential equation (or take the limit in Equation 12.6). For Equation 12.3 to be true, the concentration/absorption coefficient must be a constant everywhere, which can be true only if the volume is continuous.\nWhat has to happen is that the limit of \\(\\D s\\) cannot be literally 0 and the limit of \\(N\\) cannot be infinity. What we do is to keep reducing \\(\\D s\\) to the point where the concentration (and thus absorption coefficient) is insensitive to slight perturbation of \\(\\D s\\) (i.e., operating in the stable range in Figure 11.2), and call it the concentration/absorption coefficient of that specific \\(\\D s\\). And we repeat this for all the \\(\\D s\\). This certainly applies to the general-case models in Equation 12.12 and Equation 12.13, where we iterate over not the thin slices \\(\\D s\\) but all the tiny volumes (\\(\\D A \\times \\D s\\)). So all the integral symbols are secretly summing over an extremely fine-grained grid.\nHow big of an error are we introducing here? Technically, we should sum all \\(N\\) slices across the total traversal length \\(s\\) in Equation 12.5. If we assume \\(\\D s\\) to be very small (even though not infinitesimal) compared to \\(s\\), \\(N\\) would be large, so taking the integration (equivalent to letting \\(N \\rightarrow \\infty\\)) would be very close to summing over \\(N\\). Similarly, the integral in Equation 12.13 should have been a summation of the concentration in each of the \\(N\\) slices. If you want to be pedantic, however, the integration there is exact: we can model \\(c\\) as a piece-wise function, where the value at each piece is the concentration of the corresponding volume. Integrating over a piece-wise function is the same as summing all the pieces. Only the exponential expression in Equation 12.13 is inexact.\nThe discontinuity of the medium is, of course, orthogonal to the discontinuity and non-uniformity in the light field itself. For instance, the fact that we use \\(\\frac{cE\\D l\\epsilon}{E}\\) as the percentage of photon absorption in Equation 12.1 (and implicitly in Equation 12.4) assumes that the irradiance of the incident illumination is continuous and uniform in the small volume. This is technically not true because photons are discrete packets of energy. But in practice this is not a concern because we can assume that there is an enormous amount of photons incident on the small volume, and these photons are randomly distributed. \nIn essence, we are using the aggregated behavior of many photons to model the behavior of a small volume. This is similar to the microfacet models, where we use the aggregated behavior of many microfacets to statistically model the behavior of a small macro-surface.\nThis sort of modeling strategy is a weird case where the discrete model provides the “ground truth”, which is approximated by a continuous model. I say ground truth — to the extent that the geometrical optics can approximate the electromagnetic theory of light-matter interaction. The BBL law fails when the wave nature of photons has to be considered (Mayerhöfer, Pahlow, and Popp 2020).",
    "crumbs": [
      "Rendering",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Volume and Subsurface Scattering Processes</span>"
    ]
  },
  {
    "objectID": "rendering-sss.html#sec-chpt-mat-vs-sca",
    "href": "rendering-sss.html#sec-chpt-mat-vs-sca",
    "title": "12  Volume and Subsurface Scattering Processes",
    "section": "12.2 Scattering",
    "text": "12.2 Scattering\nScattering is much more difficult to reason about than absorption, primarily because a scattered photon is not “dead” and continues to participate in light-matter interaction. The way to study scattering is to first understand the behavior of a single scattering event and then consider the overall behavior of a large of collection of particles.\nThis section focuses a single scattering event (Section 12.2.2), and the next section discusses the general case where a large collection of particles interacts with photons. Before all these, though, it is useful to first build some intuitions as to why there is a distinction between a single scattering event and scattering by a particle collection and explicitly lay out the assumptions made for the rest of our discussions (Section 12.2.1).\n\n12.2.1 Scattering by a Particle vs. a Collection of Particles\nIn geometric optics terms, scattering can be thought of as an event that takes place between a photon and a particle. In the real world, however, objects and media are usually made of a large collection of particles, which introduces two complications: multiple scattering and interference.\n\nMultiple Scattering\nFirst, it is possible that a scattered photon, after traveling a certain distance, meets another particle and gets scattered again. This makes it considerably more difficult to analyze the effect of scattering by a medium than does the scattering of a single particle.\n\n\n\n\n\n\nFigure 12.3: Left: The atmospheric scattering of the sand coming from the Sahara during Harmattan glows in the sun and gives a hazy view of the remote mountains. Nigeria’s National Mosque is in the foreground; from Kipp Jones (2005). Right: illustrations of the glow of the sun and the haze. Both are due to scattering, and the difference is purely visual but not fundamental.\n\n\n\nLook at Figure 12.3 (left) taken during Harmattan, where the atmosphere is full of sand and dust blown from the Sahara. The large collection of particles in the atmosphere scatters light, glowing the sun and giving the remote mountains a hazy view. The right panel illustrates the scattering events that give rise to the glow and the haze.\nWithout scattering, sunlight enters the eye directly. With scattering, some photons from the sun are first knocked out of the view and could potentially be then scattered again back to the eye. Some photons that enter the eye might even come from nearby objects other than the sun. The scattering creates a glow around the sun, and, for the observer, the sun appears larger than it actually is. The hazy view of the mountains is created by exactly the same scattering processes. The photons that enter the eyes are mixed up from different parts of the mountains and from other objects. The mountains appear hazy rather than glowing as the sun does simply because the sun has a higher brightness contrast against the background than does a region on the mountain. So the distinction between “glow” and “haze” is nothing more than a visual difference at a superficial level rather than anything deeper in physics.\nYou can see why multiple scattering by large collections of particles poses challenges to our analysis. If a photon is scattered once in the medium, the only effect of scattering would be to knock photons out of our line of sight, and thus, remote objects would only look dimmer rather than hazy. In this case, scattering would function exactly like absorption, for modeling purposes at least. With multiple scattering, we have to track not only photons that are scattered out but also photons that are scattered into the rays that enter our eyes2. This is a daunting task considering that we are usually dealing with millions of particles and billions of photons, if not more.\nIf one were to be absolutely pedantic, we can distinguish the following cases:\n\na single scattering event, where a photon meets a particle and is scattered away;\nsingle scattering, where a photon is scattered once by a medium (a large collection of photons), which is under\n\na collimated illumination, so the radiance of a ray can only be weakened because photons are scattered to other directions,\nan arbitrary illumination, so the radiance of a ray can be both weakened and augmented (by photons scattered from other directions);\n\nmultiple scattering, where a photon is scattered multiple times by a medium, so the radiance of a ray can both be weakened and augmented.\n\nSection 12.2.2 studies Case 1 and Case 2(a) together, because the latter is the statistical consequence of the former. Chapter 13 studies Case 2(b) and Case 3 together because they have the same observable effects and, thus, are modeled in the same way.\n\n\nInterference and Coherence\nSecond, when there is a large collection of particles, the scattered radiation fields of individual particles can interfere with each other. The exact impact of interference can only be calculated by considering the wave nature of the light. But to the first order, the inference depends on how densely packed the particles are.\nIn fact, the specular surface scattering we discussed in Section 11.1 is just a macroscopic approximation of the microscopic volume scattering where particles interfere non-randomly. In a mirror or a glass of water, the particles/molecules are very densely packed to the point that the distance between two particles is smaller than the wavelength of the light. As a result, the scattering is coherent, which gives rise to the illusion of a specular surface. One can show that the Fresnel equations are the solution to the Maxwell’s equations when surface particles are densely packed.\nWhy would the particle density matter? If particles are very close to each other, their radiation fields are close too, so the interference is stronger and cannot be ignored. More importantly, when particles are close to each other, their spatial positions can no longer be treated as random, so the interference can become coherent. Imagine you drop particles into a vast empty space; the particle sizes are much smaller relative to the space, so their spatial distribution can be roughly described as random. But if the particles are very densely packed, where the next particle can be is very much restrained, so their positions are highly correlated, leading to coherent scattering.\nWe will generally assume incoherent scattering unless otherwise noted, where individual scattering events interfere each other in random ways, so we are spared of the complication of thinking of the wave nature of the photons. Under this assumption, the total power scattered by a collection of particles is the same as the sum of the power scattered by the individual particles. This happens when the particles are sufficient sufficiently distant (separated by more than multiple wavelengths) and their spatial arrangements are uncorrelated.\n\n\n\n\n12.2.2 A Single Scattering Event\n\nScattering Efficiency and Coefficient\nIntuitively, scattering has a similar effect as absorption: it weakens the radiance by taking photons away from a beam of light. The difference is that scattered photons are not dead; they are re-directed to other directions. We can define two important quantities, one to characterize a particle’s ability to scatter photons and the other to characterize a medium’s ability to scatter photons.\nSimilar to the situation in absorption, the intrinsic capability of a particle to scatter photons is defined by the particle’s scattering cross section \\(\\epsilon_s\\), which itself is the product of the geometrical cross-sectional area of the particle \\(\\epsilon_g\\) and the scattering efficiency \\(Q_s\\). We can then define the scattering coefficient \\(\\sigma_s\\) of a medium (a large collection of particles), which characterizes the ability of the medium to scatter photons away from its incident radiance. \\(\\sigma_s\\) is the product of the particle concentration of the medium \\(c\\) and the particle’s scattering cross section \\(\\epsilon_s\\). Again, \\(\\sigma_s\\) has a unit \\(\\text{m}^\\text{-1}\\) and is not bound by 0 and 1; it is best interpreted as the probability density (i.e., probability per unit length) of light being scattered away. Of course, both the scattering efficiency and scattering coefficient can vary spatially, angularly, and spectrally.\nThe effects of scattering and absorption add up, because they both weaken a radiance. We can extend Equation 12.13 to consider scattering (again omitting the wavelength from the equations):\n\\[\n    L(p+s\\omega, \\omega) = L(p, \\omega) e^{-\\int_0^s (\\sigma_a(p+t\\omega, \\omega) + \\sigma_s(p+t\\omega, \\omega)) \\d t},\n\\tag{12.14}\\]\nwhere \\(\\sigma_s(p, \\omega)\\) is the scattering coefficient at \\(p\\) toward the direction \\(\\omega\\).\nRearranging the terms, we can express the transmittance between \\(p\\) and \\(p+s\\omega\\) along the direction \\(\\omega\\), denoted \\(T(p \\rightarrow p+s\\omega)\\): \\[\n    T(p \\rightarrow p+s\\omega) = \\frac{L(p+s\\omega, \\omega)}{L(p, \\omega)} = e^{-\\int_0^s (\\sigma_a(p+t\\omega, \\omega) + \\sigma_s(p+t\\omega, \\omega)) \\d t}.\n\\tag{12.15}\\]\nEquation 12.14 can be derived using the same idea as that used for deriving the absorption equation in Section 12.1.1 by modeling a thin layer \\(\\D x\\) — with an additional assumption that \\(\\D x\\) is so thin that a photon is scattered at most once before leaving \\(\\D x\\). Therefore, scattering by a single particle has the same effect as absorption: they both take the photon out of the radiance, and that is why the absorption equation (Equation 12.13) can be directly extended here.\nThink of the applicability of Equation 12.14: it says that the radiance of a ray can only be weakened. If the incident light has only one direction (e.g., a collimated beam), Equation 12.14 is true when a photon is scattered at most once in the medium. This is because a scattered photon will not have a chance to get back to the ray. If the incident light is not mono-directional, e.g., diffuse illumination, Equation 12.14 in general does not apply — even if we consider only single scattering. This is because photons originally not along the direction \\(\\omega\\) can be scattered toward it through just one single scattering event. We can see how limited Equation 12.14 is: it applies only when the illumination is collimated and we assume only single scattering. We will relax this constraint later. \nJust like the absorption case (Equation 12.11), if a medium is mixed with different particles, each with a different scattering coefficient, the overall scattering coefficient is the sum of the individual scattering coefficients as if the medium is made up of a particular kind of particles.\nThe sum of the scattering coefficient and absorption coefficient is called the extinction coefficient or attenuation coefficient, denoted \\(\\sigma_t(p, \\omega)\\):\n\\[\n    \\sigma_t(p, \\omega) = \\sigma_a(p, \\omega) + \\sigma_s(p, \\omega).\n\\]\nThe ratio between the scattering coefficient and the attenuation coefficient is called the single-scattering albedo of the medium:\n\\[\n    \\rho = \\frac{\\sigma_s(p, \\omega)}{\\sigma_t(p, \\omega)}.\n\\]\nThis albedo can be seen as the volumetric counterpart of the surface albedo discussed in Equation 10.11. The two forms of albedo have the same physical meaning: the fraction of the incident energy that is scattered away (i.e., not absorbed). A dark medium (e.g., smoke) has a lower albedo, and a bright medium (e.g., mist) has a higher albedo.\nThe sum of the scattering and absorption cross sections is called the extinction cross section or attenuation cross section, denoted \\(\\epsilon_t = \\epsilon_a + \\epsilon_s\\). And of course \\(1/\\sigma_t\\) is the mean free path in a medium where both absorption and scattering take place, i.e., the mean distance a photon can travel without being absorbed or scattered away.\n\n\n\n12.2.3 Scattering Direction Distribution: Phase Function\nWhile the scattering efficiency (coefficient) characterizes how well a particle (medium) is able to scatter photons, it tells us nothing about the direction of scattering. The direction of a single scattering event is characterized by the phase function \\(f_p(p, \\os, \\oi)\\), which can be interpreted as the probability density function that a photon incident from a direction \\(\\oi\\) is scattered toward a direction \\(\\os\\). We will omit \\(p\\) and write the phase function as \\(f_p(\\os, \\oi)\\) when the discussion is unconcerned of \\(p\\).\n\\(f_p(\\os, \\oi)\\) is defined as the fraction of the irradiance incident from an infinitesimal solid angle \\(\\doi\\) that is scattered toward an infinitesimal solid angle \\(\\dos\\) per unit solid angle:\n\\[\n    f_p(\\os, \\oi) = \\lim_{\\Dos \\rightarrow 0} \\lim_{\\Doi \\rightarrow 0} \\frac{\\D E_o(\\os)}{\\D E_i(\\oi)} / \\Dos = \\frac{\\d^2 E_o(\\os)}{\\d E_i(\\oi)\\dos} = \\frac{\\d^2 E_o(\\os)}{L(\\oi) \\doi \\dos}.\n\\tag{12.16}\\]\n\\(\\D E_i(\\oi)\\) is the incident irradiance over a small solid angle \\(\\Doi\\) and scatters in all directions. \\(\\D E_o(\\oi)\\) is the outgoing irradiance over a small solid angle \\(\\Dos\\), so \\(\\frac{\\D E_o(\\os)}{\\D E_i(\\oi)}\\) is the fraction of the photons incident from \\(\\Doi\\) that are scattered over \\(\\Dos\\) or, alternatively, the probability that a photon incident from \\(\\Doi\\) is scattered toward \\(\\Dos\\); this ratio/fraction is clearly a value between 0 and 1. Dividing that fraction by \\(\\Dos\\) gets us the probability per unit solid angle. When both the incident solid angle \\(\\Doi\\) and the outgoing solid angle \\(\\Dos\\) approach 0, the fraction can be interpreted as the directional-directional reflectance (Section 10.2), and the probability per solid angle within \\(\\Dos\\) becomes the probability density toward \\(\\os\\).\nLike all density functions, the meaning of a phase function is most clear when it is integrated to compute some other quantity. Integrating Equation 12.16 over all the outgoing directions \\(\\os\\):\n\\[\n\\begin{aligned}\n    \\d E_o &= \\int^{\\Omega = 4\\pi} f_p(\\os, \\oi)L(\\oi)\\doi\\dos, \\\\\n    &= L(\\oi)\\doi\\int^{\\Omega = 4\\pi} f_p(\\os, \\oi)\\dos, \\\\\n    &= \\d E_i\\int^{\\Omega = 4\\pi} f_p(\\os, \\oi)\\dos.\n\\end{aligned}\n\\tag{12.17}\\]\nTo interpret this integration, consider a point that receives an incident radiance of \\(L(\\oi)\\) over an infinitesimal solid angle \\(\\doi\\). The point receives a total irradiance of \\(\\d E_i = L(\\oi)\\doi\\), which is scattered in all directions. The density of the irradiance scattered toward a particular direction \\(\\os\\) is \\(f_p(\\os, \\oi)L(\\oi)\\doi\\)3, which when multiplied by \\(\\dos\\) gives us the actual irradiance scattered over a small solid angle \\(\\dos\\) around \\(\\os\\). Integrating all outgoing directions over the entire sphere (\\(4\\pi\\)) we have Equation 12.17.\nNow, of course, some of the photons in \\(\\d E_i\\) might not be scattered; they could be absorbed, or they could simply not hit the cross section of any particle. So technically \\(\\d E_o \\leq \\d E_i\\) in Equation 12.17, just like how energy conservation is expressed in surface scattering in Equation 10.8. By the convention in the volume scattering literature, however, the phase function is defined such that \\(\\d E_i\\) refers to only the portion of the incident irradiance that does get scattered. Therefore, \\(\\d E_o = \\d E_i\\), so we have:\n\\[\n    \\int^{\\Omega = 4\\pi} f_p(\\os, \\oi) \\dos = \\int^{\\Omega = 4\\pi} f_p(\\os, \\oi) \\doi = 1.\n\\tag{12.18}\\]\nThat is, the phase function integrates to 1; the second integral can be derived using the Helmholtz reciprocity (since we are still dealing with geometrical optics):\n\\[\n    f_p(\\oi, \\os) = f_p(\\os, \\oi).\n\\]\nOne way to interpret the fact that the phase function integrates to 1 is that the phase function is the conditional probability density function of scattering: given that a photon is scattered, what is the probability (density) of scattering to a particular direction?\n\nPhase Function vs. BRDF\nThe phase function can be seen as the volumetric counterpart (in the sense that we are talking about volume scattering) of BRDF (Section 10.1) — with two differences. First, the definition of the BRDF accounts for absorption, so the BRDF integrates to at most 1, whereas the integral of the phase function is normalized to 1. This difference in definition is born purely of convention.\nThe second difference is more fundamental. There is no \\(\\cos\\theta\\) term when using the phase function; see, e.g., Equation 12.17, unlike how the BRDF is used to turn irradiance into radiance (e.g., Equation 10.8). In fact, from Equation 12.17 we can see that given a radiance \\(L(\\oi)\\) and a solid angle \\(\\doi\\), the irradiance is simply \\(L(\\oi)\\doi\\) rather than \\(L(\\oi)\\cos\\theta_i\\doi\\). Didn’t we say that there is a cosine fall-off between radiance and irradiance (Section 8.1.4)?\nOne intuition that might help is that in volume scattering we are dealing with points, which can receive flux from the entire sphere and have no definition of a normal (because points are dimensionless and shapeless) or, perhaps more conveniently, have a “flexible” normal that changes with the illumination direction and is always facing directly at the illumination. Entertain this thought experiment. We set up a small surface detector at a point and measure the power of the detector; if the incident light is parallel to the surface, the detector would receive no power, but would you say that the point does not receive any light and that the radiation field has no power? Of course not.\nThe fact that a parallel surface would receive no photons absolutely does not mean the illumination has no power; the radiation field is the same whether it is illuminating a surface or illuminating a point. But if we are modeling a surface, we want our model to say that the power received by the surface is 0, because it matches our phenomenological observation (that a detector arranged that way would receive no recording); when we are modeling a point in volume scattering, we want the point to receive a power as if the point has a “normal” that is directly facing the illumination because, again, this matches our phenomenological observation.\nUltimately, the difference is a conscious choice of modeling strategy even though the underlying physics is exactly the same. That is why models based on BRDF and phase function are phenomenological models. If you deal with electromagnetic theories and QED, you would not have to have this distinction between modeling surface and volume scattering.\nWith the understanding that there is no cosine fall-off in volume scattering, Equation 12.16 can be re-written as:\n\\[\n    f_p(\\os, \\oi) = \\frac{\\d^2 E_o(\\os)}{\\d E_i(\\oi)\\dos} = \\frac{\\d}{\\d E_i(\\oi)}\\frac{\\d E_o(\\os)}{\\dos} = \\frac{\\d L_o(\\os)}{\\d E_i(\\oi)} = \\frac{\\d L_o(\\os)}{L_i(\\oi)\\doi},\n\\tag{12.19}\\]\nwhere \\(\\d L_o(\\os)\\) is the infinitesimal outgoing radiance toward \\(\\os\\). In this sense, the phase function operates in exactly the same way as the BRDF (Equation 11.15): they both operate on irradiance and turn infinitesimal irradiance into infinitesimal radiance.\n\n\nIsotropic Medium and Isotropic Scatters\nGiven the normalization in the phase function, the scattering efficiency should actually be parameterized as \\(\\bar{Q_s}(p, \\os, \\oi)\\):\n\\[\n    \\bar{Q_s}(p, \\os, \\oi) = Q_s(p, \\oi) f_p(p, \\os, \\oi),\n\\tag{12.20}\\]\nwhere \\(Q_s(p, \\oi)\\) should be be interpreted as the total scattering efficiency at \\(p\\) over all outgoing directions for a given incident direction \\(\\oi\\). Similarly, the scattering coefficient would be expressed as:\n\\[\n    \\bar{\\sigma_s}(p, \\os, \\oi) = \\sigma_s(p, \\oi) f_p(p, \\os, \\oi),\n\\tag{12.21}\\]\nwhere \\(\\sigma_s(p, \\oi)\\) is interpreted as the total scattering coefficient at \\(p\\) over all outgoing directions for a given incident direction \\(\\oi\\).\n\n\n\n\n\n\nFigure 12.4: Visualizations of common phase functions; adapted from Novák et al. (2018), where \\(\\omega\\) and \\(\\bar\\omega\\) are the incident direction \\(\\oi\\) and the outgoing direction \\(\\os\\) in our notation, respectively. For an isotropic medium, the phase function depends on only the angle \\(\\theta\\) subtended by \\(\\omega\\) and \\(\\omega'\\) and is axially symmetric about \\(\\omega\\).\n\n\n\nFigure 12.4 visualizes a few common phase functions. While \\(f_p(\\cdot)\\) is technically a 4D function parameterized by \\(\\os\\) and \\(\\oi\\), the phase function of many natural media is 1D and depends only on the angle \\(\\theta\\) subtended by \\(\\os\\) and \\(\\oi\\). In Figure 12.4, the distance of a point on the contour to the center represents the magnitude of the phase function at that particular \\(\\theta\\) Consider under what conditions this simplification can be true:\n\nFirst, it says that the phase function does not depend on the absolute incident direction \\(\\oi\\) but the relative angle between \\(\\oi\\) and \\(\\os\\). To get a visual intuition, see Figure 12.5; if the phase function is invariant to the photon incident direction \\(\\oi\\), we can, without losing any generality, assign \\(\\oi\\) to the \\(z\\)-axis; the scattered direction \\(\\os\\) is parameterized by \\(\\theta\\) and \\(\\phi\\).\nSecond, it also says the phase function depends on only \\(\\theta\\) but not \\(\\phi\\). That is, the phase function is rotationally symmetric about the incident direction \\(\\oi\\). So \\(f_p(\\os, \\oi) = f_p(\\os', \\oi) \\neq f_p(\\os'', \\oi)\\).\n\n\n\n\n\n\n\nFigure 12.5: The phase function of a spherical particle is 1) invariant to the incident direction \\(\\oi\\), which, without losing generality, is taken to be the \\(z\\)-axis here, and 2) also invariant to the azimuthal angle \\(\\phi\\) of the outgoing direction \\(\\os\\) but depends on the polar angle \\(\\theta\\). So \\(f_p(\\os, \\oi) = f_p(\\os', \\oi) \\neq f_p(\\os'', \\oi)\\). Media consisting of such particles are called isotropic media, but it does not mean the particle itself is an isotropic scatterer, which does not exist, but if it did, its phase function would be a constant (invariant to both \\(\\theta\\) and \\(\\phi\\)).\n\n\n\nIntuitively, the phase function has the following two properties:\n\nIf you fix the incident direction, no matter how you rotate the particle, the phase function distribution is the same. Alternatively, if you change the incident direction, the phase function distribution moves along with the incident direction.\nGiven an incident direction, the phase function distribution is axially symmetric about the incident direction.\n\nThe two conditions above are met only when the medium consists of randomly distributed spherically symmetric particles4, in which case 1) there is no reason to think that any incident direction is special, so the phase function certainly is invariant to \\(\\oi\\), and 2) there is no reason to think \\(\\os\\) and \\(\\os'\\) are any different since one should not expect the scattering behavior to change if we rotate the sphere about the incident direction (\\(z\\)-axis).\nA medium consisting of spherically symmetric particles is called a symmetric or an isotropic medium. Usually when we refer to an isotropic medium, not only is the phase function but also the total scattering coefficient \\(\\sigma_s(p, \\oi)\\) (Equation 12.21) rotationally invariant to the incident direction5.\nAs we said earlier, “isotropic” is an unbelievably overloaded term. People also call a particle an isotropic scatterer if its phase function is a constant, i.e., invariant to \\(\\os\\); such a phase function is sometimes called an isotropic phase function. An isotropic scatterer does not exist; it is a purely theoretical construction, but if it existed, its phase function would take the value of \\(\\frac{1}{4\\pi}\\) given Equation 12.18, as shown in the first graph in Figure 12.4.\n\n\n\n12.2.4 Common Models and General “Rules”\nThere are many factors that determine the exact scattering efficiency and scattering direction, which can be calculated by solving the Maxwell’s equations. We will talk about a few common models here; we focus on the intuitions while omitting the exact mathematical expressions, which can be found in standard texts. From the models, we can identify a few general “rules” or, rather, approximations under certain assumptions.\nThe main theory or model for a single scattering event is called the Mie scattering theory, which, strictly speaking, applies only when the particle is spherical (Sharma 2003, chap. 3.5.2; Bohren and Clothiaux 2006, chap 3.5; Melbourne 2004, chap. 3). Mie scattering is not somehow a different scattering process from any other scattering, and the Mie theory is nothing more than the solution to the Maxwell’s equations under certain conditions6.\nThe Mie theory predicts that the overall scattering efficiency \\(Q_s\\) is:\n\\[\n\\begin{aligned}\n    Q_s &= \\frac{8}{3}\\gamma^4\\big(\\frac{m^2-1}{m^2+2}\\big)\\big[1+\\frac{6}{5}\\big(\\frac{m^2-1}{m^2+2}\\big)\\gamma^2 + \\cdots \\big]\\\\\n    \\gamma &= \\frac{r}{\\lambda_m}, \\\\\n    m &= \\frac{n}{n_m},\n\\end{aligned}\n\\tag{12.22}\\]\nwhere \\(m\\) is the the relative refractive index between the particle and the medium surrounding the particle, and \\(\\gamma\\) is the ratio between the particle radius \\(r\\) and the incident light wavelength in the surrounding medium \\(\\lambda_m\\). The notion of surrounding media might come across as a little surprising: doesn’t the material consist merely of its particles? Hardly. For instance, in paints, pigments are surrounded by binders (e.g., linseed oil in oil paints, egg yolk in tempera paints, and beeswax in encaustic paints) and usually some amount of water (except oil paints). When paint dries, some water might be evaporated, leaving pockets of air, which also contributes to the surrounding media.\nWe can draw a few general conclusions from the model.\n\nSmall-Particle (Rayleigh) Scattering\nFor small particles where \\(\\gamma \\ll 1\\) (generally when the radius is ten times smaller than the wavelength of the incident light), only the first term in Equation 12.22’s bracket matters, so the scattering efficiency is inversely proportional to \\(\\lambda_m^{-4}\\). The inverse proportionality to \\(\\lambda_m^{-4}\\) largely (but apparently not entirely) explains why the sky is blue and why the sun is red (Bohren and Clothiaux 2006, chap. 8.1). Why? First, recognize that individual molecules, such as air molecules, are usually sub-nm in size, so they scatter in this small-particle regime. Short wavelength lights from the sun are scattered by the atmospheric molecules more toward the sky and eventually enter your eyes, so if you look at the sky (against the sun) it would appear blue; when you look at the sun directly, the photons entering your eyes are mostly those unscattered ones that transmit directly through the atmosphere, and they are mostly longer-wavelength photons.\nBy then water molecules are also similarly small, so why would water look so different from the air? It is because water molecules are very densely packed, so their scatterings are coherent. In fact, the end result of such coherent scatterings by a collection of water molecules is that water appears specular.\nThe photopigments in a photoreceptor are very small in size compared to the wavelengths of visible light (each rhodopsin has a cross-section area of about 10-2 \\(\\text{nm}^\\text{2}\\) (Milo and Phillips 2015, p. 144)), so they almost do not scatter lights at all, only absorption. That is why we could use microspectrophotometry (MSP) to measure a photoreceptor’s (transverse) absorption rate (Section 3.2.1): MSP measures the amount of light transmitted through a photoreceptor, and if there is little scattering, then all the photons that are not measured must be absorbed by the photoreceptor.\nScattering in the small-particle regime is also called Rayleigh scattering, which, again, is not somehow a fundamentally different scattering process, and the Rayleigh scattering theory7 is nothing more than a special case of the Mie scattering theory (Sharma 2003, chap. 3.5.1; Bohren and Clothiaux 2006. chap. 3.2).\nThe phase function in the Rayleigh regime is proportional to \\(1+\\cos^2\\theta\\), so the backward and forward scatterings are roughly equally probable. Taking the phase function into account, the scattering efficiency (in the form defined in Equation 12.20) in Rayleigh scattering is proportional to:\n\\[\n    Q_s \\propto (\\frac{r}{\\lambda_m})^4 \\frac{m^2-1}{m^2+2} (1 + \\cos^2\\theta).\n\\]\n\n\nImpact of Particle Size\nWhen the particle size increases, the scattering efficiency increases, initially very quickly, but eventually saturates. In fact, the Mie theory predicts that when the particle size is much larger than the wavelength (e.g., more than 100 times larger), the scattering efficiency approaches a constant 2 regardless of \\(m\\) and \\(\\lambda_m\\) (Johnsen 2012, fig. 5.4). This is evident in Figure 12.6, which shows the scattering efficiency of a kind of particle as a function of particle radius (\\(x\\)-axis) under different \\(m\\) (different curves); the incident light wavelength is 500 \\(\\text{nm}\\).\n\n\n\n\n\n\nFigure 12.6: Scattering efficiency as a function of particle radius under different relative refractive index \\(m\\); the incident light wavelength is 500 \\(\\text{nm}\\). From Johnsen (2012, fig. 5.4).\n\n\n\nThe particle size also affects the phase function. As we have discussed above, small particles in the Rayleigh regime tend to scatter photons equally in the forward and backward directions, while large particles primarily scatter photons in the forward directions. The last two graphs in Figure 12.4 show the phase functions predicted by the Mie scattering theory under different particle sizes (both are larger than that in the Rayleigh regime). The forward fraction increases as the particle size increases.\nConsider the scenario in Figure 7.1, where Material1 sits on top of Material2, and our goal is to hide Material 2 so that the color of Material 1 is dependent only on the illumination (not the property of Material 2). There is an interesting trade-off between scattering efficiency and scattering direction here. If we want Material 1 to hide Material 2, we want the particles in Material 1 to scatter a lot of light (high scattering efficiency) backwards. If the scattering efficiency is low (so photons march on and are hindered only by absorption) or the scattering is heavy in the forward directions, photons penetrate through Material 1 and reach Material 2, which would then contribute to the overall color.\nNow, to scatter a lot of light, we need the particles to be large, but then the scattering will be mostly in the forward directions. So there exists a sweet spot of the particle size that provides the highest “hiding power” for a material per unit volume. If we work out the math, we will see that the sweet spot falls roughly in the visible wavelength range. That is why most paint pigments have a diameter between 100 \\(\\text{nm}\\) and 1 \\(\\mu\\text{m}\\) (Bruce MacEvoy 2015). Of course, no matter how poor the hiding power is for a particular paint, if you apply enough of it, it will eventually hide whatever is behind it. Dye pigments are rather small in size (\\(\\text{nm}\\) range), so they scatter few photons and that is why dye solutions look relatively transparent.\n\n\nImpact of Refractive Index\nFigure 12.6 also shows the impact of the relative refractive index \\(m\\) (between the particle and the surrounding media) on scattering efficiency. Generally, the scattering efficiency increases with \\(m\\) at all particle sizes until when the particles are so large that the scattering efficiency becomes a constant. This is supported by Equation 12.22, too (\\(\\frac{m^2-1}{m^2+2}\\) monotonically increases and has a limit of 1).\nFor large particles, while \\(m\\) does not affect the scattering efficiency, it influences the scattering directions. When \\(m\\) is small, the scattering tends to be more forward, whereas when \\(m\\) is large, the scattering tends to be toward large angles (i.e., more photons will be back-scattered). This is why wet objects look darker (recall the unpleasant experience of accidentally spilling water on your pants). In dry paints, the medium surrounding the textile particles is air, and in wet paints it is water. \\(m\\) becomes smaller when the material is wet (i.e., the relative refractive-index difference becomes smaller between the textile particles and water), so most of the scattering will be forward, increasing the traversal length of photons and essentially giving photons more opportunities to be absorbed.\n\n\nAspherical Particles\nWhat if the particle is not spherical? The Mie theory does not apply. Analytical or even numerical solutions to the Maxwell’s equations would be difficult, so perhaps a better approach is just to parameterize a model and fit it with the experimental data.\nOne popular one-parameter parameterization of the phase function is the Henyey–Greenstein phase function (Pharr, Jakob, and Humphreys 2018, chap. 11.3.1; Bohren and Clothiaux 2006, chap. 6.3.2), which takes the form:\n\\[\n    p(\\theta) = \\frac{1}{4\\pi} \\frac{1-g^2}{(1+g^2-2g\\cos\\theta)^{3/2}},\n\\]\nwhere \\(g\\) is the free parameter and is usually called the asymmetry parameter.\nWe hasten to emphasize that the Henyey–Greenstein function has absolutely zero physical meaning; it is designed for fitting experimental phase function data, so in the modern deep learning era, you might as well try a deep neural network. The second graph in Figure 12.4 shows one instantiation of the Henyey–Greenstein function.\n\n\n\n\nBohren, Craig F, and Eugene E Clothiaux. 2006. Fundamentals of Atmospheric Radiation: An Introduction with 400 Problems. John Wiley & Sons.\n\n\nBowmaker, James K, and HJk Dartnall. 1980. “Visual Pigments of Rods and Cones in a Human Retina.” The Journal of Physiology 298 (1): 501–11.\n\n\nBruce MacEvoy. 2015. “The material attributes of paints.” https://www.handprint.com/HP/WCL/pigmt3.html#particlesize.\n\n\nJohnsen, Sönke. 2012. The Optics of Life: A Biologist’s Guide to Light in Nature. Princeton University Press.\n\n\nKipp Jones. 2005. “The sand/dust from the harmattan coming from the Sahara gives the Nigeria’s National Mosque in Abuja, a nice glow; CC BY-SA 2.0 license.” https://commons.wikimedia.org/wiki/File:MosqueinAbuja.jpg.\n\n\nMayerhöfer, Thomas G, Susanne Pahlow, and Jürgen Popp. 2020. “The Bouguer-Beer-Lambert Law: Shining Light on the Obscure.” ChemPhysChem 21 (18): 2029–46.\n\n\nMelbourne, William G. 2004. Radio Occultations Using Earth Satellites: A Wave Theory Treatment. 1st ed. John Wiley & Sons.\n\n\nMilo, Ron, and Rob Phillips. 2015. Cell Biology by the Numbers. Garland Science.\n\n\nNovák, Jan, Iliyan Georgiev, Johannes Hanika, Jaroslav Křivánek, and Wojciech Jarosz. 2018. “Monte Carlo Methods for Physically Based Volume Rendering.” In ACM SIGGRAPH 2018 Courses.\n\n\nPharr, Matt, Wenzel Jakob, and Greg Humphreys. 2018. Physically Based Rendering: From Theory to Implementation. 3rd ed. MIT Press.\n\n\nSharma, Gaurav. 2003. “Color Fundamentals for Digital Imaging.” In Digital Color Imaging Handbook, edited by Gaurav Sharma, 14–127. CRC Press.",
    "crumbs": [
      "Rendering",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Volume and Subsurface Scattering Processes</span>"
    ]
  },
  {
    "objectID": "rendering-sss.html#footnotes",
    "href": "rendering-sss.html#footnotes",
    "title": "12  Volume and Subsurface Scattering Processes",
    "section": "",
    "text": "“Isotropic” is a very overloaded term; it just means some physical property is invariant when measured from different directions. So depending on what physical property you care about, “isotropic” can mean different things. The property we care about here is a volume’s ability to absorb photons, which is different from our earlier use of isotropy, which is concerned with the ability of a surface to scatter photons.↩︎\nTechnically photons from other objects can enter our eye through a single-scattering event; see the discussion at the end.↩︎\nA direction \\(\\os\\) has a solid angle of 0, so its associated irradiance is technically 0, too. What \\(f_p(\\os, \\oi)L(\\oi)\\doi\\) represents is the irradiance per solid angle.↩︎\nor when the medium consists of randomly distributed and oriented spherically asymmetric particles, in which case the medium is statistically spherically symmetric.↩︎\nIn theory, it is certainly possible to have a medium whose total scattering coefficient/efficiency varies with the incident direction but not the angular distribution/probability of the scattered photons.↩︎\nThe modern form of the solution is summarized, not invented, by Gustav Mie but the solution had been developed by many predecessors such as Ludvig Lorenz.↩︎\nworked out by Lord Rayleigh, who won the Nobel Prize in Physics in 1904↩︎",
    "crumbs": [
      "Rendering",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Volume and Subsurface Scattering Processes</span>"
    ]
  },
  {
    "objectID": "rendering-rte.html",
    "href": "rendering-rte.html",
    "title": "13  Rendering Volume and Subsurface Scattering",
    "section": "",
    "text": "13.1 Radiative Transfer Equation\nSo far we have assumed that a ray can only be attenuated, which can happen only when the illumination is collimated and we assume single scattering. Under this assumption, Equation 12.14 allows us to calculate any radiance in the medium (by weakening the initial radiance). General media are much more complicated: illumination can be from anywhere, and multiple scattering must be accounted for. As a result, external photons can be scattered into a ray of interest, as we have intuitively discussed in Section 12.2.1.\nIn the realm of geometric optics and radiometry, the general way to model lights going through a material/medium amounts to solving the so-called Radiative Transfer Equation (RTE), whose modern version was established by Chandrasekhar (1960)1, and Kajiya and Von Herzen (1984) was the first to introduce it to computer graphics. The RTE provides the mathematical tool to model an arbitrary radiance in a medium (Section 13.1).\nWe will then discuss an important way to solve the RTE by turning it to the Volume Rendering Equation (VRE) (Section 13.2), discuss the discrete form of the VRE that is commonly used in scientific visualization (Section 13.3) and, more recently, radiance-field rendering (Section 13.4), a modern iteration of image-based rendering (Section 9.2.2) that uses discrete VRE to parameterize the image formation process. Finally, we will show a simple phenomenological model that integrates surface scattering with volume scattering (Section 13.5).\nThe basic idea is to set up a differential equation to describe the (rate) of the radiance change. Given an incident radiance \\(L(p, \\os)\\), we are interested in \\(L(p+\\D s \\os, \\os)\\), the radiance after the ray has gone a small distance \\(\\D s\\). The radiance can be:\nThe attenuation (reduction) of the radiance over \\(\\D s\\) is:\n\\[\n    -L(p, \\os) \\sigma_t(p, \\os) \\D s.\n\\tag{13.1}\\]\nThe radiance augmentation due to in-scattering is given by:\n\\[\n    \\int^{\\Omega = 4\\pi} f_p(p, \\os, \\oi) \\sigma_s(p, \\os) \\D s L(\\oi) \\doi = \\sigma_s(p, \\os) \\D s \\int^{\\Omega = 4\\pi} L(p, \\oi) f_p(p, \\os, \\oi) \\doi.\n\\tag{13.2}\\]\nThe way to interpret Equation 13.5 is the following. \\(L(p, \\oi)\\) is the incident radiance from a direction \\(\\oi\\), \\(L(p, \\oi) \\doi\\) is the irradiance received from \\(\\doi\\), of which \\(\\sigma_s(p, \\oi)\\D s L(p, \\os) \\doi\\) is the irradiance scattered in all directions after traveling a distance \\(\\D s\\). That portion of the scattered irradiance is multiplied by \\(f_p(\\os, \\oi)\\) to give us the radiance toward \\(\\os\\) (see Equation 12.19). We then integrate over the entire sphere, accounting for the fact that lights can come from anywhere over the space, to obtain the total augmented radiance toward \\(\\os\\).\nIf we consider emission, the total radiance augmentation is:\n\\[\n    \\sigma_a(p, \\os) \\D s L_e(p, \\os) + \\sigma_s(p, \\os) \\D s \\int^{\\Omega = 4\\pi} L(p, \\oi) f_p(p, \\os, \\oi) \\doi,\n\\tag{13.3}\\]\nwhere \\(L_e(p, \\os)\\) is the emitted radiance at \\(p\\) toward \\(\\os\\), so the first term represents the total emission over \\(\\D s\\). If we let:\n\\[\n    L_s(p, \\os) = \\sigma_a(p, \\os) L_e(p, \\os) + \\sigma_s(p, \\os) \\int^{\\Omega = 4\\pi} L(p, \\oi) f_p(p, \\os, \\oi) \\doi,\n\\tag{13.4}\\]\nthe total augmentation can be simplified to:\n\\[\n    L_s(p, \\os) \\D s,\n\\tag{13.5}\\]\nwhere the \\(L_s\\) term is sometimes called the source term or source function in computer graphics, because it is the source of power at \\(p\\)3.\nCombining Equation 13.1 and Equation 13.5, the net radiance change is4:\n\\[\n\\begin{aligned}\n    \\D L(p, \\os) &= L(p + \\D s \\os, \\os) - L(p, \\os) \\\\\n    &= -L(p, \\os) \\sigma_t(p, \\os) \\D s + L_s(p, \\os) \\D s.\n\\end{aligned}\n\\tag{13.6}\\]\nAs \\(\\D s\\) approaches 0, we get (assuming \\(\\os\\) is a unit vector as in Equation 12.13 and Equation 12.14):\n\\[\n\\begin{aligned}\n    \\os \\cdot \\nabla_p L(p, \\os) &= \\frac{\\d L(p, \\os)}{\\d s} = \\lim_{\\D s \\rightarrow 0} \\frac{L(p + \\D s \\os, \\os) - L(p, \\os)}{\\D s} \\nonumber \\\\\n    &= -\\sigma_t(p, \\os) L(p, \\os) + L_s(p, \\os),\n\\end{aligned}\n\\tag{13.7}\\]\nwhere \\(\\nabla_p\\) denotes the gradient of \\(L\\) with respect to \\(p\\), and \\(\\os \\cdot \\nabla_p\\) denotes the directional derivative, which is used because technically \\(p\\) and \\(\\os\\) are both defined in a three-dimensional space, so what we are really calculating is the rate of radiance change at \\(p\\) along \\(\\os\\).\nEquation 13.7 is the RTE, which is an integro-differential equation, because it is a differential equation with an integral embedded. The RTE has an intuitive interpretation: if we think of radiance as the power of a ray, as a ray propagates, its power is attenuated by the medium but also augmented by “stray photons” from other rays. The latter is given by \\(L_s(p, \\os)\\), which can be thought of as the augmentation of the radiance per unit length.\nThe RTE describes the rate of change of an arbitrary radiance \\(L(p, \\os)\\). But our ultimate goal is to calculate the radiance itself? Generally the RTE has no analytical solution. There are two strategies to solve it. First, we can derive analytical solutions under certain certain assumptions and simplifications.\nThe second approach deserves its own section.",
    "crumbs": [
      "Rendering",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Rendering Volume and Subsurface Scattering</span>"
    ]
  },
  {
    "objectID": "rendering-rte.html#sec-chpt-mat-vs-rte-rte",
    "href": "rendering-rte.html#sec-chpt-mat-vs-rte-rte",
    "title": "13  Rendering Volume and Subsurface Scattering",
    "section": "",
    "text": "attenuated by the medium because of absorption;\nattenuated by the medium because photons are scattered out into other directions; this is called out-scattering in graphics;\naugmented by photons that are scattered into the ray direction from all other directions — because of multiple scattering2; this is called in-scattering in graphics;\naugmented because particles can emit photons.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor instance, the integral in Equation 13.7 can be approximated by a summation along \\(N\\) directions; then we can turn Equation 13.7 into a system of \\(N\\) differential equations to be solved. This is sometimes called the N-flux theory.  You might have heard of the famous Kubelka-Munk model (Kubelka and Munk 1931b, 1931a; Kubelka 1948) widely used in modeling the color of pigment mixture; it is essentially a special case of the N-flux theory where \\(N=2\\), which we will discuss in Section 14.1.\nAnother assumption people make is to assume that volume scattering is isotropic and can be approximated as a diffusion process. This is called the diffusion approximation (Ishimaru 1977; Ishimaru et al. 1978), which is widely used in both scientific modeling (Farrell, Patterson, and Wilson 1992; Eason et al. 1978; Schweiger et al. 1995; Boas et al. 2001) and in rendering (Stam 1995, chap. 7; Jensen et al. 2001; Dong et al. 2013); see Bohren and Clothiaux (2006, chap. 6.2) for a theoretical treatment.",
    "crumbs": [
      "Rendering",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Rendering Volume and Subsurface Scattering</span>"
    ]
  },
  {
    "objectID": "rendering-rte.html#sec-chpt-mat-vs-rte-vre",
    "href": "rendering-rte.html#sec-chpt-mat-vs-rte-vre",
    "title": "13  Rendering Volume and Subsurface Scattering",
    "section": "13.2 Volume Rendering Equation",
    "text": "13.2 Volume Rendering Equation\nThe second approach, which is particularly popular in computer graphics, is to first turn the RTE into a purely integral equation and then numerically (rather than analytically) estimate the integral using Monte Carlo integration, very similar to how the rendering equation is dealt with for surface scattering (Section 10.3).\n\n\n\n\n\n\nFigure 13.1: (a): Illustration of the continuous VRE (Equation 13.8). (b): Illustration of a discrete VRE (Equation 13.9), where the integral in the continuous VRE is replaced by a summation between \\(p_0\\) and \\(p\\) at an interval of \\(\\D s\\); \\(t_i\\) is the total transmittance between \\(p_i\\) and \\(p_{i+1}\\); \\(L_i\\) is a shorthand for \\(L_s(p_i, \\os)\\), the source term of at \\(p_i\\) toward \\(\\os\\).\n\n\n\nThe way to think of this is that in order to calculate any given radiance \\(L(p, \\os)\\), we need to integrate all the changes along the direction \\(\\os\\) up until \\(p\\). Where do we start the integration? We can start anywhere. Figure 13.1 (a) visualizes the integration process. Let’s say we want to start from a point \\(p_0\\), whose initial radiance toward \\(\\os\\) is \\(L_0(p_0, \\os)\\). Let \\(p = p_0 + s\\os\\), where \\(\\os\\) is a unit vector and \\(s\\) is the distance between \\(p_0\\) and \\(p\\). An arbitrary point \\(p'\\) between \\(p_0\\) and \\(p\\) would then be \\(p' = p_0 + s' \\os\\)5.\nNow we need to integrate from \\(p_0\\) to \\(p\\) by running \\(s'\\) from 0 to \\(s\\). Observe that the RTE is a form of a non-homogeneous linear differential equation, whose solution is firmly established in calculus. Without going through the derivations, its solution is:\n\\[\n    L(p, \\os) = T(p_0 \\rightarrow p)L_0(p_0, \\os) + \\int_{0}^{s} T(p' \\rightarrow p) L_s(p', \\os)\\d s',\n\\tag{13.8}\\]\nwhere \\(T(p_0 \\rightarrow p)\\) is the transmittance between \\(p_0\\) and \\(p\\) along \\(\\os\\), and \\(T(p' \\rightarrow p)\\) is the transmittance between \\(p'\\) and \\(p\\) along \\(\\os\\). Recall the definition of transmittance in Equation 12.15: it is the remaining fraction of the radiance after attenuation by the medium after traveling the distance between two points. In our case here:\n\\[\n\\begin{aligned}\n    T(p' \\rightarrow p) = \\frac{L(p+s\\os, \\os)}{L(p+s'\\os, \\os)} = e^{-\\int_{s'}^s \\sigma_t(p+t\\omega, \\omega) \\d t}, \\\\\n    T(p_0 \\rightarrow p) = \\frac{L(p+s\\os, \\os)}{L(p, \\os)} = e^{-\\int_{0}^s \\sigma_t(p+t\\omega, \\omega) \\d t},\n\\end{aligned}\n\\]\nThe integral equation Equation 13.8 in the graphics literature is called the volume rendering equation (VRE) or the volumetric light transport equation — the counterpart of the surface LTE (Section 10.3). Looking at the visualization in Figure 13.1 (a), the VRE has an intuitive interpretation: the radiance at \\(p\\) along \\(\\os\\) is the the contribution of \\(p_0\\) plus and contribution of every single point between \\(p_0\\) and \\(p\\).\n\nThe contribution of \\(p_0\\) is given by its initial radiance \\(L_0\\) weakened by the transmittance between \\(p_0\\) and \\(p\\);\nWhy would a point \\(p'\\) between \\(p_0\\) and \\(p\\) make any contribution? It is because of the source term (Equation 13.4): \\(p'\\) might emit lights, and some of the in-scattered photons at \\(p'\\) will be scattered toward \\(\\os\\). The contribution of \\(p'\\) is thus given by the source term \\(L_s\\) weakened by the transmittance between \\(p'\\) and \\(p\\).\n\nThe form of the VRE might appear to suggest that it is enough to accumulate along only the direct path between \\(p_0\\) and \\(p\\), which is surprising given that there are infinitely many scattering paths between \\(p_0\\) and \\(p\\) (due to multiple scattering). For instance, it appears that we consider only the outgoing radiance toward \\(\\os\\) from \\(p_0\\), but \\(p_0\\) might have outgoing radiances over other directions, which might eventually contribute to \\(L(p, \\os)\\) through multiple scattering. Are we ignoring them?\nThe answer is that the VRE implicitly accounts for all the potential paths between \\(p_0\\) and \\(p\\) because of the \\(L_s\\) term, which expands to Equation 13.4. That is, every time we accumulate the contribution of a point between \\(p_0\\) and \\(p\\), we have to consider the in-scattering from all the directions at that point. Another way to interpret this is to observe that the radiance term \\(L\\) appears on both sides of the equation. Therefore, the VRE must be solved recursively by evaluating it everywhere in space.\nDoes this remind you of the rendering equation (Equation 10.18)? Indeed, the VRE can be thought of as the volumetric counterpart of the rendering equation. Similarly, we can use Monte Carlo integration to estimate it, just like how the rendering equation is dealt with — with an extra complication: the VRE has two integrals: the outer integral runs from \\(p_0\\) to \\(p\\) and, for any intermediate point \\(p'\\), there is an inner integral that runs from \\(p'\\) to \\(p\\) to evaluate the transmittance \\(T(p' \\rightarrow p)\\). Therefore, we have to sample both integrands.\nSimilar to the situation of the rendering equation, sampling recursively would exponentially increase the number of rays to be tracked. Put it another way, since there are infinitely many paths from which a ray gains its energy due to multiple scattering, we have to integrate infinitely many paths. Again, a common solution is path tracing, for which Pharr, Jakob, and Humphreys (2023, chap. 14) is a great reference.\nA simplification that is commonly used is to assume that there is only single scattering directly from the light source. In this way, the \\(L_s\\) term does not have to integrate infinitely many incident rays over the sphere but only a fixed amount of rays emitted from the light source non-recursively. This strategy is sometimes called local illumination in volume rendering, as opposed to global illumination, where one needs to consider all the possible paths of light transport. The distinction is similar to that in modeling surface scattering (Section 10.3).",
    "crumbs": [
      "Rendering",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Rendering Volume and Subsurface Scattering</span>"
    ]
  },
  {
    "objectID": "rendering-rte.html#sec-chpt-mat-vs-rte-vis",
    "href": "rendering-rte.html#sec-chpt-mat-vs-rte-vis",
    "title": "13  Rendering Volume and Subsurface Scattering",
    "section": "13.3 Discrete VRE and Scientific Volume Visualization",
    "text": "13.3 Discrete VRE and Scientific Volume Visualization\nSometimes the VRE takes the following discrete form:\n\\[\n    L = \\sum_{i=0}^{N-1}\\big(L_i\\D s\\prod_{j=i+1}^{N-1}t_j\\big).\n\\tag{13.9}\\]\nEquation 13.9 is the discrete version of Equation 13.8: the former turns the two integrals in the latter (both the outer integral and the inner one carried by \\(T(\\cdot)\\)) to discrete summations using the Riemann sum over \\(N\\) discrete points along the ray between \\(p_0\\) and \\(p\\) at an interval of \\(\\D s = \\frac{s}{N}\\).\nThe notations are slightly different; Figure 13.1 (b) visualizes how this discrete VRE is expressed with the new notations.\n\n\\(L\\) is \\(L(p, \\os)\\), the quantity to be calculated;\n\\(L_i\\) is a shorthand for \\(L_s(p_i, \\os)\\), i.e., the source term (Equation 13.4) for the \\(i^{th}\\) point between \\(p_0\\) and \\(p\\) toward \\(\\os\\); by definition, \\(p_0\\) is the \\(0^{th}\\) point (so \\(L_0\\) is the initial radiance \\(L_0(p_0, \\os)\\) in Equation 13.8) and \\(p\\) is the \\(N^{th}\\) point;\n\\(t_i\\) (or more explicitly \\(t(p_{i} \\rightarrow p_{i+1})\\)) represents the total transmittance between the \\(i^{th}\\) and the \\((i+1)^{th}\\) point and is given by \\(e^{-\\sigma_t(p_i, \\os) \\D s}\\) (notice the integral in continuous transmittance Equation 12.15 is gone, because we assume the transmittance between two adjacent points is a constant in the Reimann sum);\n\\(\\alpha_i\\) is the opacity between the \\(i^{th}\\) and the \\((i+1)^{th}\\) point, which is defined as the residual of the transmittance between the two points: \\(1-t_i\\).\n\nSee Max (1995, Sect. 4) or Kaufman and Mueller (2003, Sect. 6.1) for a relatively straightforward derivation of Equation 13.9, but hopefully this form of the VRE is equally intuitive to interpret from Figure 13.1 (b). It is nothing more than accumulating the contribution of each point6 along the ray, but now we also need to accumulate the attenuation along the way just because of how opacity is defined by convention (per step), hence the product of a sequence of the opacity residuals.\nWe can also re-express Equation 13.9 using opacity rather than transmittance: \\[\n\\begin{aligned}\n    L &= \\sum_{i=0}^{N-1}\\big(L_i\\D s\\prod_{j=i+1}^{N-1}(1-\\alpha_j)\\big) \\\\\n    &= L_{N-1} \\D s + L_{N-2} \\D s(1-\\alpha_{N-1}) + L_{N-3} \\D s(1-\\alpha_{N-1})(1-\\alpha_{N-2}) +~\\cdots~  \\\\\n    &~~~+ L_1\\D s\\prod_{j=2}^{N-1}(1-\\alpha_j)+ L_0\\D s\\prod_{j=1}^{N-1}(1-\\alpha_j).\n\\end{aligned}\n\\tag{13.10}\\]\nThe discrete VRE is usually used in the scientific visualization literature, where people are interested in visualizing data obtained from, e.g., computer tomography (CT) scans or magnetic resonance imaging (MRI). There, it is the relative color that people usually care about, not the physical quantity such as the radiance, so people sometimes lump \\(L_i\\D s\\) together as \\(C_i\\) and call it the “color” of the \\(i^{th}\\) point. The VRE is then written as:\n\\[\n    C = \\sum_{i=0}^{N-1}\\big(C_i\\prod_{j=i+1}^{N-1}(1-\\alpha_j)\\big).\n\\tag{13.11}\\]\nThe \\(C\\) terms are defined in a three-dimensional RGB space, and Equation 13.11 is evaluated for the three channels separately, similar to how Equation 13.9 and Equation 13.8 are meant to be evaluated for each wavelength independently. Since color is a linear projection from the spectral radiance, the so-calculated \\(C\\) (all three channels) is indeed proportional to the true color, although in visualization one usually does not care about the true colors anyway (see Section 13.3.3).\nEquation 13.11 is also called the back-to-front compositing formula in volume rendering, since it starts from \\(p_0\\), the farthest point on the ray to \\(p\\). We can easily turn the order around to start from \\(p\\) and end at \\(p_0\\) in a front-to-back fashion (\\(C_{N-1}\\) now corresponds to \\(p_0\\)):\n\\[\n    C = \\sum_{i=0}^{N-1}\\big(C_i\\prod_{j=0}^{i-1}t_j\\big).\n\\tag{13.12}\\]\nWhile theoretically equivalent, the latter is better in practice because it allows us to opportunistically terminate the integration early when, for instance, the accumulated opacity is high enough (transmittance is low enough), at which point integrating further makes little numerical contribution to the result.\n\n13.3.1 Another Discrete Form of VRE\nA perhaps more common way to express the discrete VRE is to approximate the transmittance \\(t\\) using the first two terms of its Taylor series expansion and further assume that the medium has a low albedo, i.e., \\(\\sigma_t \\approx \\sigma_a\\) and \\(\\sigma_s \\approx 0\\) (that is, the medium emits and absorbs only); we have:\n\\[\n\\begin{aligned}\n    & 1 - \\alpha_i = t_i = t(p_{i} \\rightarrow p_{i+1}) = e^{-\\sigma_t(p_i, \\os) \\D s} = 1 - \\sigma_t(p_i, \\os) \\D s + \\frac{(\\sigma_t(p_i, \\os) \\D s)^2}{2} - \\cdots \\\\\n    \\approx & 1 - \\sigma_t(p_i, \\os) \\D s \\\\\n    \\approx & 1 - \\sigma_a(p_i, \\os) \\D s.\n\\end{aligned}\n\\]\nTherefore:\n\\[\n\\alpha_i \\approx \\sigma_a(p_i, \\os) \\D s.\n\\tag{13.13}\\]\nNow, observe that the \\(L_i\\) term in Equation 13.9 is the source term in Equation 13.4, which under the low albedo assumption has only the emission term, so:\n\\[\n\\begin{aligned}\n    L &= \\sum_{i=0}^{N-1}\\big(L_i\\D s\\prod_{j=i+1}^{N-1}(1-\\alpha_j)\\big) \\\\\n    &=  \\sum_{i=0}^{N-1}\\big(\\sigma_a(p_i, \\os) L_e(p_i, \\os)\\D s\\prod_{j=i+1}^{N-1}(1-\\alpha_j)\\big).\n\\end{aligned}\n\\tag{13.14}\\]\nNow plug in Equation 13.13, we have:\n\\[\n    L =  \\sum_{i=0}^{N-1}\\big(L_e(p_i, \\os)\\alpha_i\\prod_{j=i+1}^{N-1}(1-\\alpha_j)\\big).\n\\tag{13.15}\\]\nIf we let \\(C_i = L_e(p_i, \\os)\\), the discrete VRE is then expressed as (Levoy 1988):\n\\[\n    C = \\sum_{i=0}^{N-1}\\big(C_i\\alpha_i\\prod_{j=i+1}^{N-1}(1-\\alpha_j)\\big).\n\\tag{13.16}\\]\nThis can be interpreted as a form of alpha blending (Smith 1995), a typical trick in graphics to render transparent materials. It makes sense for our discrete VRE to reduce to alpha blending: our derivation assumes that the volume does not scatter lights, so translucent materials become transparent.\nAgain, Equation 13.16 is the back-to-front equation, and the front-to-back counterpart looks like:\n\\[\n    C = \\sum_{i=0}^{N-1}\\big(C_i\\alpha_i\\prod_{j=0}^{i-1}t_j\\big).\n\\tag{13.17}\\]\nIf you compare the two discrete forms in Equation 13.11 and Equation 13.16, it would appear that the two are not mutually consistent! Of course we know why: 1) Equation 13.16 applies two further approximations (low albedo and Taylor series expansion) and 2) the two \\(C\\) terms in the two equations refer to different physical quantities (compare Equation 13.10 with Equation 13.15).\n\n\n13.3.2 The Second Form is More Flexible\nWhat is the benefit of this new discrete form, comparing Equation 13.12 and Equation 13.17? Both equations can be interpreted as a form of weighted sum, where \\(C_i\\) is weighted by a weight \\(w_i\\), which is \\(\\prod_{j=0}^{i-1}t_j\\) in the first case and \\(\\alpha_i\\prod_{j=0}^{i-1}t_j\\) in the second case. The most obvious difference is that the weights in the first case are correlated but less so in the second case. The weights are strictly decreasing as \\(i\\) increases in the first case, since \\(t_i &lt; 1\\).\nIn the second case, the weights are technically independent. One way to understand this is to observe, in the second case, that \\(w_0 = \\alpha_0\\) and \\(w_{i+1} = w_i \\frac{\\alpha_{i+1}(1-\\alpha_i)}{\\alpha_i}\\), so there is generally a unique assignment of the \\(\\alpha\\) values for a given weight combination. This “flexibility” will come in handy when we can manually assign (Section 13.3.3) or learn the weights (Section 13.4). Note, however, that if we impose the constraint that \\(\\alpha\\in[0, 1]\\), we are effectively constraining the weights too.\n\n\n13.3.3 Visualization is Not (Necessarily) Physically-Based Rendering!\nThese discrete VRE forms might give you the false impression that we have avoided the need to integrate infinitely many paths, because, computationally, the evaluation of the VRE comes down to a single-path summation along the ray trajectory. Not really. Calculating the \\(C_i\\) terms in the new formulations still requires recursion if the results are meant to be physically accurate. Of course we can sidestep this by, e.g., applying the local-illumination approximation, as mentioned before, to avoid recursion.\nScientific visualization offers another opportunity: we can simply assign values to the \\(C\\)s and even the \\(\\alpha\\)s without regard to physics. The goal of visualization is to discover/highlight interesting objects and structures while de-emphasizing things that are irrelevant. So the actual colors are not as important, which gives us great latitude to determine VRE parameters.\n\n\n\n\n\n\nFigure 13.2: Comparing volume rendering for visualization and rendering. (a): two examples of scientific visualization (of CT data) using volume rendering; from Sjschen (2025) and MathiasRav (2009). (b): photorealistic volume rendering (a scene from Disney’s Moana, 2016); from Gnash (2017).\n\n\n\nFigure 13.2 compares volume-rendered images for scientific visualization (a) and for photorealistic rendering (b). In the case of visualization, the data were from CT scans. In both scans, the outer surface is not transparent but is rendered so just because we are interested in seeing the inner structures that are otherwise occluded. The user makes an executive call to assign a very low transparency to the bones in the knee model 0 but a very high transparency value to the skin and other tissues: this is not physically accurate but a good choice for this particular visualization. Photorealistic rendering, in contrast, has to be physically based and does not usually have this flexibility. See figures in Wrenninge and Bin Zafar (2011) and Fong et al. (2017) for more examples.\nThere is volume rendering software that would allow the users to make such an assignment depending on what the user wants to highlight and visualize. With certain constraints and heuristics, one can also procedurally assign the \\(\\alpha\\) and \\(C\\) values from the raw measurement data, usually a density field (see below) acquired from whatever measurement device is used (e.g., CT scanners or MRI machines), using what is called the transfer functions in the literature7. Making an assignment usually is tied to a classification problem: voxels/points of different classes should have different assignments.\n\n\n13.3.4 Density Fields\nPhysically speaking, the medium in RTE/VRE is parameterized by its absorption and scattering coefficients, which are a product of cross section and concentration, which is sometimes also called the density. In physically-based volume rendering, this is indeed how the density is used from the very beginning (Kajiya and Von Herzen 1984; Blinn 1982).\nIn visualization where being physically accurate or photorealistic is unimportant, the notion of an attenuation coefficient8 loses its physical meaning; it is just a number that controls how the brightness of a point weakens. People simply call the attenuation coefficient the density (Kaufman and Mueller 2003), presumably because, intuitively, if the particle density is high the color should be dimmer. If you want to be pedantic, you might say that the attenuation coefficient depends not only on the density/concentration but also on the cross section (Equation 12.2), so how can we do that? Remember in visualization one gets to make an executive call and assign the density value (and thus control \\(\\alpha\\)), so it does not really matter if the value itself means the physical quantity of density/concentration. This is apparent in early work that uses volume rendering for scientific visualization (Sabella 1988; Williams and Max 1992), where attenuation coefficients are nowhere to be found.\nFor this reason, the raw volume data obtained from raw measurement device for scientific (medical) visualization are most often called the density field, even though what is being measured is almost certainly not the density field but a field of optical properties that are related to, but certainly do not equate, density. For instance, the raw data you get from a CT scanner is actually a grid of attenuation coefficients (Bharath 2009, chap. 3).",
    "crumbs": [
      "Rendering",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Rendering Volume and Subsurface Scattering</span>"
    ]
  },
  {
    "objectID": "rendering-rte.html#sec-chpt-mat-vs-rte-nr",
    "href": "rendering-rte.html#sec-chpt-mat-vs-rte-nr",
    "title": "13  Rendering Volume and Subsurface Scattering",
    "section": "13.4 Discrete VRE in (Neural) Radiance-Field Rendering",
    "text": "13.4 Discrete VRE in (Neural) Radiance-Field Rendering\nThere is another field where the discrete VRE (especially our second form Equation 13.16) is becoming incredibly popular: (neural) radiance-field rendering. The two most representative examples are NeRF (Mildenhall et al. 2021) and 3D Gaussian Splatting (3DGS) (Kerbl et al. 2023). They are fundamentally image-based rendering or light-field rendering (Section 9.2), where they sample the light field of the scene by taking a set of images at different camera poses and learn to reconstruct the underlying light field, from which they can then re-sample a new camera pose and, thus, render the corresponding image as if it was taken at that new camera pose. This is re-branded in the modern era as “novel view synthesis”.\nWe will assume that you have read the two papers above so that we can focus on interpreting these radiance-field methods within the fundamental framework of physically-based rendering. Such a re-interpretation would allow us to better understand where these methods come from, how they have introduced new tweaks to physically-based rendering, and what their limitations might be.\nThe first thing to notice is that NeRF and 3DGS, being essentially a sampling and reconstruction method, use the discrete form of the VRE (mostly Equation 13.16) as the reconstruction function. This means they assume that the radiance is calculated by a single-path summation along the ray trajectory without the need for path tracing and solving the actual RTE/VRE. The reason they can reduce infinite paths to a single-path evaluation is very similar to that in scientific visualization, except now instead of assigning the “color” values \\(C\\) and opacity values \\(\\alpha\\) (or equivalently the density field as discussed above), they train a neural network to directly learn these values.\n\n13.4.1 VRE for Surface Rendering?\nIt is interesting to observe that both NeRF and 3DGS (and the vast majority of their later developments) use the discrete VRE form in Equation 13.16 as their forward model and can evidently do a very good job at rendering opaque surfaces. Is this surprising? Isn’t Equation 13.16 designed to render transparent materials/volumes (alpha blending)?\nFor opaque surfaces, the “ground truth” is the surface rendering equation (Section 10.3), which can be seen as a form of weighted sum, where the BRDF \\(f_r(p, \\os, \\oi)\\) weighs the incident light irradiance \\(L(p, \\oi) \\cos\\theta_i \\text{d}\\oi\\). In theory, the weights are independent of each other: the value of \\(f_r(p, \\os, \\oi)\\) at different \\(\\oi\\) can technically be completely uncorrelated. But in reality they are most likely somewhat correlated for real materials: the appearance of a material does not change dramatically when the incident light direction changes slightly. For volumes/translucent materials, the “ground truth” is the VRE, which you could also say is a weighted sum, although the weights are constrained if the \\(\\alpha\\) values are constrained (e.g., between 0 and 1), which is the case in NeRF and 3DGS training (Section 13.3.2).\nSo effectively, when rendering opaque surfaces, we are using a form of (theoretically) constrained weighted sum to approximate another (practically) constrained weighted sum, and we hope that we can learn the approximation from a large amount of offline samples. The learned parameters (color and opacity of each point) should not be interpreted literally in the physical sense. One advantage of this parameterization is that it could be used to render volumes or translucent materials if needed, where the ground truth is the VRE, in which case the learned parameters might be more amenable to physical interpretations.\n\n\n13.4.2 Volume Graphics vs. Point-Based Graphics\nRelated to volume graphics, there is also a subtly different branch of graphics called point-based graphics (PBG) (Levoy and Whitted 1985; Gross and Pfister 2011). The boundary is somewhat blurred, but given the way the two terms are usually used, we can observe a few similarities and distinctions. Both volume graphics and PBG use discrete points as the rendering primitives (as opposed to continuous surfaces such as a mesh), although the input points in volume graphics are usually placed on uniform grids (Engel et al. 2006, chap. 1.5.2) whereas points in PBG can be spatially arbitrary.\nTraditionally, PBG is almost exclusively used for photorealistic rendering of surfaces. In fact, the points used in PBG are usually acquired as samples on continuous surfaces (Gross and Pfister 2011, chap. 3). PBG usually uses object-order rendering through splatting, although ray casting is used too, but RTE/VRE is not involved in the rendering process (Gross and Pfister 2011, chap. 6).\nIn contrast, the use of volume graphics is much broader. Volume rendering can be used for photorealistic rendering of participating media and translucent surfaces (by solving the RTE/VRE), or it can be used for non-photorealistic data visualization (by evaluating the single-path, discrete VRE), at which point whether the object to be rendered is called a participating medium, a translucent surface, or anything else is irrelevant, because visualization does not care much about being physically accurate.\n3DGS is a somewhat interesting case. It is largely a form of PBG because the rendering primitives are unaligned surface samples, and its splatting technique (which we will discuss shortly) resembles that developed in the PBG literature (Gross and Pfister 2011, chap. 6.1). However, 3DGS does use the discrete VRE as the forward model. Again, as discussed just above, VRE is just a way for 3DGS to parameterize its forward mode, so the comparison with traditional volume graphics and PBG should not be taken literally.\n\n\n13.4.3 Splatting is Signal Filtering\nSplatting, initially proposed by Westover (1990) for visualizing volume data, is a common rendering technique used in PBG and 3DGS-family models. We discuss what splatting is, why it works, and how it is used in NeRF and 3DGS. We start by asking: how can we render continuous surfaces from discrete points? If we directly project the points to the sensor plane, we obviously will get holes, as shown in Figure 13.3 (a). This is, of course, not an issue if the rendering primitives are meshes (or procedurally-generated surfaces).\n\n\n\n\n\n\nFigure 13.3: (a): directly projecting discrete points to the image plane would create holes in the rendered image. (b): in splatting, each point is associated with a splat or a footprint function, which can distribute the color of the point to a spatial region on the image plane. (c): splatting essentially allows signal interpolation, which amounts to first reconstructing the underlying signal from the samples (with potential anti-aliasing filtering) followed by re-sampling at new, desired positions.\n\n\n\nThe key is to realize that “meshless” does not mean surfaceless: the fact that we do not have a mesh as the rendering primitives does not mean the surface does not exist. Recall that the points used by PBG are actually samples on the surface. To render an image pixel is essentially to estimate the color of a surface point that projects to the pixel (ignoring supersampling for now). From a signal processing perspective, this is a classic problem of signal filtering: reconstruction and resampling.\nThat is, ideally what we need to do is to reconstruct the underlying signal, i.e., the color distribution of the continuous surface9 (combined with an anti-aliasing pre-filter10) and then resample the reconstructed/filtered signal at positions corresponding to pixels in an image. This is shown in Figure 13.3 (c). %This amounts to applying a single composite filter \\(F\\) at new sampling positions, and The name of the game is to design proper filters. The issue of signal sampling, reconstruction, and resampling is absolutely fundamental to all forms of photorealistic rendering and not limited to PBG; Pharr, Jakob, and Humphreys (2023, chap. 8) and Glassner (1995, Unit II) are great references.\nAnother way to think of this is that the color of a surface point is very likely related to its nearby points that have been sampled as part of the rendering input, so one straightforward thing to do is to interpolate from those samples to calculate colors of new surface points. Signal interpolation is essentially signal filtering/convolution.\nThere is one catch. In classic signal sampling theories (think of the Nyquist-Shannon sampling theorem), samples are uniformly taken and, as a result, we can use a single reconstruction filter. But in PBG the surface samples are non-uniformly taken, so a single reconstruction filter would not work. Instead, we need a different filter for each point. The filter in the PBG parlance is called a splat, or a footprint function; it is associated with each point (surface sample) and essentially distributes the point color to a local region, enabling signal interpolation. This is shown in Figure 13.3 (b). The exact forms of the footprint functions would determine the exact forms of the signal filters. Gaussian filters are particularly common, and Gaussian splatting is a splatting method that uses Gaussian filters (Greene and Heckbert 1986; Heckbert 1989; Zwicker et al. 2001b).\nFrom a 3D modeling perspective, instead of having a continuous mesh, the scene is now represented by a set of discrete points, each of which is represented by a 2D Gaussian distribution, which is called a surface element or a surfel (Pfister et al. 2000). Each surfel is then projected to the screen space to generate a splat as the corresponding reconstruction kernel of that surface point; the reconstruction kernel, after projection, is another Gaussian filter (which can be cascaded with an anti-aliasing pre-filter). The color of each screen space point is then calculated by summing over all the splats (each of which is, of course, scaled by the color of the sample), essentially taking a weighted sum of the colors of the neighboring surface samples (see Zwicker et al. (2001b, fig. 3) for a visualization) or, in signal processing parlance, resampling the reconstructed signal with the reconstruction kernels.\nThis rendering process is traditionally called surface splatting (Zwicker et al. 2001b). Yifan et al. (2019) is an early attempt to learn the surfels through a differential surface splatting process. Surface splatting is a reasonable rendering model in PBG: the “ground truth” in rendering surfaces is the rendering equation, which can also be interpreted as a weighted sum.\nOne can also apply the same splatting idea to volume rendering. In this case, each point in the scene is represented by a 3D Gaussian distribution, which is projected to a 2D splat in the screen space. The color of a pixel is then calculated through, critically, alpha blending the corresponding splats, not weighted sum. This is called volume splatting (Zwicker et al. 2001a). 3DGS can be seen as a differential variant of traditional volume splatting, even though it is also very effective in rendering opaque surfaces (and we have discussed the reason on Section 13.4.1).",
    "crumbs": [
      "Rendering",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Rendering Volume and Subsurface Scattering</span>"
    ]
  },
  {
    "objectID": "rendering-rte.html#sec-chpt-mat-vs-sca-bssrdf",
    "href": "rendering-rte.html#sec-chpt-mat-vs-sca-bssrdf",
    "title": "13  Rendering Volume and Subsurface Scattering",
    "section": "13.5 Integrating Surface Scattering with Volume Scattering",
    "text": "13.5 Integrating Surface Scattering with Volume Scattering\nThe rendering equation governs the surface scattering or light transport in space, and the RTE/VRE governs the volume/subsurface scattering or light transport in a medium. Both processes can be involved in a real-life scene. For instance, the appearance of a translucent material like a paint or a wax is a combination of both forms of scattering/light transport (Figure 7.1). Another example would be rendering smoke against a wall.\n\nConceptually nothing new needs to be introduced to deal with the two forms of light transport together. Say we have an opaque surface (a wall) located with a volume (smoke) in the scene. If we want to calculate the radiance of a ray leaving a point on the wall, we would evaluate the rendering equation there, and for each incident ray, we might have to evaluate the VRE since that ray might come from the volume. In practice it amounts to extending the path tracing algorithm to account for the fact that a path might go through a volume and bounce off between surface points. See Pharr, Jakob, and Humphreys (2023, chap. 14.2) and Fong et al. (2017, Sect. 3) for detailed discussions.\nAnother approach, which is perhaps more common when dealing with translucent materials (whose appearance, of course, depends on both the surface and subsurface scattering), is through a phenomenological model based on the notion of Bidirectional Scattering Surface Reflectance Distribution Function (BSSRDF) (Nicodemus et al. 1977). The BSSRDF is parameterized as \\(f_s(p_s, \\os, p_i, \\oi)\\), describing the infinitesimal outgoing radiance at \\(p_s\\) toward \\(\\os\\) given the infinitesimal power incident on \\(p_i\\) from the direction \\(\\oi\\):\n\\[\n    f_s(p_s, \\os, p_i, \\oi) = \\frac{\\text{d}L(p_s, \\os)}{\\text{d}\\Phi(p_i, \\oi)}.\n\\tag{13.18}\\]\nBSSRDF can be seen as an extension of BRDF in that it considers the possibility that the radiance of a ray leaving \\(p_s\\) could be influenced by a ray incident on another point \\(p_i\\) due to SSS/volume scattering. Given the BSSRDF, the rendering equation can be generalized to:\n\\[\n    L(p_o, \\os) = \\int^A\\int^{\\Omega=2\\pi} f_s(p_s, \\os, p_i, \\oi) L(p_i, \\oi) \\cos\\theta_i \\doi \\d A,\n\\tag{13.19}\\]\nwhere \\(L(p_o, \\os)\\) is the outgoing radiance at \\(p_o\\) toward \\(\\os\\), \\(L(p_i, \\oi)\\) is the incident radiance at \\(p_i\\) from \\(\\oi\\), \\(A\\) in the outer integral is the surface area that is under illumination, and \\(\\Omega=2\\pi\\) means that each surface point receives illumination from the entire hemisphere.\nWe can again use path tracing and Monte Carlo integration to evaluate Equation 13.19 if we know the BSSRDF, which can, again, either be analytically derived given certain constraints and assumptions or measured (Frisvad et al. 2020). To analytically derive it, one has to consider the fact that the transfer of energy from an incident ray to an outgoing ray is the consequence of a cascade of three factors: two surface scattering (refraction) factors, one entering the material surface \\(p_i\\) from \\(\\oi\\) and the other leaving the material surface at \\(p_i\\) toward \\(p_o\\), and a volume scattering factor that accounts for the subsurface scattering between the incident ray at \\(p_i\\) and the exiting ray at \\(p_o\\) (Pharr, Jakob, and Humphreys 2018, chap. 11.4). If all three factors have an analytical form, the final BSSRDF has an analytical form too. This is the approach that, for instance, Jensen et al. (2001) takes.\n\n\n\n\nBharath, Anil. 2009. Introductory Medical Imaging. Morgan & Claypool.\n\n\nBlinn, James F. 1982. “Light Reflection Functions for Simulation of Clouds and Dusty Surfaces.” Acm SIGGRAPH Computer Graphics 16 (3): 21–29.\n\n\nBoas, David A, Dana H Brooks, Eric L Miller, Charles A DiMarzio, Misha Kilmer, Richard J Gaudette, and Quan Zhang. 2001. “Imaging the Body with Diffuse Optical Tomography.” IEEE Signal Processing Magazine 18 (6): 57–75.\n\n\nBohren, Craig F, and Eugene E Clothiaux. 2006. Fundamentals of Atmospheric Radiation: An Introduction with 400 Problems. John Wiley & Sons.\n\n\nChandrasekhar, Subrahmanyan. 1960. Radiative Transfer. Courier Corporation.\n\n\nDong, Yue, Stephen Lin, Baining Guo, et al. 2013. Material Appearance Modeling: A Data-Coherent Approach. Springer.\n\n\nEason, G, AR Veitch, RM Nisbet, and FW Turnbull. 1978. “The Theory of the Back-Scattering of Light by Blood.” Journal of Physics D: Applied Physics 11 (10): 1463.\n\n\nEngel, Klaus, Markus Hadwiger, Joe M Kniss, Christof Rezk-Salama, and Daniel Weiskopf. 2006. Real-Time Volume Graphics. A K Peters, Ltd.\n\n\nFarrell, Thomas J, Michael S Patterson, and Brian Wilson. 1992. “A Diffusion Theory Model of Spatially Resolved, Steady-State Diffuse Reflectance for the Noninvasive Determination of Tissue Optical Properties in Vivo.” Medical Physics 19 (4): 879–88.\n\n\nFong, Julian, Magnus Wrenninge, Christopher Kulla, and Ralf Habel. 2017. “Production Volume Rendering: Siggraph 2017 Course.” In ACM SIGGRAPH 2017 Courses, 1–97.\n\n\nFrisvad, Jeppe Revall, Soeren A Jensen, Jonas Skovlund Madsen, António Correia, Li Yang, Søren Kimmer Schou Gregersen, Youri Meuret, and P-E Hansen. 2020. “Survey of Models for Acquiring the Optical Properties of Translucent Materials.” In Computer Graphics Forum, 39:729–55. 2. Wiley Online Library.\n\n\nGlassner, Andrew S. 1995. Principles of Digital Image Synthesis. Elsevier.\n\n\nGnash. 2017. “Rendering of the Extremely Large Telescope from 2009; CC BY-SA 4.0 license.” https://commons.wikimedia.org/wiki/File:Latest_Rendering_of_the_E-ELT.jpg.\n\n\nGreene, Ned, and Paul S Heckbert. 1986. “Creating Raster Omnimax Images from Multiple Perspective Views Using the Elliptical Weighted Average Filter.” IEEE Computer Graphics and Applications 6 (6): 21–27.\n\n\nGross, Markus, and Hanspeter Pfister. 2011. Point-Based Graphics. Elsevier.\n\n\nHeckbert, Paul S. 1989. “Fundamentals of Texture Mapping and Image Warping. Master’s Thesis.” University of California, Berkeley.\n\n\nIshimaru, Akira. 1977. “Theory and Application of Wave Propagation and Scattering in Random Media.” Proceedings of the IEEE 65 (7): 1030–61.\n\n\nIshimaru, Akira et al. 1978. Wave Propagation and Scattering in Random Media. Vol. 2. Academic press New York.\n\n\nJensen, Henrik Wann, Stephen R Marschner, Marc Levoy, and Pat Hanrahan. 2001. “A Practical Model for Subsurface Light Transport.” ACM SIGGRAPH Computer Graphics, 511–18.\n\n\nKajiya, James T, and Brian P Von Herzen. 1984. “Ray Tracing Volume Densities.” ACM SIGGRAPH Computer Graphics 18 (3): 165–74.\n\n\nKaufman, Arie, and Klaus Mueller. 2003. “Volume Visualization and Volume Graphics.” Technical Report; Stony Brook University.\n\n\nKerbl, Bernhard, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 2023. “3d Gaussian Splatting for Real-Time Radiance Field Rendering.” ACM Trans. Graph. 42 (4): 139–31.\n\n\nKubelka, Paul. 1948. “New Contributions to the Optics of Intensely Light-Scattering Materials. Part i.” Josa 38 (5): 448–57.\n\n\nKubelka, Paul, and Franz Munk. 1931a. “An Article on Optics of Paint Layers (Translated by Stephen h. Westin).” Z. Tech. Phys 12 (593-601): 259–74.\n\n\n———. 1931b. “Ein Beitrag Zur Optik Der Farbanstriche.” Z. Tech. Phys 12:593–601.\n\n\nLevoy, Marc. 1988. “Display of Surfaces from Volume Data.” IEEE Computer Graphics and Applications 8 (3): 29–37.\n\n\nLevoy, Marc, and Turner Whitted. 1985. “The Use of Points as a Display Primitive.” Technical Report; University of North Carolina at Chapel Hill.\n\n\nMathiasRav. 2009. “Volume rendering of a mouse skull (CT) using shear warp algorithm; CC BY-SA 3.0 license.” https://en.wikipedia.org/wiki/File:VolRenderShearWarp.gif.\n\n\nMax, Nelson. 1995. “Optical Models for Direct Volume Rendering.” IEEE Transactions on Visualization and Computer Graphics 1 (2): 99–108.\n\n\nMildenhall, Ben, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. 2021. “Nerf: Representing Scenes as Neural Radiance Fields for View Synthesis.” Communications of the ACM 65 (1): 99–106.\n\n\nNicodemus, FE, JC Richmond, JJ Hsia, IW Ginsberg, and T Limperis. 1977. Geometrical Considerations and Nomenclature for Reflectance. Vol. 160. US Department of Commerce, National Bureau of Standards Washington, DC, USA.\n\n\nPfister, Hanspeter, Matthias Zwicker, Jeroen Van Baar, and Markus Gross. 2000. “Surfels: Surface Elements as Rendering Primitives.” In Proceedings of the 27th Annual Conference on Computer Graphics and Interactive Techniques, 335–42.\n\n\nPharr, Matt, Wenzel Jakob, and Greg Humphreys. 2018. Physically Based Rendering: From Theory to Implementation. 3rd ed. MIT Press.\n\n\n———. 2023. Physically Based Rendering: From Theory to Implementation. 4th ed. MIT Press.\n\n\nSabella, Paolo. 1988. “A Rendering Algorithm for Visualizing 3D Scalar Fields.” ACM SIGGRAPH Computer Graphics 22 (4): 51–58.\n\n\nSchweiger, Martin, SR Arridge, M Hiraoka, and DT Delpy. 1995. “The Finite Element Method for the Propagation of Light in Scattering Media: Boundary and Source Conditions.” Medical Physics 22 (11): 1779–92.\n\n\nSjschen. 2025. “Volume rendered CT scan of a forearm with different color schemes for muscle, fat, bone, and blood; released into the public domain by the copyright holder.” https://commons.wikimedia.org/wiki/File:CTWristImage.png.\n\n\nSmith, Alvy Ray. 1995. “Alpha and the History of Digital Compositing.” Citeseer.\n\n\nStam, Jos. 1995. “Multiple Scattering as a Diffusion Process.” In Rendering Techniques’ 95: Proceedings of the Eurographics Workshop in Dublin, Ireland, June 12–14, 1995 6, 41–50. Springer.\n\n\nWestover, Lee. 1990. “Footprint Evaluation for Volume Rendering.” In Proceedings of the 17th Annual Conference on Computer Graphics and Interactive Techniques, 367–76.\n\n\nWilliams, Peter L, and Nelson Max. 1992. “A Volume Density Optical Model.” In Proceedings of the 1992 Workshop on Volume Visualization, 61–68.\n\n\nWrenninge, Magnus, and Nafees Bin Zafar. 2011. “Production Volume Rendering: Siggraph 2011 Course.” In ACM SIGGRAPH 2011 Courses, 1–71.\n\n\nYifan, Wang, Felice Serena, Shihao Wu, Cengiz Öztireli, and Olga Sorkine-Hornung. 2019. “Differentiable Surface Splatting for Point-Based Geometry Processing.” ACM Transactions On Graphics (TOG) 38 (6): 1–14.\n\n\nZwicker, Matthias, Hanspeter Pfister, Jeroen Van Baar, and Markus Gross. 2001a. “EWA Volume Splatting.” In Proceedings Visualization, 2001. VIS’01., 29–538. IEEE.\n\n\n———. 2001b. “Surface Splatting.” In Proceedings of the 28th Annual Conference on Computer Graphics and Interactive Techniques, 371–78.",
    "crumbs": [
      "Rendering",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Rendering Volume and Subsurface Scattering</span>"
    ]
  },
  {
    "objectID": "rendering-rte.html#footnotes",
    "href": "rendering-rte.html#footnotes",
    "title": "13  Rendering Volume and Subsurface Scattering",
    "section": "",
    "text": "Subrahmanyan Chandrasekhar won the Nobel Prize in physics in 1983 (not for the RTE).↩︎\nTechnically, even single scattering can lead to augmentation if there is illumination coming from anywhere outside the ray direction.↩︎\nSome definitions do not include emission in the source term, while in other definitions the source term is what is defined here divided by \\(\\sigma_t\\).↩︎\nA subtlety you might have noticed is that not all the out-scattering of \\(L(p, \\os)\\) attenuates the radiance; some of the scattering could be toward \\(\\os\\) so should augment the radiance. This is not a concern since our augmentation term Equation 13.5 integrates over the entire sphere, so it considers \\(L(p, \\os)\\) again as part of in-scattering and accounts for the forward-scattered portion of \\(L(p, \\os)\\).↩︎\nThere are two alternative parameterizations, both of which are common in graphics literature. The first (Pharr, Jakob, and Humphreys 2023) is to express \\(p_0 = p + s\\os\\) (\\(s\\) being positive), but then the initial radiance would have to be expressed as \\(L(p_0, -\\os)\\), since \\(\\os\\) now points from \\(p\\) to \\(p_0\\). The other is to express \\(p_0 = p - s\\os\\) (\\(s\\) again being positive) (Fong et al. 2017); this avoids the need to switch directions but uses a negative sign. It is a matter of taste which one to use, but be alert to the different conventions.↩︎\ntechnically it is the contribution of each small segment between two discrete points because of the Reimann sum.↩︎\nSome (color) transfer functions could have physical underpinnings, such as applying a single-scattering shading algorithms (i.e., local illumination); see, e.g., Levoy (1988, Sect. 3) or Max (1995, Sect. 5), but the goal there is not to precisely model physics but for better, subjective visualization.↩︎\nwhich is the only coefficient needed and which participates in calculating \\(\\alpha\\) (Equation 13.11).↩︎\nassuming a diffuse surface so we care to reconstruct the color of each point, not the radiance of each ray.↩︎\nThe compound filter combining reconstruction and anti-aliasing filters is sometimes also called a resampling filter, because the compound filter is used during resampling to calculate the new sample values.↩︎",
    "crumbs": [
      "Rendering",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Rendering Volume and Subsurface Scattering</span>"
    ]
  },
  {
    "objectID": "rendering-nflux.html",
    "href": "rendering-nflux.html",
    "title": "14  The N-Flux Theory",
    "section": "",
    "text": "14.1 The Kubelka–Munk Model\nAs discussed in Section 13.1, the general RTE is difficult to solve, and there are two general strategies. Section 13.2 discusses one strategy that numerically approximates the solution using Monte Carlo methods. Another strategy is to make some simplified assumptions and/or apply additional constraints, which would allow us to derive analytical solutions. The most common form of this strategy is called the N-flux or N-stream theory. This chapter will start from the case where \\(N=2\\), which gives us the Kubelka–Munk Model (Section 14.1), and then extend the theory to the general case (Section 14.2).\nKubelka and Munk, two Czechoslovakian chemists, built a phenomenological model that estimates the spectral reflectance/transmittance of a material (Kubelka and Munk 1931b, 1931a; Kubelka 1948). Their model cares only about the hemispherical-hemispherical reflectance (Section 10.2), i.e., the ratio of total flux scattered upward to the hemisphere to the total toward flux incident from the hemisphere. The K-M model also considers that the material under modeling is in immediate contact with a Lambertian substrate that reflects light uniformly over all directions.  The K-M model is the most aggressive form of simplification to the RTE that, nevertheless, is very widely used, especially in the printing, painting, and dye industry (and to some extent in graphics).\nWe will go through an excruciatingly long derivation, but the intuitions behind the derivation are exactly the same as that of the RTE, because the K-M model is a simplification of the RTE. The derivation allows us to make clear what simplifications we have made and, thus, when the model is and is not applicable.",
    "crumbs": [
      "Rendering",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>The N-Flux Theory</span>"
    ]
  },
  {
    "objectID": "rendering-nflux.html#sec-chpt-mat-vs-km",
    "href": "rendering-nflux.html#sec-chpt-mat-vs-km",
    "title": "14  The N-Flux Theory",
    "section": "",
    "text": "14.1.1 Deriving the Model\nFigure 14.1 illustrates the setup that we will use to derive the model. The first important assumption that the K-M model makes is that the every point on the material surface receives exactly the same irradiance and that the material itself is homogeneous, consisting of particles that are statistically randomly distributed and oriented. As a result, there is no difference between different positions at the same depth anywhere inside the material. Of course the radiation fields at different depths are different; at the very least, the deeper you go, the fewer photons there are due to absorption. Another way to think of this is that we are intentionally limiting our consideration to only a small area where there is no spatial difference at the same depth, so we can analyze information only at different depths rather than different positions at the same depth.\n\n\n\n\n\n\nFigure 14.1: The setup of deriving the K-M model. We focus on the change of downward irradiance (\\(E_{\\downarrow}\\)) and upward irradiance (\\(E_{\\uparrow}\\)) as they travel a thin layer \\(\\D x\\). We make the assumption that the material is an isotropic medium and is homogeneous spatially.\n\n\n\nWe focus on a very thin layer \\(\\D x\\); since there are no differences between horizontal positions, we can arbitrarily pick a point at depth \\(0 \\leq x \\leq X\\) for analysis, where \\(X\\) is the total depth (the material surface has \\(x=0\\) and the material bottom has \\(x=X\\)). The point at \\(x\\) receives photons from all directions. Let’s say all the downward photons (going to the lower hemisphere) have a total irradiance of \\(E_{\\downarrow}(x)\\) and all the upward photons (going to the upper hemisphere) have a total irradiance of \\(E_{\\uparrow}(x)\\). The (hemispherical-hemispherical) reflectance at \\(x\\) is then the ratio of the two:\n\\[\n    R(x) = \\frac{E_{\\uparrow}(x)}{E_{\\downarrow}(x)},\n\\]\nand the reflectance at the material surface (which is what the K-M model is interested in calculating) is \\(R(0)\\).\nHow do we express \\(E_{\\downarrow}(x)\\)? As usual, we set a differential equation to describe the change of \\(E_{\\downarrow}(x)\\). Consider the following conservation of energy when \\(E_{\\downarrow}(x)\\) goes through the thin layer \\(\\D x\\):\n\\[\n\\begin{aligned}\n    E_{\\downarrow}(x + \\D x) - E_{\\downarrow}(x) = & - \\int^{S^2_+}\\sigma_a(x, \\omega) \\D x L(x, \\omega) \\do \\\\\n    & - \\int^{S^2_+}\\int^{S^2_+}\\sigma_s(x, \\omega) f_p(x, \\omega', \\omega) \\D x L(x, \\omega) \\do' \\do \\\\\n    & + \\int^{S^2_-}\\int^{S^2_-}\\sigma_s(x + \\D x, \\omega) f_p(x + \\D x, \\omega', \\omega) \\D x L(x + \\D x, \\omega) \\do' \\do,\n\\end{aligned}\n\\tag{14.1}\\]\nwhere \\(S^2_+\\) and \\(S^2_-\\) represent the upper and lower hemispheres, respectively; \\(L(x, \\omega)\\) is the radiance coming from direction \\(\\omega\\) incident on a point of depth \\(x\\), \\(f_p(x, \\omega', \\omega)\\) is the phase function between the incident direction \\(\\omega\\) and outgoing direction \\(\\omega'\\), and \\(\\sigma_s(x, \\omega)\\) is the scattering coefficient at \\(x\\) for an incident direction \\(\\omega\\) (Section 12.2.2).\nThe interpretation of this equation is exactly the same as that of the RTE. The equation essentially says that the downward irradiance after traveling \\(\\D x\\) is (omitting emission):\n\nreduced by absorption (the first negative term in Equation 14.1), which is the absorption component in the RTE;\nreduced by upward scattering (the second negative term in Equation 14.1), which is the out-scattering component in the RTE;\nincreased by the downward-scattering of upward irradiance reaching the bottom of the thin layer (the positive term in Equation 14.1), which is the in-scattering component in RTE.\n\nNow let’s take a closer look at the absorption term in Equation 14.1 and re-write it:\n\\[\n    \\int^{S^2_+}\\sigma_a(x, \\omega) \\D x L(x, \\omega) \\do = \\sigma_a(x, \\bar{\\omega}) \\D x \\int^{S^2_+} L(x, \\omega) \\do.\n\\tag{14.2}\\]\nEquation 14.2 is derived based on the mean-value theorem1 in integral calculus, which says that there exists some value of \\(\\bar{\\omega}\\in S^2_+\\) that would allow us to take the \\(\\sigma_a\\) term out of the integral. Once we do that, the integral on the right-hand side of Equation 14.2 is simply the total downward irradiance, which we denote \\(E_{\\downarrow}(x)\\).\nIf we further assume that the material absorption is isotropic, then \\(\\sigma_a(x, \\bar{\\omega})\\) is independent of \\(\\bar{\\omega}\\) and is simply a function of \\(x\\), so we write it as \\(K_{\\downarrow}(x)\\), which allows us to re-write Equation 14.2 as:\n\\[\n    \\int^{S^2_+}\\sigma_a(x, \\omega) \\D x L(x, \\omega) \\do = K_{\\downarrow}(x) \\D x E_{\\downarrow}(x).\n\\tag{14.3}\\]\n\\(K_{\\downarrow}(x)\\) is a phenomenological coefficient, but it is related to the fundamental absorption coefficient \\(\\sigma_a\\) and, thus, carries physical meanings. This is clear from Equation 14.4:\n\\[\n    K_{\\downarrow}(x) = \\frac{\\int^{S^2_+}\\sigma_a(x, \\omega) L(x, \\omega) \\do}{\\D x E_{\\downarrow}(x)}.\n\\tag{14.4}\\]\n\\(K_{\\downarrow}(x)\\), by definition, represents the fraction of downward irradiance that is absorbed per unit length. Therefore, \\(K_{\\downarrow}(x)\\) can be intuitively interpreted as the effective downward absorption coefficient at depth \\(x\\). Note that while the mean-value theorem tells us that some \\(\\bar{\\omega}\\in S^2_+\\) exists, it does not tell us what its values is. In reality, \\(K_{\\downarrow}(x)\\) will very much depend on \\(L(x, \\omega)\\).\nSimilarly, we can rewrite the upward scattering term in Equation 14.1 by first rearranging the terms:\n\\[\n\\begin{aligned}\n    &\\int^{S^2_+}\\int^{S^2_+}\\sigma_s(x, \\omega) f_p(x, \\omega', \\omega) \\D x L(x, \\omega) \\do' \\do \\\\\n    = &\\int^{S^2_+}\\Big(\\int^{S^2_+}\\sigma_s(x, \\omega) f_p(x, \\omega', \\omega) \\do'\\Big) \\D x L(x, \\omega) \\do.\n\\end{aligned}\n\\tag{14.5}\\]\nWe then invoke the mean-value theorem again, which says that there exists \\(\\bar{\\omega}\\in S^2_+\\) that allows us to write the upward scattering term as:\n\\[\n    (\\int^{S^2_+}\\sigma_s(x, \\bar{\\omega}) f_p(x, \\omega', \\bar{\\omega}) \\do' \\D x) \\int^{S^2_+}L(x, \\omega) \\do.\n\\tag{14.6}\\]\nWe use \\(S_{\\downarrow\\uparrow}(x)\\) to denote the first integral in Equation 14.6: \\(S_{\\downarrow\\uparrow}(x) = \\int^{S^2_+}\\sigma_s(x, \\bar{\\omega}) f_p(x, \\omega', \\bar{\\omega}) \\do'\\), which means the upward scattering term can be simplied to:\n\\[\n    S_{\\downarrow\\uparrow}(x) \\D x E_{\\downarrow}(x).\n\\tag{14.7}\\]\nCombining Equation 14.5 and Equation 14.7, we get: \\[\n    S_{\\downarrow\\uparrow}(x) = \\frac{\\int^{S^2_+}\\int^{S^2_+}\\sigma_s(x, \\omega) f_p(x, \\omega', \\omega) L(x, \\omega) \\do' \\do}{\\D x E_{\\downarrow}(x)}.\n\\tag{14.8}\\]\n\\(S_{\\downarrow\\uparrow}(x)\\) is again a phenomenological coefficient that is related to the fundamental scattering coefficient. Its physical interpretation is clear from Equation 14.8: it is the fraction of the downward irradiance that is scattered upward per unit length. We can think of it as the effective “upward scattering coefficient of the downward irradiance”.\nFinally, we can do the same thing to the downward scattering term in Equation 14.1:\n\\[\n    \\int^{S^2_-}\\int^{S^2_-}\\sigma_s(x + \\D x, \\omega) f_p(x + \\D x, \\omega', \\omega) \\D x L(x + \\D x, \\omega) \\do' \\do\n\\tag{14.9}\\]\nby first invoking the mean-value theorem and re-express it as:\n\\[\n    (\\int^{S^2_-}\\sigma_s(x + \\D x, \\hat{\\omega}) f_p(x + \\D x, \\omega', \\hat{\\omega}) \\do' \\D x) \\int^{S^2_-}L(x + \\D x, \\omega) \\do.\n\\tag{14.10}\\]\nWe use \\(S_{\\uparrow\\downarrow}(x)\\) to denote the first integral in Equation 14.10. The second integral in Equation 14.10 is essentially the total upward irradiance at the depth \\(x+\\D x\\), which we denote \\(E_{\\uparrow}(x + \\D x)\\). Therefore, the downward scattering term becomes:\n\\[\n    S_{\\uparrow\\downarrow}(x + \\D x) \\D x E_{\\uparrow}(x + \\D x).\n\\tag{14.11}\\]\nCombining Equation 14.9 and Equation 14.11, we have:\n\\[\n\\begin{aligned}\n    S_{\\uparrow\\downarrow}(x + \\D x) = & \\frac{\\int^{S^2_-}\\int^{S^2_-}\\sigma_s(x + \\D x, \\omega) f_p(x + \\D x, \\omega', \\omega) \\D x L(x + \\D x, \\omega) \\do' \\do}{\\D x E_{\\uparrow}(x + \\D x)},\\\\\n    S_{\\uparrow\\downarrow}(x) = & \\frac{\\int^{S^2_-}\\int^{S^2_-}\\sigma_s(x, \\omega) f_p(x, \\omega', \\omega) \\D x L(x, \\omega) \\do' \\do}{\\D x E_{\\uparrow}(x)}.\n\\end{aligned}\n\\tag{14.12}\\]\nNow plug Equation 14.3, Equation 14.7, and Equation 14.11 back into Equation 14.1, we get:\n\\[\n    E_{\\downarrow}(x + \\D x) = E_{\\downarrow}(x) - K_{\\downarrow}(x) \\D x E_{\\downarrow}(x) - S_{\\downarrow\\uparrow}(x) \\D x E_{\\downarrow}(x) + S_{\\uparrow\\downarrow}(x + \\D x) \\D x E_{\\uparrow}(x + \\D x).\n\\]\nRewrite it and take the limit as \\(\\D x \\rightarrow 0\\):\n\\[\n\\begin{aligned}\n    \\frac{E_{\\downarrow}(x + \\D x) - E_{\\downarrow}(x)}{\\D x} &= - K_{\\downarrow}(x) E_{\\downarrow}(x) - S_{\\downarrow\\uparrow}(x) E_{\\downarrow}(x) + S_{\\uparrow\\downarrow}(x + \\D x) E_{\\uparrow}(x + \\D x),\\\\\n    \\frac{\\d E_{\\downarrow}(x)}{\\d x} &= - K_{\\downarrow}(x) E_{\\downarrow}(x) - S_{\\downarrow\\uparrow}(x) E_{\\downarrow}(x) + S_{\\uparrow\\downarrow}(x) E_{\\uparrow}(x).\n\\end{aligned}\n\\tag{14.13}\\]\nSimilarly we can express the rate of change of the upward irradiance \\(E_{\\uparrow}(x)\\) based on the same energy conservation constraint:\n\\[\n    \\frac{\\d E_{\\uparrow}(x)}{\\d x} = K_{\\uparrow}(x) E_{\\uparrow}(x) + S_{\\uparrow\\downarrow}(x) E_{\\uparrow}(x) - S_{\\downarrow\\uparrow}(x) E_{\\downarrow}(x),\n\\tag{14.14}\\]\nwhere the three terms on the right-hand side, again, represent the absorption, out-scattering, and in-scattering in the original RTE.\nNow a few more assumptions. If we assume that the material absorption is isotropic (the total absorption per unit length is invariant to incident light direction over the entire sphere), we have \\(K_{\\uparrow}(x) = K_{\\downarrow}(x)\\) because:\n\\[\n    K_{\\uparrow}(x) = \\sigma_a(x, \\hat{\\omega}) = \\sigma_a(x, \\bar{\\omega}) = K_{\\downarrow}(x),\n\\tag{14.15}\\]\nfor some \\(\\hat{\\omega} \\in S^2_-\\) and \\(\\bar{\\omega} \\in S^2_+\\).\nIf we further assume that 1) the material is an isotropic scattering medium, then \\(\\sigma_s(x, \\omega)\\) is invariant to \\(\\omega\\) (the total amount of scattering per unit length does not change with the incident light direction over the entire sphere), and 2) the particles are also isotropic scatters (which theoretically do not exist), then \\(f_p(x, \\omega', \\omega)\\) is a constant (\\(\\frac{1}{4\\pi}\\)). Therefore, \\(S_{\\uparrow\\downarrow}(x) = S_{\\downarrow\\uparrow}(x)\\), because:\n\\[\n\\begin{aligned}\n    S_{\\uparrow\\downarrow}(x) &= \\int^{S^2_-}\\sigma_s(x, \\hat{\\omega}) f_p(x, \\omega', \\hat{\\omega}) \\do' \\\\\n    &= \\sigma_s(x, \\hat{\\omega}) \\int^{S^2_-}f_p(x, \\omega', \\hat{\\omega}) \\do' = \\sigma_s(x, \\hat{\\omega})/2 \\\\\n    &= \\sigma_s(x, \\bar{\\omega}) \\int^{S^2_-}f_p(x, \\omega', \\bar{\\omega}) \\do' = \\sigma_s(x, \\bar{\\omega})/2 \\\\\n    &= S_{\\downarrow\\uparrow}(x),\n\\end{aligned}\n\\tag{14.16}\\]\nfor some \\(\\hat{\\omega} \\in S^2_-\\) and \\(\\bar{\\omega} \\in S^2_+\\).\nA few notes on the assumptions here:\n\nThe assumption of isotropic scatters is important; assuming only an isotropic medium is not enough. This is because the integrals in Equation 14.16 integrate only the upper (or lower) hemisphere rather than the entire sphere, so if the phase function itself is not a constant, the integral result will still depend on \\(\\bar{\\omega}\\) or \\(\\hat{\\omega}\\). See the distinction between an isotropic medium and an isotropic scatter on Section 12.2.3.2.\nAlternatively, if the particles are not isotropic scatters, \\(S_{\\uparrow\\downarrow}(x) = S_{\\downarrow\\uparrow}(x)\\) can still hold if we assume that the upward and downward irradiance are both diffuse, i.e., the radiance \\(L(x, \\omega)\\) is invariant to \\(\\omega\\). The isotropic medium assumption still needs to hold. This can be proven by going back to the respective definitions of \\(S_{\\uparrow\\downarrow}(x)\\) and \\(S_{\\downarrow\\uparrow}(x)\\) in Equation 14.8 and Equation 14.122. Interestingly, if the particles are isotropic scatters, the outgoing irradiance will necessarily be diffuse.\nYet another way for \\(S_{\\uparrow\\downarrow}(x) = S_{\\downarrow\\uparrow}(x)\\) to hold is if we are considering a very idealized scenario where photons travel and are scattered only upward or downward (Bohren and Clothiaux 2006, chap. 5.2). If the medium is also isotropic (but does not have to consist of isotropic scatters), the fraction of the upward irradiance turned downward would be the same as the fraction of the downward irradiance turned upward, so \\(S_{\\uparrow\\downarrow}(x) = S_{\\downarrow\\uparrow}(x)\\).\n\nFinally, given the assumption that the material is spatially homogeneous, both \\(\\sigma_a\\), \\(\\sigma_s\\), and \\(f_p\\) are all independent of \\(x\\). Therefore, we can denote \\(K = K_{\\uparrow}(x) = K_{\\downarrow}(x)\\) and \\(S = S_{\\uparrow\\downarrow}(x) = S_{\\downarrow\\uparrow}(x)\\). \\(K\\) and \\(S\\) are simply called the absorption and scattering coefficients, respectively, of the medium, but we now should know that they are of the phenomenological nature and, with all the simplifications above, are derived from the fundamental optical absorption/scattering properties of the medium (see Equation 14.15 and Equation 14.16).\nNow we combine Equation 14.13 and Equation 14.14 and get to the famous pair of differential equations underlying the K-M model:\n\\[\n\\begin{aligned}\n    \\frac{\\d E_{\\downarrow}(x)}{\\d x} &= - (K +S)E_{\\downarrow}(x) + S E_{\\uparrow}(x), \\\\\n    \\frac{\\d E_{\\uparrow}(x)}{\\d x} &= (K + S)E_{\\uparrow}(x) - S E_{\\downarrow}(x).\n\\end{aligned}\n\\tag{14.17}\\]\n\n\n14.1.2 The Model and Its Interpretation\nEquation 14.17 gives us a pair of linear differential equations. What we are interested in solving for is \\(R(0) = \\frac{E_{\\uparrow}(0)}{E_{\\downarrow}(0)}\\). The boundary condition is that \\(R(X) = R_g\\), which is the (assumed-to-be) known reflectance of the substrate. Solving the differential equations gives us:\n\\[\n\\begin{aligned}\n    & R(0) = \\frac{E_{\\uparrow}(0)}{E_{\\downarrow}(0)} = \\frac{1-R_g[a-b\\coth(bSX)]}{a-R_g+b\\coth(bSX)},\\\\\n    & T(X) = \\frac{E_{\\downarrow}(X)}{E_{\\downarrow}(0)} = \\frac{b}{a\\sinh(bSX)+b\\cosh(bSX)},\\\\\n    & a = \\frac{K+S}{S},\\\\\n    & b = \\sqrt{a^2-1},\n\\end{aligned}\n\\tag{14.18}\\]\nwhere \\(R(0)\\) is the hemispherical-hemispherical reflectance at the material surface and \\(T(X)\\) is the hemispherical-hemispherical transmittance at the bottom of the material3; \\(\\coth(\\cdot)\\) is the hyperbolic cotangent function, \\(\\sinh(\\cdot)\\) is the hyperbolic sine function, and \\(\\cosh(\\cdot)\\) is the hyperbolic cosine function. Note that we omit \\(\\lambda\\) for simplicity but keep in mind that \\(R(0), T(X), S, K, R_g, a, b\\) are all spectral quantities.\nEquation 14.18 is the famous K-M model. Consider the case where the material is purely absorptive and scatters little (e.g., dyes on textiles or fabrics), so \\(S\\) is close to 0 and the reflectance is simplified to:\n\\[\n    R(0) = R_g e^{-2KX} = e^{-KX} R_g e^{-KX}.\n\\tag{14.19}\\]\nWe can understand \\(R_X\\) by decomposing it into the product of three terms, each representing a step in the overall, observed reflection. First, photons go through the material from the top down, being absorbed as they go. The percentage of photons that are still left (i.e., unabsorbed) just before they hit the substrate is \\(e^{-KX}\\), which is consistent with the Beer-Lambert law. Second, the substrate reflects \\(R_g\\) amount of light back toward the material. Finally, as the photons make their way back to the surface, they go through another round of absorption governed by the same Beer-Lambert law (Section 12.1.1).\nNow consider the case where there is no substrate or when the substrate is a perfect black substrate; in both cases \\(R_g = 0\\). Reflectance is now:\n\\[\n    R_{black} = \\frac{1}{a+b\\coth(bSX)}.\n\\tag{14.20}\\]\nFinally, consider the case where the material is so thick that no photon reaches the substrate. In this case, the reflectance is not affected by the substrate and can be simplified to (by letting \\(X\\) approach infinity):\n\\[\n    \\lim_{X \\rightarrow \\infty} = R_{\\infty} = \\frac{1}{a+b} = 1 + \\frac{K}{S} - \\sqrt{\\Big(\\frac{K}{S}\\Big)^2 + 2 \\frac{K}{S}}.\n\\tag{14.21}\\]\nIn the painting industry, we say the paint’s “hiding is complete” when the substrate does not influence the material color. We can quantify the “hiding power” of a paint by \\(H = \\frac{R_{white}}{R_{black}}\\), i.e., the ratio of reflectance between when the substrate is black (absorbs everything) and white (reflects everything back). If the hiding is complete, \\(H\\) would be 1.\nYou might be wondering how we know \\(K\\) and \\(S\\) of a material — we measure them. For instance, observe Equation 14.20 and Equation 14.21; we can measure the reflectance \\(R_{black}\\) when the substrate is nearly black and the reflectance \\(R_{\\infty}\\) when the hiding is near complete. We can then solve the system of equations to estimate \\(K\\) and \\(S\\).\nYou can see that we are not actually taking a very thin layer of particles, illuminating it, and then measuring how much of the light is scattered vs. absorbed. Instead, we estimate \\(K\\) and \\(S\\) macroscopically. We have in mind a model (a set of equations or functions, if you will) that is parameterized by unknown variables. We then probe the model by giving it different inputs and measuring the outputs. From the input-output pairs, we can then estimate the unknown parameters. This is why a model so defined and derived is called a phenomenological model and the parameters (e.g., the \\(K\\) and \\(S\\) coefficients) are called the phenomenological parameters — because all we do is to observe the phenomena.\n\n\n14.1.3 K-M Model for Mixture of Materials\nThe basic K-M model can be extended to account for materials that consist of a mixture of materials, each with different absorption/scattering behaviors. This is done by first expressing the overall absorption and scattering coefficients of the mixture and then plugging them into the K-M model.\nSpecifically, if we are mixing \\(N\\) materials, each with a phenomenological absorption and scattering coefficient \\(K_i\\) and \\(S_i\\), the overall absorption and scattering coefficient of the material is:\n\\[\n\\begin{aligned}\n    K &= \\sum_{i=1}^N \\eta_i K_i, \\\\\n    S &= \\sum_{i=1}^N \\eta_i S_i,\n\\end{aligned}\n\\tag{14.22}\\]\nwhere \\(\\eta_i\\) is the volume concentration of the \\(i^{th}\\) material in the overall material mixture and is defined as:\n\\[\n    \\eta_i = \\frac{V_i}{\\sum_{j=1}^N V_j},\n\\]\nwhere \\(V_j\\) is the volume of the \\(j^{th}\\) material. Once we have the \\(K\\) and \\(S\\) coefficients, we simply invoke Equation 14.18 to calculate the reflectance/transmittance of the material mixture.\nWhy would it make sense to calculate the overall \\(K\\) and \\(S\\) using the volume-weighting method in Equation 14.22? Let’s derive it using absorption as an example. The overall absorption coefficient \\(K\\) of the material mixture is also the optical absorption coefficient \\(\\sigma_a\\) of the mixture (given the set of assumptions we have made so far; see Equation 14.15), which is given by Equation 12.11 as:\n\\[\n    K = \\sigma_a = \\sum_i^N c^i\\epsilon^i,\n\\]\nwhere \\(c^i\\) and \\(\\epsilon^i\\) are the number concentration and the absorption cross section of the \\(i^{th}\\) material in the material mixture. Specifically, \\(c^i\\) is defined as:\n\\[\n    c^i = \\frac{n_i}{\\sum_{j=1}^N V_j} = \\frac{V_i}{\\sum_{j=1}^N V_j} \\frac{n_i}{V_i},\n\\tag{14.23}\\]\nwhere \\(n_i\\) is the number of particles of the \\(i^{th}\\) material in the mixture. We assume that when we mix \\(N\\) materials, their volumes add. That is, the volume of the mixture is the sum of that of the individual materials. This is in general true if different materials do not chemically react and if materials do not dissolve into each other. Therefore:\n\\[\n    K = \\sum_i^N c^i\\epsilon^i = \\sum_i^N \\frac{V_i}{\\sum_{j=1}^N V_j} \\frac{n_i}{V_i} \\epsilon^i = \\sum_i^N \\eta_i \\frac{n_i}{V_i} \\epsilon^i.\n\\tag{14.24}\\]\nWe then, again, use the fact (Equation 14.15) that the phenomenological absorption coefficient of the \\(i^{th}\\) material \\(K_i\\) is the same as its optical absorption coefficient \\(\\sigma_{a, i} = c_i \\epsilon_i\\), where:\n\n\\(c_i = \\frac{n_i}{V_i}\\) is the number concentration of the \\(i^{th}\\) material on its own (note the subtle difference of \\(c_i\\) here and \\(c^i\\) in Equation 14.23, which is the number concentration of the \\(i^{th}\\) material in a mixture), and\n\\(\\epsilon_i\\) is just \\(\\epsilon^i\\): they both refer to absorption cross section of the \\(i^{th}\\) material, which does not change whether the material is in a mixture or not.\n\nTherefore: \\[\n    K = \\sum_i^N \\eta_i c_i \\epsilon_i = \\sum_i^N \\eta_i K_i,\n\\tag{14.25}\\]\nwhich is exactly Equation 14.22.\n\n\n14.1.4 Correction for Surface Reflection\nOne thing that is ignored in the K-M model is the surface reflection/refraction yet. Part of the photons will be reflected away at the air/material interface before they enter the material. Similarly, when we consider the transmittance, we have ignored the reflection/refraction at the other side of the material. So the reflectance and transmittance calculated by the K-M model are defined at the point when the photons are just about to leave the material.\nTo account for the surface phenomena, we can apply what is called the Saunderson correction, derived by Saunderson (1942). See Sharma (2003, chap. 3.6.3) for a derivation, but briefly, if the illumination is diffuse, the corrected surface reflectance is:\n\\[\n    R = r_s + \\frac{(1-r_s)(1-r_i)~R(0)}{1-r_i~R(0)},\n\\]\nwhere \\(R(0)\\) is the reflectance given by the K-M model, \\(r_s\\) is the fraction of incident irradiance scattered by the air-material surface, and \\(r_i\\) is the fraction of the internal irradiance approaching the air-material interface that is scattered back by the interface. Assuming a smooth surface, both \\(r_s\\) and \\(r_i\\) can be calculated by the Fresnel equations given the refractive index of the material (Section 11.1).",
    "crumbs": [
      "Rendering",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>The N-Flux Theory</span>"
    ]
  },
  {
    "objectID": "rendering-nflux.html#sec-chpt-mat-vs-km-nflux",
    "href": "rendering-nflux.html#sec-chpt-mat-vs-km-nflux",
    "title": "14  The N-Flux Theory",
    "section": "14.2 The N-Flux Model",
    "text": "14.2 The N-Flux Model\nThe basic K-M model is called the two-stream or two-flux model, because it considers only the total upward irradiance and total downward irradiance. We basically have divided all the directions possible in the space into only two solid angles, the upper hemisphere and the lower hemisphere. What if we want to know the irradiance at a finer granularity (i.e., over a smaller solid angle)? We divide all the directions into more solid angles and analyze the irradiance change in them using a similar method.\nFor instance, if we now consider irradiance in four directions: \\(E_{\\nwarrow}(x)\\), \\(E_{\\nearrow}(x)\\), \\(E_{\\searrow}(x)\\), and \\(E_{\\swarrow}(x)\\), we can write the change of the irradiance in \\(E_{\\searrow}(x)\\) as:\n\\[\n\\begin{aligned}\n    \\frac{\\d E_{\\searrow}(x)}{\\d x} = & - K_{\\searrow}(x) E_{\\searrow}(x) \\nonumber \\\\\n    & + S_{\\nwarrow\\searrow}(x) E_{\\nwarrow}(x) + S_{\\nearrow\\searrow}(x) E_{\\nearrow}(x) + S_{\\swarrow\\searrow}(x) E_{\\swarrow}(x) \\nonumber \\\\\n    & - S_{\\searrow\\nwarrow}(x) E_{\\searrow}(x) - S_{\\searrow\\nearrow}(x) E_{\\searrow}(x) - S_{\\searrow\\swarrow}(x) E_{\\searrow}(x),\n\\end{aligned}\n\\tag{14.26}\\]\nwhere \\(S_{\\nwarrow\\searrow}\\) is the scattering coefficient of from the northwest irradiance to the southeast direction, and so on. We can express the changes of the other three directions similarly. In general, if we divide the space into \\(N\\) “channels”, each representing a set of directions (a finite solid angle), we can extend the two-flux model to a “N-flux” model.\n\n\n\n\n\n\nFigure 14.2: The setup for the N-flux model. All the possible directions (consider all the arrows that can possibly go out from the origin) are divided into “channels” or “streams”, each of which represents a finite solid angle within which an irradiance travels. The N-flux model models the changes of each of these irradiances. Adapted from Li (2003, fig. 4.1).\n\n\n\nFigure 14.2 shows the setup for deriving the N-flux model, where all the possible directions (consider all the arrows that can possibly go out from the origin) are divided into “channels” or “streams”, each of which represents a finite solid angle within which an irradiance travels. The figure visualizes eight such channels. The change of each channel is modeled by taking away from each channel photons that are absorbed and scattered to all other channels and by adding photons scattered into the channel from all other channels. In the end, we get \\(N\\) linear differential equations. Equation 14.26 is one such equation when \\(N=4\\). The channels are usually rotationally symmetric about the \\(z\\)-axis, assuming that the material is rotationally symmetric about the \\(z\\)-axis.\nComparing against the RTE in Equation 13.7, which has an integral term \\(L_s\\) given in Equation 13.4. What the N-flux model does is to essentially approximate that integral with a finite sum. In general, the larger the \\(N\\) the better the approximation but also the more computationally intensive to solve. We will omit a formal treatment but refer you to Bohren and Clothiaux (2006, chap. 6.1), Volz and Simon (2001, vol. 2, chap. 3.1.2), and Klein (2010, chap. 5.5) for details.\n\n\n\n\nBohren, Craig F, and Eugene E Clothiaux. 2006. Fundamentals of Atmospheric Radiation: An Introduction with 400 Problems. John Wiley & Sons.\n\n\nKlein, GA. 2010. “Industrial Color Physics.” Springer Science+ Business Media.\n\n\nKubelka, Paul. 1948. “New Contributions to the Optics of Intensely Light-Scattering Materials. Part i.” Josa 38 (5): 448–57.\n\n\nKubelka, Paul, and Franz Munk. 1931a. “An Article on Optics of Paint Layers (Translated by Stephen h. Westin).” Z. Tech. Phys 12 (593-601): 259–74.\n\n\n———. 1931b. “Ein Beitrag Zur Optik Der Farbanstriche.” Z. Tech. Phys 12:593–601.\n\n\nLi, Yang. 2003. “Ink-Paper Interaction-a Study in Ink-Jet Color Reproduction.” Institute of Technology-Linköpings University. Norrköping, Sweden: UniTryck.\n\n\nSaunderson, JL. 1942. “Calculation of the Color of Pigmented Plastics.” JOSA 32 (12): 727–36.\n\n\nSharma, Gaurav. 2003. “Color Fundamentals for Digital Imaging.” In Digital Color Imaging Handbook, edited by Gaurav Sharma, 14–127. CRC Press.\n\n\nVolz, Hans G, and Frederick T Simon. 2001. Industrial Color Testing. Vol. 2. Wiley-VCH New York.",
    "crumbs": [
      "Rendering",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>The N-Flux Theory</span>"
    ]
  },
  {
    "objectID": "rendering-nflux.html#footnotes",
    "href": "rendering-nflux.html#footnotes",
    "title": "14  The N-Flux Theory",
    "section": "",
    "text": "which says that there exists \\(c\\in [a, b]\\) such that \\(\\int_a^b f(x)g(x)\\d x = f(c)\\int_a^b g(x)\\d x\\), if \\(g(x)\\) is integrable and does not change its sign in \\([a, b]\\).↩︎\nIf \\(L(x, \\omega)\\) is invariant to \\(\\omega\\) and \\(f_p(x, \\omega', \\hat{\\omega})\\), under the isotropic medium assumption, is reduced to \\(f_p(x, \\theta)\\), where \\(\\theta\\) is the angle subtended by \\(\\omega\\) and \\(\\omega'\\), both \\(S_{\\uparrow\\downarrow}(x)\\) and \\(S_{\\downarrow\\uparrow}(x)\\) are essentially calculating \\(\\int^{\\pi}\\int^{\\pi}c~\\d \\theta\\d \\theta'\\).↩︎\nYou can see that \\(T(X)\\) does not involve \\(R_g\\): when we calculate the transmittance of the material, we assume that there is no substrate.↩︎",
    "crumbs": [
      "Rendering",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>The N-Flux Theory</span>"
    ]
  },
  {
    "objectID": "imaging.html",
    "href": "imaging.html",
    "title": "Imaging",
    "section": "",
    "text": "This part of the book will discuss imaging, the transduction of optical signals to electrical systems. Our discussion will center on the two fundamental components of any imaging system: optics and the image sensor.\nWe begin with an overview of optics in imaging systems (Chapter 15  Imaging Optics), covering core concepts such as the pinhole model, lenses, aberrations, and diffraction limits. We also explain key photographic concepts, including depth of field, magnification, and field of view, and introduce computational models of imaging through the lens of linear system theory.\nWe then turn to image sensors, examining the basic principles by which optical signals are converted to electrical charges and then to digital values (Chapter 16  Image Sensor Architecture). We will introduce hardware implementations of these processes for both monochromatic and color sensing.\nWe will then discuss the sources of noise in image sensors and how to model them (Chapter 17  Noise), followed by how the raw sensor signals are processed to yield images that can be consumed by human or machine vision systems (Chapter 18  Camera Signal Processing).",
    "crumbs": [
      "Imaging"
    ]
  },
  {
    "objectID": "imaging-optics.html",
    "href": "imaging-optics.html",
    "title": "15  Imaging Optics",
    "section": "",
    "text": "15.1 Overview\nThis chapter provides an introduction to imaging optics. We start from the pinhole model; from its pros and cons, we motivate a lens-based imaging system. We discuss important artifacts, a.k.a., aberrations, introduced by lenses that significantly impact the imaging quality. Finally, we conclude with a computational model for modeling the image formation process carried out by optics. The model provides a first-order approximation of the imaging quality and is widely used in various fields of visual computing.\nThis chapter focuses on the first stage in an imaging system: the optics, i.e., optics that are used for image formation. The goal of this chapter is to build a good understanding of the image formation process in optics. Optics manipulates/transforms optical signals, so the signal after optics is still in the optical domain. In later chapters, we will discuss how the optical signals are transformed into electrical signals (first to analog and then to digital signals) and how such electrical signals are further processed.\nImaging optics is important for human vision (because the ocular media of our eyes form an image on the retina), cameras (almost all of which have some form of optics), and graphics (where modeling optics is important for photorealistic rendering). Optics can also be used for ostensibly non-imaging purposes such as communication and computation. But of course the distinction is not black and white. You can argue that imaging is simultaneous communication (transferring signals from one side of the imaging system to the other side) and computation (the output signal is the result of a transfer function, usually not an identity function, applied to the input signal).\nWe will generally assume that the goal of the optics design is to form visually pleasing images as well as possible. The thinking is that if we provide a high-quality image, we are giving the downstream consumer, whether a human observer or a machine vision algorithm, the best chance to extract information from it.\nThis might not always be necessary. For instance, in machine vision/robotics applications, the consumer of an image is a computer vision algorithm such as object detection; so long as the algorithm can detect the object, the quality of the image itself is of no significance. In fact, one might argue that it is beneficial to design the imaging system so that the output image is obfuscated to protect privacy as long as essential features pertaining to downstream algorithms are preserved — this is an active area of research.\nThere is also a burgeoning area of research, which this chapter is largely unconcerned with, called computational imaging, where a significant amount of computation is involved to form a final image (Bhandari, Kadambi, and Raskar 2022). In many cases under such a paradigm, the initial image formed by the optics is rather unintelligible, and the name of the game is to design computational algorithms that can recover the “clean” image. This is usually formulated as an inverse problem: the optics (which could be anything, even a duct tape (Antipa et al. 2017)) transforms information in the physical world into a set of observations, and the algorithm inverts that forward model to obtain the original physical information from the observations. Even in this case, understanding and modeling the forward image formation process of the optics is crucial: only with that knowledge can we invert that process to obtain the physical information. In fact, one usually co-designs the image formation process (e.g., optics) with the inversion algorithm to maximize the overall performance.\nIn this sense, imaging is a form of sensing, and the ultimate goal of imaging is to obtain information about the physical world. A visually pleasing image is one way such information can be represented, but there are other forms of information we might be interested in: depth, geometry, spectral radiance, polarization, absorption/scattering coefficient of the media, etc. Many imaging systems are designed to obtain such non-visual information, which is beyond our scope. For instance, an X-ray CT scanner is an essentially computational imaging device; it captures a set of raw images, which by themselves are not directly useful. Subsequent computational algorithms are used to obtain the actual information of interest, the absorption/scattering coefficient of the medium, from the raw images. We have actually covered the gist of the forward process in this imaging device when we discuss volume scattering.\nWe will assume that there is a sensor plane on the other side of the imaging system to capture observations. An actual sensor has many pixels (along with many other components, some of which are optics!), each of which has a small but non-zero size, which plays a role in signal processing. In this chapter, however, we will assume that each pixel is infinitesimal. Therefore, the image formed on the sensor plane, for now, is assumed to be a continuous 2D function: for any \\((x, y)\\) point on the sensor, there is an irradiance value. A retina is a sensor, and the continuous image on the retina is usually called the optical image in vision science. An actual image captured by the sensor, whether biological (retina) or engineered, is necessarily discretized — by pixels or photoreceptors.",
    "crumbs": [
      "Imaging",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Imaging Optics</span>"
    ]
  },
  {
    "objectID": "imaging-optics.html#sec-chpt-imaging-optics-pinhole",
    "href": "imaging-optics.html#sec-chpt-imaging-optics-pinhole",
    "title": "15  Imaging Optics",
    "section": "15.2 Pinhole Model",
    "text": "15.2 Pinhole Model\nWe will start by discussing the pinhole system, which is very simple and not commonly used but carries interesting properties and implications for more complex imaging systems that we will turn to later.\n\n15.2.1 (Why) Do We Need Optics in an Imaging System?\nWhat if we just expose the image sensor or our retina to lights? We will get garbage because each pixel/photoreceptor receives light from everywhere in the scene. Figure 15.1 (left) illustrates the geometry of this imaging system.\n\n\n\n\n\n\nFigure 15.1: Left: without a pinhole, different pixels get roughly the same signal, and the differences between pixels are due mostly to noise. Middle: with a pinhole, the signal received by each pixel is restricted to a small area in the physical scene. Right: with a lens, the signal received by each pixel is still restricted to a small physical area, but the signal is much stronger.\n\n\n\nEach pixel receives light from everywhere in space. Assuming that each point in space is an ideal Lambertian emitter/scatterer, the two highlighted pixels will receive slightly different energy from the same point because of the cosine fall-off as a function of the incident direction. But if the sensor is much smaller relative to the distance to the physical space, the differences in fall-offs between different pixels are small, so we can say that each pixel roughly receives the same energy. In this case, the differences in pixel values are due to noise. So that’s why the image looks like a random garbage.\n\n\n15.2.2 Pinhole Imaging\nWhat we need is for each pixel to receive information only from a small spatial region in the scene. This is what a pinhole camera does, as illustrated in Figure 15.1 (middle). If the pinhole is infinitesimally small such that it allows only a single ray direction to go through, each pixel (which, again, for now is assumed to be an infinitesimal point on the sensor plane) captures light from only a single point in the scene.\nAs the pinhole size shrinks, the information captured by two adjacent pixels becomes more distinct, which is desirable, but if the pinhole size is too small, there are two issues. First, a pinhole that is too small requires a long exposure time. We will discuss this in Section 16.2, but a pixel is very much like a photoreceptor in that it is a photon collection device. Intuitively, the amount of photons a pixel collects (which we care about because it relates to the brightness of the captured image) is, roughly, proportional to both the pinhole area and the exposure time, so if we reduce the pinhole size, we need to increase the exposure time to maintain the pixel brightness.\nAn excessively long exposure time not only poses challenges to actually taking the photo but also leads to motion blurs. Figure 15.2 (b) shows an image captured by a pinhole camera where, during exposure, objects are moving. As a result, each pixel receives light from different points in the scene and, visually, the resulting image carries motion blurs.\n\n\n\n\n\n\nFigure 15.2: (a): blur from large aperture/pinhole (defocus blur); from Thycoop/photographs (2022). (b): blur from long exposure (motion blur); from Jürgen Königs (2006). (c): defocus blur arises from large pinholes, but when the pinhole becomes very small (on the order of light wavelength) diffraction occurs; adapted from Dominic Alves (2006).\n\n\n\nSecond, as the pinhole size gets smaller and smaller, eventually we get to the diffraction limit, which means we cannot use geometric optics anymore and a single point in the scene does not translate to a single point in the image plane. We will discuss this shortly in Section 15.2.3.\nWhat happens if we increase the pinhole size? We get a blurrier image. Figure 15.2 (a) shows one such image captured by a pinhole camera using a pinhole size of 0.5 mm. The blur can be easily explained by the geometry of pinhole imaging, as shown in Figure 15.2 (c), where the information of a point in the scene is spread or “smeared” across multiple pixels if the pinhole size is too large, leading to the blurs. For this reason, the blur here is a form of defocus blur; we will later see how a lens-based imaging system can also have a defocus blur with the same mechanism: information at a physical point in the scene is spread across multiple pixels even when the point itself is stationary.\nEven in a lens-based imaging system, we do not technically have a pinhole, but we usually still have an aperture, which acts like a pinhole in the sense that it limits the amount of light that is allowed into the rest of the system, so the aperture size certainly dictates the imaging quality. Our eye is certainly a lens-based imaging system, and the pupil acts as the aperture. The pupil size changes from roughly 2 mm in relatively high ambient light levels to about 8 mm under low light intensities.\nAmazingly, pinhole-only imaging is used in some animals. The most famous one is perhaps Nautilus, which has a pinhole eye without lenses (Zhang et al. 2021). The pinhole size is relatively large; the diameter varies between 0.4 and 2.8 mm (Hurley, Lange, and Hartline 1978), so you can imagine the imaging quality is not great.\n\n\n\n15.2.3 Diffraction Limit\nWhen the pinhole becomes very small, diffraction becomes visible. The diffraction pattern is called the Airy disk. Figure 15.3 (left) shows a computer-simulated Airy disk, and Figure 15.3 (right) shows how the intensity of the Airy disk falls off from the center.\n\n\n\n\n\n\nFigure 15.3: Left: compute-simulated Airy disk (contrast is slightly exaggerated); from Sakurambo (2007). Right: the intensity of an Airy disk pattern as a function of the spatial position (0 being the center); adapted from Inductiveload (2009).\n\n\n\nDiffraction is usually thought of as a wave phenomenon, where the light wave propagated from a small pinhole gets expanded spatially and forms the Airy disk pattern But perhaps a more principled way to understand diffraction is through quantum mechanics, which says that the more certain we are of the position of a photon we are less certain of the direction of its travel, and vice versa. When the pinhole is infinitesimal, we know for certain where a photon is, so we are uncertain where it is going to go: the result is the Airy disk pattern. In contrast, when the pinhole is large, we are less certain of the spatial position of a photon, so we are more certain of its direction of travel; as a result, diffraction contributes little to the overall imaging.\nImaging through a small pinhole can be thought of as a “single-slit” experiment. When we have a “double-slit” experiment with two small pinholes, the diffraction patterns from the two pinholes interfere, and we get the beautiful interference pattern that you perhaps have seen in middle-school physics class. Interestingly, there is a sequential version of the double-slit experiment, where photons are sent to the two slits sequentially, one by one. Amazingly, if we wait long enough, we will still see the interference pattern. This firmly establishes the fact that lights do behave like particles, not waves, just in a probabilistic manner.\n\nTheoretical Maximum Resolving Power\nDiffraction is a form of blur because the optical power of a power in the scene is spread spatially on the detector plane. Therefore diffraction limits the maximum resolving power of an imaging system. The way to quantify that is to imagine that we have two different points in space imaged through a pinhole. Each point, of course, will cause a diffraction pattern. The two Airy disks add up linearly in the power domain in the captured image, but when the two points are sufficiently apart spatially, the peaks of the two Airy disks will be sufficiently apart on the detector plane as well, which means we can tell the two points apart from the image (because the power of the Airy disks falls off very quickly with the distance to the center). When the two points are closer, the peaks of the Airy disks are closer; when the two peaks are sufficiently close, the superposition of the two Airy disks will result in an image where we cannot easily tell the two peaks apart, and that is when we know we have reached the resolution limit of the imaging system.\nA common criterion used to quantify such a limit is called the Rayleigh criterion, first defined by Lord Rayleigh1, which says the two points are regarded as just resolvable when the center of the airy disk of one point coincides with the first minimum of the other (Rayleigh 1879). If you go through the math, this translates to:\n\\[\n    \\theta \\approx 1.22\\frac{\\lambda}{D},\n\\tag{15.1}\\]\nwhere \\(\\lambda\\) is the light wavelength, \\(D\\) is the diameter of the pinhole, and \\(\\theta\\) is the angular resolution of the imaging system, i.e., the angle subtended by the two points and the pinhole. As an example, assuming a 550 nm typical visible light, when the pupil size is about 2 mm, which is a typical size under normal daylight, the resolvable angular resolution between two points is about 0.02 degree.\nNote that I italicize “regarded” in the text above. There is no reason why one cannot distinguish between two points separated less than the Rayleigh criterion in a given scenario or train a deep neural network to do so. The Rayleigh criterion for the most part serves as an intuitive criterion that works empirically well with observations.\nHow do we improve the resolving power of an imaging system? One way is to use shorter wavelength lights, which, according to Equation 15.1, would allow us to resolve objects that are closer. Optical microscopes use visible light, whereas electron microscopes take advantage of the wave nature of electrons to achieve much higher resolution than optical microscopes. The de Broglie wavelength of an electron is inversely proportional to its momentum. An electron microscope accelerates electrons to very high speeds, which reduces their wavelengths to below 1 nm (c.f., hundreds of nm for visible light) and increases the overall resolving power of the imaging system.",
    "crumbs": [
      "Imaging",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Imaging Optics</span>"
    ]
  },
  {
    "objectID": "imaging-optics.html#sec-chpt-imaging-optics-lens",
    "href": "imaging-optics.html#sec-chpt-imaging-optics-lens",
    "title": "15  Imaging Optics",
    "section": "15.3 Lenses",
    "text": "15.3 Lenses\nA convex lens brings many rays from a point together, as shown in Figure 15.1 (right). If the sensor plane is placed such as the image is in focus (which we will discuss shortly), the captured image is the geometrically the same as the one captured by a pinhole camera, but much brighter given the same exposure time. Both a pinhole imaging system and a convex-lens imaging system perform a perspective projection, which is basically the camera model used in computer vision when a camera needs to be modeled and in simple graphics rendering pipelines.\n\n15.3.1 Image Formation with an Ideal Lens\nWhat is the imaging process of a convex lens? How do we model the behavior of a (convex) lens? We can model this using the basic geometrical optics. Figure 15.4 shows the setup. Assume we have a convex lens, which is made of two spherical surfaces combined together The curvatures of the two surfaces are \\(R_1\\) (right surface) and \\(R_2\\) (left surface). The two surfaces are separated by a distance \\(d\\), which we call the thickness of the lens. The refractive indices of the air and the lens are \\(n_1\\) and \\(n_2\\), respectively. The goal is to calculate, for a ray originating from a distance \\(u\\) on the optical axis in the scene and traveling in a direction that subtends an angle \\(\\theta\\) with the optical axis, what happens when it reaches the other side.\n\n\n\n\n\n\nFigure 15.4: The setup to derive the lens maker’s equation, which requires two assumptions: the thin lens assumption and paraxial assumption.\n\n\n\nWe will apply the Snell’s law at the two interfaces, essentially tracing the ray through the lens. At the first interface, we have:\n\\[\n\\begin{aligned}\n    n_1 \\sin\\alpha &= n_2 \\sin{\\beta}, \\\\\n    \\alpha &= \\theta_1 + \\mu_1, \\\\\n    \\beta &= \\mu_1 - \\phi, \\\\\n    \\sin\\mu_1 &= \\frac{h_1}{R_1}, \\\\\n    \\tan\\theta_1 &= \\frac{h_1}{u}.\n\\end{aligned}\n\\]\nAs the light travels inside the lens and reaches the second interface, we have:\n\\[\n\\begin{aligned}\n    n_2 \\sin\\delta &= n_1 \\sin{\\gamma}, \\\\\n    \\delta &= \\mu_2 + \\phi, \\\\\n    \\gamma &= \\mu_2 + \\theta_2, \\\\\n    \\sin\\mu_2 &= \\frac{h_2}{R_2}, \\\\\n    \\tan\\theta_2 &= \\frac{h_2}{v}.\n\\end{aligned}\n\\]\nNow, we are going to make two assumptions. First, we will assume that the lens is very thin; the thickness \\(d\\) is very small. As a result, \\(h_1 \\approx h_2\\). This is called the thin-lens assumption. Second, we will assume that the ray stays close to the optical axis as it travels. Such rays are paraxial rays, and this assumption is called the paraxial assumption. That is, \\(\\theta_1, \\theta_2, \\alpha, \\beta, \\gamma, \\delta, \\mu_1, \\text{and~} \\mu_2\\) are very small angles, for which we can apply the usual small-angle approximation in trigonometry, e.g., \\(\\sin(\\alpha) \\approx \\tan(\\alpha) \\approx \\alpha\\) and \\(\\cos(\\alpha) = 1\\).\nUsing these two assumptions and through a little algebra, we will get:\n\\[\n    \\frac{n_2 - n_1}{n_1}(\\frac{1}{R_1} + \\frac{1}{R_2}) = (\\frac{1}{u} + \\frac{1}{v}).\n\\tag{15.2}\\]\nThis is called the Lens Maker’s Equation. Critically, observe that \\(v\\) depends only on \\(u\\) regardless of the path the ray takes (for a given lens with a particular set of \\(n_1, n_2, R_1, \\text{and~} R_2\\)). Therefore, all rays originating from an on-axis point will converge at the same point on the other side of the optical axis. This is crucial, because it means we can place a single detector Q (e.g., a pixel) on the imaging side (right side of the lens in this diagram) to capture all the rays from the point P. In other words, if we place the detector at Q, the point P would be in focus. In fact, you can show that this is true for all points in space, not just on-axis points: all the rays originating from a point on one side of the lens will converge to another point on the other side of the lens. In reality, of course, only paraxial rays with a thin lens follow this.\nNow, if the ray originates from infinity (as if it is parallel to the optical axis), where \\(u = \\infty\\), we have\n\\[\n    \\frac{1}{v} = \\frac{n_2 - n_1}{n_1}(\\frac{1}{R_1} + \\frac{1}{R_2}) := \\frac{1}{f}.\n\\tag{15.3}\\]\nThis allows us to derive the position \\(v\\) where a parallel ray intersects with the optical axis. We define that position as the focal length \\(f\\) of the imaging system.\nPlugging Equation 15.3 into Equation 15.2 gives us the familiar Gaussian Lens Equation:\n\\[\n    \\frac{1}{u} + \\frac{1}{v} = \\frac{1}{f}.\n\\tag{15.4}\\]\nUnder the ideal thin lens and paraxial approximation, the ray-tracing diagram is simplified to the one depicted in Figure 15.5, where:\n\nrays parallel to the optical axis always pass through the focal point (which has a distance \\(f\\) to the optical center) on the other side;\nrays passing through the focal point will be parallel on the other side;\na ray passing through the optical center does not change its direction if the lens is symmetric; otherwise the incident ray at the first interface is parallel to that leaving the second interface as if the ray has been shifted.\n\n\n\n\n\n\n\nFigure 15.5: Under an ideal thin lens and paraxial approximation, the ray-tracing diagram can be simplified so that points at the same depth (\\(u\\)) are all in focus at the same depth (\\(v\\)) on the other side of the lens. The chief ray is the ray that passes the center of the aperture, which is assumed to coincide with the optical center (not shown). The magnification of the lens is \\(M = H'/H\\), and the field-of-view (FoV) of the system is \\(2\\theta\\) (assuming that \\(H'\\) is half of the sensor, which is symmetric).\n\n\n\nFigure 15.5 shows that if we place an image sensor in the image space (right side of the lens) at \\(v\\), all the points at the depth \\(u\\) in the world space (left side of the lens) will be in focus. Another important point Figure 15.5 makes clear is that, if in focus, the captured image is the geometrically the same as the one captured by a pinhole camera (but of course brighter given the same exposure time because more rays are captured): the optical center of the lens is the pinhole here geometrically.\n\nAccommodation\nLet’s assume that \\(R_1 = R_2 = R\\); Equation 15.3 suggests that if we reduce \\(R\\) (increase the curvature of the lens surface), \\(f\\) reduces as well. Then look at Equation 15.4; if \\(f\\) reduces and we fix the object at the distance \\(u\\), for that object to be in focus we have to reduce \\(v\\), i.e., move the sensor plane closer to the lens. That is, if we curve the lens surfaces more, rays focus closer to the optical center as if the lens bends light more, and vice versa.\nAnother way to think of this is that if we cannot move the relative distance between the sensor and the lens, to focus on an object (in the world space) closer to the lens (\\(u\\) reduces), the lens focal length \\(f\\) has to reduce too. This is exactly what our eye lens does: to focus on closer objects, the lens curves more to gain more light-bending power. For that to take place, the ciliary muscle would have to contract. Conversely, to focus on farther objects, the ciliary muscle relaxes, which reduces the curvature of the lens, which now bends light less and, thus, allows us to focus on farther objects. Changing the focal length through changing the curvature is called accommodation. As one gets older, the ciliary muscle is not as effective in contracting the lens. That is why one uses the reading glasses, which provide additional light-bending power to assist that of the eye lens. Recall, from Section 2.2.1, that while the lens is flexible, most of the light refraction was done at the air-cornea interface because of the large difference in the refractive index there.\nIn cameras, unless you are using liquid lenses, the curvature of each lens surface stays fixed once fabricated, so how do we focus on objects closer or farther than we are currently focused on? The answer is we move the lens, essentially solving for \\(v\\) given a new \\(u\\) using Equation 15.4. This is essentially how auto-focus works in cameras. Alternatively, we could also move the sensor, but in practice the sensor stays fixed (e.g., attached to the back of the camera housing), and it is the lens that is movable.\n\n\n\n15.3.2 Magnification vs. Field-of-View\nUsing simple trigonometry in Figure 15.5, we can relate the size of an object in the world space (\\(H\\)) and that in the image space (\\(H'\\)):\n\\[\n    M = \\frac{H'}{H} = \\frac{f}{u-f} = \\frac{1}{\\frac{u}{f}- 1},\n\\tag{15.5}\\]\nwhere \\(M\\) is the magnification of the imaging system. We can see that \\(M\\) increases as \\(f\\) does. That is why telephoto cameras, those that you see in, for instance, sports broadcasting, are very long: they need to be long to accommodate a large focal length so that they can magnify objects that are very small (far away).\nWhat do we sacrifice when we increase magnification by increasing the focal length? The FoV reduces. The FoV of an imaging system is the extent of the observable world that can be captured by the sensor. Let’s use Figure 15.5 to derive this, and for simplicity’s sake, let’s just assume that the sensor size is \\(2H'\\) and is symmetric about the optical axis, i.e., the object at \\(u\\) is just fully captured by the sensor. The figure omits the upper half of the sensor. The FoV is defined as \\(2\\theta = 2 \\times \\arctan(H'/v)\\).\n\n\n\n\n\n\nFigure 15.6: Increasing the focal length increases the magnification but reduces the FoV. For simplicity, we use a pinhole geometry here, but both the pinhole camera and the ideal thin lens (with the paraxial approximation) perform the same perspective projection, leading to the same image formation geometrically.\n\n\n\nNow for the same object at \\(u\\), if \\(f\\) increases, \\(v\\) has to increase as well for the object to be captured in focus. As a result, \\(\\theta\\) reduces, so does the FoV. This intuitively makes sense: if an object is magnified more on the sensor, which has a fixed size, the amount of other objects that can be captured naturally reduces, hence the reduction of the FoV. As an example, the two imaging systems in Figure 15.6 differ only in the focal length: the one on the left has a shorter focal length \\(f\\) and hence a shorter sensor-lens distance \\(v\\) (for the same object to be captured in focus), which translates to a larger magnification and narrower FoV.\nFigure 15.7 shows a few concrete examples of how the focal length affects magnification and FoV. The fisheye lens does not perform a perspective projection (straight lines in the world space are not straight in the image space), so its image formation is not directly comparable, but we can see that it has the widest FoV. Other seven photons are taken with the same sensor but different lenses that differ in their focal lengths.\n\n\n\n\n\n\nFigure 15.7: The effect of focal length on both magnification and FoV; adapted from Canon (2006, p. 123–25). The fisheye lens does not perform a perspective projection, so its FoV is not directly comparable with others.\n\n\n\n\n\n15.3.3 Magnifying Glasses and Projection Lenses\nThe Gaussian lens equation also helps us understand the geometry behind magnifying glasses and the projection lenses in AR/VR devices and cinematography.\nWhen \\(u &lt; f\\) in Equation 15.4, \\(v\\) is negative. Figure 15.8 (top) shows the geometry of this case. As a result, the object does not form a physical image in the image space, because rays from a point on the object do not converge to a point in the image space. Instead, those rays diverge, and the extension of those rays actually converge at a point farther away from the lens in the world space. Now, if our eye is at the right place, i.e., the diverging rays converge on the retina after traveling through the eye lens, as is the case in Figure 15.8 (top), we will see a magnified object. In this case, the lens acts as a magnifying glass.\n\n\n\n\n\n\nFigure 15.8: Top: the geometry of a magnifying glass; adapted from Hecht (2016, figs. 5.102 (c)). Bottom: a magnifying glass projects a small and close real image to a larger and farther virtual image; VR devices all have a pair of magnifying glasses to create a virtual display that our eye lenses can focus on.\n\n\n\nThe magnifying glass functionally 1) projects a small physical object to an apparently larger virtual object that is 2) farther away from the eye. These two functionalities are exactly what a project lens in AR/VR need. Figure 15.8 (bottom) illustrates a projection lens in VR; the optics in AR are much more complicated, but the basic idea of a projection lens applies there too. In AR/VR devices, the actual display is very close to the eye, to the point that no eye lens can actually be accommodated to focus on the display (the lens would have to be curved so unrealistically much). Of course the display itself is very small, so seeing details is hard, too. The solution is to place a convex lens between the display and the eye, and the three components are so positioned that the display is closer to the lens than a focal length. As a result, the actual, physical display is projected to a much larger virtual display that is also farther away, to which our eye lens could actually accommodate. When you watch a movie in a cinema on a large screen or use a home projector, there is a projection lens sitting at the back doing the same thing.\n\n\n15.3.4 Depth of Field\nWhat if the sensor is not correctly positioned according to the Gaussian lens equation (Equation 15.4)? The object/point being imaged will be out of focus, and the result is a blur on the image. Figure 15.9 shows three cases, where the sensor (5) and the lens are fixed in position (4), under which objects at plane 2 (with a distance \\(T\\) to the lens) would be in focus, but both object 1 and object 3 would be out of focus because rays originating from them will be spread across a small area on the sensor plane, looking like blurs. The shape of the blur is called the bokeh, which is mostly determined by the aperture shape (and also aberrations introduced by the imaging system, which we will see later). If the aperture is a circle, the bokeh would be one too, and we call such a blur the circle of confusion (CoC).\n\n\n\n\n\n\nFigure 15.9: The setup to derive the depth-of-field (DoF) equation; adapted from BenFrantzDale (2010a). CoC: circle of confusion.\n\n\n\nAs the CoC increases, eventually it becomes objectionable to the human visual system. Exactly what that CoC threshold depends on a number of factors that we will omit here (e.g., how the image will be scaled when being viewed, the contrast sensitivity of the human visual system, etc.), but let’s just use \\(C\\) to denote that threshold for now. You can see that if an object is placed slightly before or after the depth \\(T\\) (where the object is perfectly in focus), as long as the resulting CoC is smaller than \\(C\\), our visual system would still regard it as in focus. The distance between the nearest and farthest objects whose CoCs are still within \\(C\\) is called the depth-of -field (DoF) of the system.\nUsing geometrical optics and with a few assumptions, we can show that the DoF is given by:\n\\[\n    DoF \\approx \\frac{2CT^2N}{f^2} = \\frac{2CT^2}{fA},\n\\tag{15.6}\\]\nwhere \\(T\\) is the distance of the object that is perfectly in focus, \\(f\\) is the focal length, and \\(N = f/A\\) is called the F-number of the camera, which is defined as the ratio between the focal length and the aperture size (\\(A\\)).\nGiven Equation 15.6, there are three ways to increase the DoF. First, we can increase \\(T\\), i.e., focus on objects that are farther away (e.g., landscape photography). Second, we can decrease the focal length, but just keep in mind that changing the focal length will also affect the magnification and FoV as discussed in Section 15.3.1. Finally, we can also reduce the aperture size, which would increase the F-number. Changing the aperture size, however, will have implications on other aspects of the imaging quality. Specifically, a small aperture increases the exposure time and, thus, motion blur.\nA larger DoF would mean that objects within a larger depth range could be simultaneously in focus. A shallow DoF, however, is at many times desirable. The “portrait mode” in many modern smartphone cameras essentially captures photos with a shallow DoF. Intuitively, one can invert all three methods above to obtain a shallow DoF, but what if the hardware does not permit us to do that? For instance, what if we cannot increase the aperture size and focal length but want to capture a close object with a shallow DoF?\nComputation comes to the rescue. There is a notion of Synthetic DoF, which uses post-processing algorithms to emulate a shallow DoF. For instance, one might first capture an all-in-focus photo, estimate the depth for each pixel in the photo (including the pixels that correspond to the object that we do want to have in focus), then selectively blur pixels that are farther or closer than the objects of interest. This is what the portrait mode in Google’s Pixel phone does (Wadhwa et al. 2018).\nSynthetic DoF is a classic example of computational photography, where the imaging system is largely assisted by computational algorithms (to reduce the design complexities of the imaging hardware). In this case, computation is required mainly to estimate depth. In turns out that auto-focus in cameras is all about depth estimation, which, again, usually involves some form of collaboration between software and hardware.\n\n\n15.3.5 Radiometric Analysis of Lens\nWhat does a convex lens do to the radiance of incident light? We know that the radiance of a ray does not change as the ray propagates through space along a particular direction, but what does a lens do to the radiance? This is an important problem in practice: the lens essentially transforms the light field in the physical scene to the light field inside the camera, which means if we know the latter and the radiance transformation done by the lens, we can infer the light field in the scene.\nThe way to reason about it is to think of a lens as performing a sequence of two refractions at its two surfaces, so we will have to first reason about, at each surface, what happens to the radiance and then consider the composite effect of the two surfaces.\nWith a little radiometry (which we will omit but refer to Bohren and Clothiaux (2006, chap. 4.1.6) for the derivation), we can show that the radiance after refraction \\(L_r\\) relates to the incident radiance \\(L_i\\) by:\n\\[\n    L_r = n^2 L_i,\n\\]\nwhere \\(n\\) is the relative refractive index of the lens/medium to the air. Usually \\(n &gt; 1\\), which means after refraction the radiance increases. This makes sense because after refraction from the air to, say, glass, the set of incident rays maps to a smaller solid angle.\nWhat happens in the second surface? The same thing except the relative refractive index is now \\(1/n\\), since we are now going from the medium to the air:\n\\[\n    L_o = \\frac{1}{n^2} L_r,\n\\]\nwhere \\(L_o\\) is the radiance leaving the lens. Combining the two equations above, we can see \\(L_o = L_i\\), meaning the lens does not change the radiance. This is a nice result, because it essentially means we can simply trace rays through a lens and be reasonably sure that the ray radiance does not change.\nIntuitively, this conclusion is obviously wrong: some energy of incident light is absorbed/reflected away by the lens, so the energy leaving the lens is definitely smaller than that entering the lens. So the derivation above is a bit of a simplification, because we have assumed that no reflection takes place at each surface and no absorption by the lens. That said, this invariance largely still holds if we confine ourselves to near-normal angles of incidence and assume typical materials for lenses (which are mostly transparent with little absorption).",
    "crumbs": [
      "Imaging",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Imaging Optics</span>"
    ]
  },
  {
    "objectID": "imaging-optics.html#sec-chpt-imaging-optics-lens-abe",
    "href": "imaging-optics.html#sec-chpt-imaging-optics-lens-abe",
    "title": "15  Imaging Optics",
    "section": "15.4 Aberrations",
    "text": "15.4 Aberrations\nWhen building an imaging system, we ideally want a point in the physical space to be captured as a single point in the image space. In our derivation of the Gaussian lens equation in Section 15.3.1, this is indeed the case, so if the sensor is correctly positioned, we will capture a sharp image of the point. This derivation, however, assumes an ideal thin lens and considers only paraxial rays. It turns out that the equation still holds even if the lens is thick (i.e., the distance between the two surfaces is not negligible), even though the definition of the focal length would have to be slightly more complicated than Equation 15.3.\nThe real complication is that in practice we cannot ignore non-paraxial rays (i.e., rays that do not stay close to the optical axis), in which case rays from a single point (or from infinitely far away) will not all converge at a single point in the image space, resulting in a blur. Mathematically, this means we cannot invoke the small angle approximations. For instance, using Taylor expansion, we have:\n\\[\n    \\sin\\theta = \\theta - \\frac{\\theta^3}{3!} + \\frac{\\theta^5}{5!} - \\cdots.\n\\]\nWhen considering paraxial rays, we can afford to consider only the first term, but when \\(\\theta\\) is large, we have to include other terms. When considering the second term of Taylor series expansion (compared to considering only the first term), five forms of aberrations show up: spherical aberration, coma, astigmatism, field curvature, distortion. Geometrical optics that consider the second term are called the third-order theory, as opposed to the first-order theory or Gaussian optics that considers only the first term.\n\n15.4.1 Spherical Aberration\nIt turns out that for a spherical lens, non-paraxial rays originating from a point on the optical axis will not converge at the same point. This can be shown by going through the derivation of the Gaussian lens equation (Section 15.3.1) but this time without the small angle approximations. We would then see that \\(v\\) depends not only on \\(u\\) and \\(f\\) but also on the direction of the ray leaving \\(u\\). By extension, not all rays parallel to the optical axis (especially those that are far away from the optical axis) will focus at the same point. This is called the spherical aberration, which is illustrated in Figure 15.10 (left).\n\n\n\n\n\n\nFigure 15.10: Left: spherical aberration; from Mglg (2008). Right: coma; from Glrx (2018).\n\n\n\nMirrors have spherical aberrations too. Perfectly spherically curved mirrors cannot focus parallel lights; parabolic mirrors are free from spherical aberrations. One might venture to guess that’s why Archimedes could not have used mirrors to burn Roman ships, because they could not have had the skills to make parabolic mirrors. The burning mirror story is more likely a story than a fact. There are just too many technical reasons why that would have been very hard. For instance, it would have taken a very large mirror given the intensity of sunlight and the distance of the ships, and the ship would have to be perfectly positioned at the focal point (Chris Rorres n.d.; Mills and Clift 1992).\n\n\n15.4.2 Coma\nWhile spherical aberration is concerned with rays from on-axis points or parallel rays that are also parallel to the optical axis, another aberration called coma or comatic aberration is concerned with rays from off-axis points or, as illustrated in Figure 15.10 (right), parallel rays that have an oblique incident angle w.r.t. the optical axis. We can show that rays from an off-axis point focus on different points and, by extension, parallel rays that are not parallel to the optical axis do not focus on the same point. This aberration is called coma because the resulting blur looks like a coma.\n\n\n15.4.3 Astigmatism\nYet another form of aberration is called astigmatism. It is also concerned with points off the optical axis. In particular, we are concerned with rays propagated in two planes. The first plane is one defined by the object point and the optical axis and is called the tangential plane or the meridional plane. The other plane is one that is orthogonal to the meridional plane and is called the sagittal plane. It turns out that rays from the two planes focus on different points on the optical axis. This is illustrated in Figure 15.11 (left), where all the rays in the meridional (M) plane focus at \\(B_M\\) and all the rays in the sagittal (S) plane focus at \\(B_S\\).\n\n\n\n\n\n\nFigure 15.11: Left: an illustration of astigmatism; adapted from Michael Schmid (2008). Right: the images captured when the sensor plane is placed at different positions; from Tallfred (2005).\n\n\n\nThe blur we get depends on where we place the sensor plane, and some examples are shown in Figure 15.11 (right). If we place the sensor at \\(B_M\\), a single point source gets imaged as a horizontal/lateral “line” due to the spread of the rays in the S plane. We say a line, but it is not actually a line because rays in other planes (other than the M and S planes) will not focus at \\(B_M\\) and still contribute to the image formation, so the resulting image is really a very much elongated ellipse. If the object is not a point but, say, spans a plane (top-left), the resulting image has a somewhat horizontal/lateral blur as if the in-focus image is smeared laterally (bottom-right).\nAs we move the sensor beyond \\(B_M\\), the elongated ellipse gradually expands vertically and then becomes circular, and then shrinks laterally; eventually, when the sensor is placed at \\(B_S\\), we get a vertical “line” (an elongated ellipse along the vertical axis) because, mainly, of the rays in the M plane. The resulting image would appear to have a somewhat vertical blur as if the in-focus area were smeared vertically (bottom-left). The somewhat circular blur when the sensor plane is in-between \\(B_M\\) and \\(B_S\\) means that the resulting image (top-right) appears as if the in-focus image is smeared in all directions.\n\n\n15.4.4 Field Curvature\nIf an imaging system is free of all the previous aberrations, a single point in the world space corresponds to a single point in the image space. However, a plane of points in the world space would not correspond to a plane in the image space. In fact, it would correspond to a curved surface. If we used a planar sensor for imaging, we would get a blurred image. This form of aberration is called field curvature, as illustrated in Figure 15.12 (left).\n\n\n\n\n\n\nFigure 15.12: Left: a manifestation of field curvature; from BenFrantzDale (2010b). Right: focal plane of Kepler space telescope is curved to mitigate field curvature; from HandWiki (2024).\n\n\n\nWhile it might be difficult to build a single curved sensor, it is relatively easy to assemble a set of sensors on a curved surface. The image-sensor array of the Kepler space observatory is curved to compensate for the field curvature, as shown in Figure 15.12 (right) Interestingly, you might recall that the human retina is not planar either; it is curved. This to some extent helps mitigate the effect of field curvature.\n\n\n15.4.5 Distortion\nEven when all the previous aberrations are somehow corrected, the image would look sharp but distorted. Distortion does not introduce blurs. Rather, it is a result of the variation of magnification as a function of the distance to the optical axis (object height).\n\n\n\n\n\n\nFigure 15.13: A comparison of pincushion (positive) distortion and barrel (negative) distortion; from WolfWings (2008b) and WolfWings (2008a).\n\n\n\nEquation 15.5 suggests that magnification depends only on the object distance to the lens \\(u\\), but in reality the magnification depends also on the object height. We can imagine that for a point that is distant from the optical axis, rays originating from that point will not be paraxial rays. If the magnification increases with the height, we have a positive or pincushion distortion; otherwise, we have a negative or barrel distortion. The two forms of distortion are illustrated in Figure 15.13.\n\n\n15.4.6 Chromatic Aberration\nAll the aberrations we have discussed before are present even if we consider only a single wavelength; they are called monochromatic aberrations. When we consider lights that comprise a mixture of different wavelengths, chromatic aberration shows up. Chromatic aberration arises fundamentally because the refractive index is a function of wavelength; after all, that is how Newton was able to disperse white light and show the spectrum. Figure 15.14 (left) illustrates the issue of chromatic adaptation, which introduces “colorful” blurs. Figure 15.14 (middle) shows how the refractive index of BK7 glass (which is commonly used in lenses) changes with wavelength.\n\n\n\n\n\n\nFigure 15.14: Left: illustration of chromatic adaptation; adapted from Bob Mellish (2006). Middle: refractive index vs. wavelength for BK7 glass; from DrBob (2007). Right: sRGB white as displayed by a 4th-generation iPad Pro; the image is captured when focusing on the green subpixel, so other subpixels are out of focus.\n\n\n\nAs another example, I took a picture of my 4th-gen iPad Pro when it displayed sRGB white. I intentionally focused on the green subpixels. As a result, the other two subpixels are out of focus — due to chromatic aberration; see Figure 15.14 (right).\n\n\n15.4.7 Correction for Aberrations\nOne of the main tasks of optical design, especially for imaging lenses, is to correct for aberrations. There are two main approaches: non-spherical (aspherical) lenses and compound lenses.\nOptical designers often use multiple (compound) lenses in combination to correct various aberrations. For instance, chromatic doublets or apochromatic triplets are specifically designed to counteract chromatic aberration. One obvious downside of compound lenses is form factor, which becomes an issue for systems like Augmented Reality that need to be very compact.\nOne promising technology that people are currently investigating is called freeform optics. Traditional aspherical lenses, while deviating away from a spherical design and can avoid compound lenses in many cases, are still rotationally symmetric, so they are still limited in what they can do. Freeform optics take this concept further by allowing surfaces that lack rotational symmetry, providing additional degrees of freedom in optical design. This enables better correction of higher-order aberrations, such as coma and astigmatism, which aspheric lenses alone may not fully eliminate.\n\n\n15.4.8 Not All Blurs are Created Equal\nIdeally a point source in the scene should really be captured as a single point in the image plane, but we have seen a few ways that a blur can occur. But not all blurs are created equal; it is perhaps useful to review the different causes of a blur.\nBlurs can result from aberrations, diffraction, defocus, and motion. We have just seen blurs from aberrations, but just note that not all aberrations result in blurs, an example of which would be distortion. Assuming an aberration-free imaging system, if the sensor is not placed as the focal plane, we could get a de-focus blur, as we have seen in the DoF section (Section 15.3.4). Note that a pinhole camera would never have defocus blur, because its DoF is infinite (using \\(A=0\\) in Equation 15.6).\nEven if the sensor is placed as the focal plane, but if the object is motion, we would most likely get motion blur, because the exposure time is finite — unless of course the exposure time is so short that the object motion, when projected on the sensor plane, is within the pixel width. The longer the exposure time, the more pronounced the motion blur becomes.\nFinally, we have blurs from diffraction, which, as we have discussed in Section 15.2.3, is fundamentally a result of the quantum nature of light. If an imaging system is free from all previous forms of blur2, we say it is “diffraction limited”, because its imaging capability (the ability to avoid blurs) is limited only by diffraction.",
    "crumbs": [
      "Imaging",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Imaging Optics</span>"
    ]
  },
  {
    "objectID": "imaging-optics.html#sec-chpt-imaging-optics-modeling",
    "href": "imaging-optics.html#sec-chpt-imaging-optics-modeling",
    "title": "15  Imaging Optics",
    "section": "15.5 Linear System Modeling",
    "text": "15.5 Linear System Modeling\nHow do we model the image formed on the sensor plane? We could trace rays out of points in the scene, but it has many limitations. First, we could afford to sample only a few rays for a point. Second, we could afford to sample only a few points on an object. Third, things like diffraction that go beyond geometrical optics need special treatment if not straight up impossible using ray tracing.\n\n15.5.1 Basic Idea of a Linear System Theory\nA common modeling strategy is to first characterize the response of the imaging system against a single point source. If we assume that the system is linear and shift invariant (LSI), we can derive how the system responds to an arbitrarily complex object, which is treated as nothing more than a collection of (infinitely many) points, using the linear system theory. Let’s unpack this step by step.\n\nPoint Spread Function\nThe response of a single point source is called the Point Spread Function (PSF) of the imaging system. How does the PSF look like? Ideally, a single point in the world space would be imaged as a single point in the image plane, so the corresponding PSF would be a Dirac delta function, but as we have discussed in Section 15.4.8, in reality the image of a single point would be blurred, whether it is because of diffraction, defocus, or aberration (assuming the point source is stationary).\nFigure 15.15 (left) shows a few examples of the PSFs. The bottom-right corner shows a diffraction-limited PSF, which is essentially the Airy disk (but visualized as a 2D grayscale map). As we move vertically up, we add more spherical aberration to the system, and as we move to the right, we add more defocus to the system. Figure 15.15 (middle) shows a PSF of a system with astigmatism; this time the PSF is visualized in 3D rather than a 2D grayscale map. We can see that the PSF is not radially symmetric; rather, it is elongated along one dimension, which matches our intuition of astigmatism (see Figure 15.11).\n\n\n\n\n\n\nFigure 15.15: Left: PSF of a diffraction-limited system (bottom-left), with spherical aberration (vertically) and with defocus (horizontally); adapted from Mdf (2005). Middle: PSF (visualized in 3D) with astigmatism. Right: in a linear and shift-invariant imaging system, image formation is equivalent to a convolution using the PSF; from Mdf (2006).\n\n\n\n\n\nLinear System\nInformally, if there are two inputs \\(x\\) and \\(y\\) to the system, say two points in the world space, and the responses to these two inputs, i.e., their respective PSFs, are \\(H(x)\\) and \\(H(y)\\), the response of a linear system to a new input \\(\\alpha x + \\beta y\\) would be \\(\\alpha H(x) + \\beta H(y)\\). \\(\\alpha x\\) means to scale the input \\(x\\)’s value (e.g., irradiance of a point) by a factor of \\(\\alpha\\).\nA linear system essentially means when we image two points simultaneously, the resulting image is equal to the sum of the individual image of each point. You can imagine how this would simplify our modeling later. In practice, an imaging system is linear when it interrogates non-coherent light, e.g., sunlight or OLEDs, rather than lasers.\n\n\nShift-Invariant System\nAn imaging system is shift invariant if its PSF of a point is invariant to the shifts of the point in the world space. This property allows us to use a single PSF to characterize the system.\nOf course, in reality a system is hardly shift-invariant. For instance, if we move a point away from or closer to the lens, we get different kinds of defocus blurs, so the point response depends on depth. Even if we shift a point within a single depth plane, the rays incident on the lens, leaving the lens, and, by extension, hitting the sensor plane would be different. Even ignoring aberrations, different incident angles result in different irradiance captured by the sensor plane (because of the Lambertian cosine law and is a form of “vignetting”).\nIn general, however, shift invariance approximately holds if we assume that the object to be imaged is very far away from the lens (so the depth variation within an object is negligible with respect to the overall distance to the lens) and the imaging system has a relatively small FoV.\n\n\n\n15.5.2 Modeling Image Formation in LSI Systems\nUnder the linear and shift-invariance assumptions, we can derive a simple but incredibly useful computational model for the image formation process, which is decoupled into two conceptual steps.\nIn the first step, we calculate an ideal image \\(I_{ideal}\\) formed by a pinhole imaging system, where the imaging system PSF is a delta function. Effectively, this means the imaging system has no diffraction/aberration and every (unoccluded) scene point is sharply in focus (no defocus blur).\nGeometrically, \\(I_{ideal}\\) is a perspective projection of the 3D scene to the sensor plane. That is, each \\((x, y)\\) point in this ideal image \\(I_{ideal}\\) corresponds to a point \\(P(x', y', z')\\) in the scene as if that scene point is captured through a pinhole (recall that geometrically an ideal thin lens performs the same projection as a pinhole system). This is shown in Figure 15.16 (left). Radiometrically, the value of \\(I_{ideal}(x, y)\\) is an irradiance quantity, representing the irradiance emitted from \\(P(x', y', z')\\) that is captured at \\(I_{ideal}(x, y)\\). We will discuss in Section 16.5 exactly how to calculate this irradiance.\n\n\n\n\n\n\nFigure 15.16: Left: geometrically, the ideal image \\(I_{ideal}\\) is a perspective projection of the scene, and radiometrically \\(I_{ideal}(x, y)\\) corresponds to the irradiance from \\(P(x', y', z')\\) that are captured by the imaging system. Right: the intuition behind why imaging in a LSI system is a convolution with the PSF.\n\n\n\nIn the second step, we then using the PSF function \\(f(\\cdot)\\) to convolve \\(I_{ideal}\\), and the result:\n\\[\n    I_{actual} = I_{ideal} \\star f,\n\\tag{15.7}\\]\nis the actual image formed by the imaging system. This is illustrated in Figure 15.15 (right).\nThe convolution is a natural conclusion once we assume linearity (irradiances add) and shift invariance (constant PSF) of the imaging system. Figure 15.16 (right) illustrates the intuition using an 1D example. \\(I_{ideal}(x, y)\\) is the irradiance at \\((x, y)\\) with a delta PSF. With a non-delta PSF, the irradiance of \\(I_{ideal}(x, y)\\) is distributed over the sensor plane as defined in the PSF. Each point on the sensor, thus, receives contributions from all the point spreads. Since we assume linearity, the result is a convolution between \\(I_{ideal}\\) and the PSF.\nWhy? Here is a quick demonstration. Taking a discrete case with four points as an example and assuming we are interested in calculating the actual irradiance \\(I_{actual}(x_0)\\), the contribution from \\(x_0\\) itself is \\(I_{ideal}(x_0)f(0)\\), the contribution from \\(x_1\\) is \\(I_{ideal}(x_1)f(x_0-x_1)\\), and similarly the contributions from \\(x_2\\) and \\(x_3\\) are, respectively, \\(I_{ideal}(x_2)f(x_0-x_2)\\) and \\(I_{ideal}(x_3)f(x_0-x_3)\\). So the actual irradiance received by \\(x_0\\) is:\n\\[\n\\begin{aligned}\n    I_{actual}(x_0) = &I_{ideal}(x_0)f(0) + I_{ideal}(x_1)f(x_0-x_1) \\nonumber\\\\\n    + &I_{ideal}(x_2)f(x_0-x_2) + I_{ideal}(x_3)f(x_0-x_3).\n\\end{aligned}\n\\tag{15.8}\\]\nYou can see when we generalize from four points to a continuous signal \\(I_{ideal}\\), Equation 15.8 becomes Equation 15.7.\nIn the literature, it is common to see people taking images from a dataset, e.g., ImageNet, and simply convolve a PSF against them. The underlying assumption is that those images are captures of distant objects with an ideal system (with no blurs) and, thus, can be treated as essentially irradiance maps \\(I_{ideal}\\).\nWhat if the system is not shift invariant? For instance, if we cannot assume that objects are all very far away from the lens, scene points at different depths will have different PSFs. So long as we can still assume linearity, however, we can still relatively easily simulate the image formation process using the exact same principle shown before in Figure 15.16: “convolving” against spatially varying PSFs is equivalent to summing the PSFs (each of course scaled by the corresponding irradiance). This is a bit similar to surface splatting in PBG (Section 13.4.3), where each surface sample has a different reconstruction filter, so reconstruction amounts to summing each reconstruction filter, each scaled by the sample color.\n\n\n15.5.3 Fourier Perspectives: OTF and MTF\nSince we are using convolution to model imaging in LSI systems, it is only natural to take a Fourier perspective. Recall the convolution theorem:\n\\[\n\\begin{aligned}\n    \\mathcal{F}(f \\star g) &= \\mathcal{F}(f)\\mathcal{F}(g), \\\\\n    f \\star g &= \\mathcal{F}^{-1}(\\mathcal{F}(f)\\mathcal{F}(g)),\n\\end{aligned}\n\\]\nwhere \\(\\mathcal{F}\\) and \\(\\mathcal{F}^{-1}\\) denote Fourier transform and inverse Fourier transform. This allows us to reason about the effect of an imaging system in the frequency domain.\nThe Fourier transform of a PSF is called the Optical Transfer Function (OTF), which is necessarily complex-valued, which has a magnitude and a phase component. The magnitude component of the OTF is called the Modulation Transfer Function (MTF) and the phase component of the OTF is called the Phase Transfer Function (PTF):\n\\[\n    OTF(\\omega) = MTF(\\omega)e^{i PTF(\\omega)}.\n\\]\nWhat is the OTF of an ideal PSF, i.e., a delta function? It is a constant 1 across all frequencies. This makes sense: an ideal PSF introduces no blur so it does nothing to each spatial frequency.\n\n\n\n\n\n\nFigure 15.17: Top: OTF, PSF, and the resulting image of a diffraction-limited system. Bottom: OTF, PSF, and the resulting image of a diffraction-limited but defocused system. From Tom.vettenburg (2017).\n\n\n\nFigure 15.17 shows two more examples; the top half shows the OTF, PSF, and the resulting imaging of a diffraction-limited system (i.e., PSF being an Airy disk), and the bottom half shows the same system with a defocus blur. In both cases, the OTF is the same as the MTF because the Fourier transform of both PSFs have zero phase (PTF is zero at any \\(\\omega\\)). You can convince yourself of this by taking a Fourier transform of the Airy function and assuming that defocus adds a Gaussian blur to the Airy disk; we will omit the math here. General OTFs do have a phase term because the PSFs of many aberrations, e.g., coma and astigmatism, are not radially symmetric.\nWe can see that in the diffraction-limited case, the OTF drops to 0 at a frequency of 500, meaning information at any frequency higher than the cut-off is lost. The (first) cut-off for the defocused system is at an even lower frequency (about 200), naturally leading to more blurs in the resulting image.\n\n\n\n\nAntipa, Nick, Grace Kuo, Reinhard Heckel, Ben Mildenhall, Emrah Bostan, Ren Ng, and Laura Waller. 2017. “DiffuserCam: Lensless Single-Exposure 3D Imaging.” Optica 5 (1): 1–9.\n\n\nBenFrantzDale. 2010a. “Effect of aperture on blur and DOF; CC BY-SA 3.0 license.” https://commons.wikimedia.org/wiki/File:Depth_of_field_illustration.svg.\n\n\n———. 2010b. “Ray diagram showing field curvaure; CC BY-SA 3.0 license.” https://commons.wikimedia.org/wiki/File:Field_curvature.svg.\n\n\nBhandari, Ayush, Achuta Kadambi, and Ramesh Raskar. 2022. Computational Imaging. MIT Press.\n\n\nBob Mellish. 2006. “Chromatic aberration diagram; CC BY-SA 3.0 license.” https://commons.wikimedia.org/wiki/File:Chromatic_aberration_lens_diagram.svg.\n\n\nBohren, Craig F, and Eugene E Clothiaux. 2006. Fundamentals of Atmospheric Radiation: An Introduction with 400 Problems. John Wiley & Sons.\n\n\nCanon. 2006. EF Lens Work III: The Eye of EOS. 8th ed. Canon Inc. Lens Products Group.\n\n\nChris Rorres. n.d. “Burning Mirrors: Refuting the Legend.” https://math.nyu.edu/Archimedes/Mirrors/legend/legend.html.\n\n\nDominic Alves. 2006. “Pinhole Size Chart; CC BY 2.0 license.” https://www.flickr.com/photos/dominicspics/4589206921.\n\n\nDrBob. 2007. “Refractive index vs. wavelength for BK7 glass; CC BY-SA 3.0 license.” https://en.wikipedia.org/wiki/File:Sellmeier-equation.svg.\n\n\nGlrx. 2018. “Ray diagram illustrating a form of coma aberration; CC BY-SA 3.0 license.” https://commons.wikimedia.org/wiki/File:Lens-coma.svg.\n\n\nHandWiki. 2024. “Kepler space telescope focal plane; CC BY-SA 3.0 license.” https://handwiki.org/wiki/index.php?curid=2015813.\n\n\nHecht, Eugene. 2016. Optics. 5th ed. Pearson.\n\n\nHurley, Ann C, G David Lange, and Peter H Hartline. 1978. “The Adjustable ‘Pinhole Camera’ Eye of Nautilus.” Journal of Experimental Zoology 205 (1): 37–43.\n\n\nInductiveload. 2009. “Mathematical function of an airy disk pattern; released into the public domain by the copyright holder.” https://commons.wikimedia.org/wiki/File:Airy_Pattern.svg.\n\n\nJürgen Königs. 2006. “Analog pinhole photography with multiple exposure; CC BY-SA 4.0 license.” https://commons.wikimedia.org/wiki/File:1)_Fenstertisch_mit_R%C3%BChrger%C3%A4t.jpg.\n\n\nMdf. 2005. “A simulation of spherical aberration in an optical system with a circular, unobstructed aperture admitting a monochromatic point source; released into the public domain by the copyright holder.” https://commons.wikimedia.org/wiki/File:Spherical-aberration-disk.jpg.\n\n\n———. 2006. “Imaging as a convolution against the PSF; released into the public domain by the copyright holder.” https://commons.wikimedia.org/wiki/File:Convolution_Illustrated_eng.png.\n\n\nMglg. 2008. “Conceptual ray diagrams of spherically aberrated lenses; released into the public domain by the copyright holder.” https://commons.wikimedia.org/wiki/File:Spherical_aberration_2.svg.\n\n\nMichael Schmid. 2008. “Graphic illustratic the astigmatism phenomenon; CC BY-SA 3.0 license.” https://commons.wikimedia.org/wiki/File:Meridional%2BSaggittalEbene_1.svg.\n\n\nMills, Allan A, and R Clift. 1992. “Reflections of the’burning Mirrors of Archimedes’. With a Consideration of the Geometry and Intensity of Sunlight Reflected from Plane Mirrors.” European Journal of Physics 13 (6): 268.\n\n\nRayleigh, Lord. 1879. “XXXI. Investigations in Optics, with Special Reference to the Spectroscope.” The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science 8 (49): 261–74.\n\n\nSakurambo. 2007. “A computer-generated image of an Airy disk (the grayscale intensities have been adjusted to enhance the brightness of the outer rings of the Airy pattern); released into the public domain by the copyright holder.” https://commons.wikimedia.org/wiki/File:Airy-pattern.svg.\n\n\nTallfred. 2005. “Text blurred by different focal positions of an astigmatic lens; 3-clause BSD License.” https://commons.wikimedia.org/wiki/File:Astigmatism_text_blur.png.\n\n\nThycoop/photographs. 2022. “Small park in logatec made with matchbox pinhole camera on kodak portra 400 film; CC BY-SA 4.0 license.” https://commons.wikimedia.org/wiki/File:Small_park_in_logatec_matchbox_pinhole_camera.jpg.\n\n\nTom.vettenburg. 2017. “Illustration of the optical transfer function and its relation to image quality; CC BY-SA 4.0 license.” https://en.wikipedia.org/wiki/File:Illustration_of_the_optical_transfer_function_and_its_relation_to_image_quality.svg.\n\n\nWadhwa, Neal, Rahul Garg, David E Jacobs, Bryan E Feldman, Nori Kanazawa, Robert Carroll, Yair Movshovitz-Attias, Jonathan T Barron, Yael Pritch, and Marc Levoy. 2018. “Synthetic Depth-of-Field with a Single-Camera Mobile Phone.” ACM Transactions on Graphics (ToG) 37 (4): 1–13.\n\n\nWolfWings. 2008a. “Barrel distortion visual example; released into the public domain by the copyright holder.” https://commons.wikimedia.org/wiki/File:Barrel_distortion.svg.\n\n\n———. 2008b. “Pincushion distortion visual example; released into the public domain by the copyright holder.” https://commons.wikimedia.org/wiki/File:Pincushion_distortion.svg.\n\n\nZhang, Yang, Fan Mao, Huawei Mu, Minwei Huang, Yongbo Bao, Lili Wang, Nai-Kei Wong, et al. 2021. “The Genome of Nautilus Pompilius Illuminates Eye Evolution and Biomineralization.” Nature Ecology & Evolution 5 (7): 927–38.",
    "crumbs": [
      "Imaging",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Imaging Optics</span>"
    ]
  },
  {
    "objectID": "imaging-optics.html#footnotes",
    "href": "imaging-optics.html#footnotes",
    "title": "15  Imaging Optics",
    "section": "",
    "text": "who won the Nobel Prize in Physics in 1904 and the namesake of the Rayleigh scattering↩︎\nmostly aberration, because defocus can be easily fixed and motion blur is out of the hands of an imaging system↩︎",
    "crumbs": [
      "Imaging",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Imaging Optics</span>"
    ]
  },
  {
    "objectID": "imaging-sensor.html",
    "href": "imaging-sensor.html",
    "title": "16  Image Sensor Architecture",
    "section": "",
    "text": "16.1 Overview\nThis chapter discusses image sensors, the devices that transform optical signals to electrical signals. We start from the basic principle that governs this signal transduction inside a pixel and then discuss how pixels are architected together to form an image sensor. We then turn to various in-sensor optics, which are not necessarily important for forming images but are important for forming visually pleasing images that, for instance, have realistic colors and are free of aliasing effects.\nThe main job of the sensor is to turn optical signals, i.e., the optical image impinging on the sensor plane, into electrical signals, i.e., digital images. This conversion is broken down into two steps, first by converting photons to charges followed by turning charges into digital numbers.\nFigure 16.1 (a) shows a cross-sectional view of the sensor hardware, which has three main components.\nFrom a computational perspective, we can model an image sensor as a signal processing chain, a transfer function \\(f\\), that transfers the optical signal to the electrical signal. Figure 16.2 visualizes this chain of signal processing. This chain of processing is best understood as computing on random variables. The input optical signal can be seen a random variable \\(R_o(\\mu_o, \\sigma_o)\\) with a mean and standard deviation of \\(\\mu_o\\) and \\(\\sigma_o\\), respectively. Every step in the signal processing chain not only manipulates the signal itself but also introduces/affects the noise. As a result, the output electrical signal is another random variable \\(R_e(\\mu_e, \\sigma_e)\\). So the transfer function, viewed this way, is:\n\\[\n    f: (\\mu_o, \\sigma_o) \\mapsto (\\mu_e, \\sigma_e),\n\\]\nAny imaging session can be seen as drawing a concrete value from the distribution of \\(R_o\\), and its output (raw pixel values) can be seen as drawing a value from the distribution of \\(R_e\\). An important goal of our study is to build an analytical model for this transfer function \\(f\\). For simplicity, we will first ignore noise as if \\(f\\) operates only on the mean signal. We will then discuss the sources of noise and how to model them.\nThere are two ways the pixels and the wires that read out the pixel outputs are physically arranged, shown in Figure 16.1 (b). In the back-side illumination (BSI) arrangement, the wiring of the circuitries is behind the photodiodes, which directly interface with the lights. In the front-side illumination (FSI) arrangement, the metal wiring sits between the light and the photodiodes. This means light could be absorbed and scattered through the metal layer before reaching the photodiodes, reducing the chance of a photon being properly captured. While earlier image sensors used FSI because it is easier to manufacture, almost all commercial image sensors use BSI now (Swain and Cheskis 2008).\nFSI is actually quite similar to the structure of human eyes, where, if you recall, the photoreceptors are “hiding” behind other retinal neurons such as the retinal ganglion cells, which are functionally the last layer of retinal processing but anatomically sitting at the first layer on the retina. Different from the FSI sensor, however, the non-photoreceptor neurons on the retina do very little to light: they do not absorb or scatter light much and can be generally thought of as transparent. Metal wires, of course, disrupt incident photons significantly.",
    "crumbs": [
      "Imaging",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Image Sensor Architecture</span>"
    ]
  },
  {
    "objectID": "imaging-sensor.html#sec-chpt-imaging-sensor-ov",
    "href": "imaging-sensor.html#sec-chpt-imaging-sensor-ov",
    "title": "16  Image Sensor Architecture",
    "section": "",
    "text": "Figure 16.1: (a): a conceptual, cross-sectional view of the sensor with the optical elements, photodiodes, and the peripheral circuitries. (b): comparison between 1) front-illuminated sensor, where lights have to first traverse through the peripheral circuitries before reaching the light-sensitive photodiodes, and 2) back-illuminated sensor, where lights can directly reach the photodiodes; from Cmglee (2019).\n\n\n\n\n\nFirst, there is a set of optical elements sitting on the sensor. These optical elements are not the imaging optics we discussed in the previous chapter because their main goal is not to form an image.\nSecond, under these optical elements are the photodiodes, which turn optical signals carried in photons to electrical signals in the form of electric charges.\nThird, interleaved with the photodiodes is the circuitry that processes the output of the photodiodes, turning charges into digital values.\n\n\n\n\n\n\n\nFigure 16.2: Image sensor can be seen as a chain of processing, or a transfer function, that transfers the optical signal, a random variable, with a mean \\(\\mu_p\\) and standard deviation \\(\\sigma_p\\) to the electrical signal, another random variable, with a mean \\(\\mu_y\\) and standard deviation \\(\\sigma_y\\). Redrawn based on European Machine Vision Association Standard 1288 EMVA (2021, fig. 1).",
    "crumbs": [
      "Imaging",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Image Sensor Architecture</span>"
    ]
  },
  {
    "objectID": "imaging-sensor.html#sec-chpt-imaging-sensor-pixel",
    "href": "imaging-sensor.html#sec-chpt-imaging-sensor-pixel",
    "title": "16  Image Sensor Architecture",
    "section": "16.2 From Photons to Charges and Digital Numbers",
    "text": "16.2 From Photons to Charges and Digital Numbers\nWe will talk about how optical signals are first converted to electrical signals in the form of charges, and then talk about how the charges are detected, at which point the electrical signals are manifested as voltage potentials. The voltage potentials are then quantized as digital numbers, which are the raw pixel values. We will focus on the basic building blocks that enable these conversions and leave it to Section 16.3 to discuss how these building blocks are connected in a global sensor architecture. The discussion here assumes monochromatic sensing without noise. We will talk about color sensing and the noise issue later.\n\n16.2.1 Photons to Charges\nWhat turns optical signals to electrical signals is the light-sensitive photodiode in a pixel. A photodiode is a p-n junction made of silicon, a semiconductor material. When a photon hits silicon and is absorbed, an electron from the silicon might be freed/emitted, transforming optical signals to electrical signals. This is called the photoelectric effect (Einstein 1905b, 1905a), the discovery of which won Albert Einstein his Nobel Prize.\nIn particular, when a photon is absorbed, if its energy is greater than or equal to the work function \\(\\phi\\) of the material, which is the minimum energy needed to free an electron from the surface of the material, the photon can transfer its energy to an electron and free the electron. A photon’s energy is given by the Planck’s relation:\n\\[\n    \\mathcal{E} = h f = \\frac{hc}{\\lambda},\n\\tag{16.1}\\]\nwhere \\(h\\) is the Planck constant, \\(f\\) is the photon frequency, and \\(c\\) is the speed of light. So if \\(h f &gt; \\phi\\), an absorbed photon can free an electron. Interestingly, the residual energy \\(hf - \\phi\\) becomes the kinetic energy of the electron, so a photon with a shorter wavelength (i.e., higher frequency) would allow the emitted electron to move faster.\nIt is clear that there is a frequency threshold \\(\\phi/h\\), lower than which a photon would never be able to free an electron. Higher than the threshold, there is generally a one-to-one mapping between an absorbed photon and an emitted electron: an absorbed photon always frees an electron. Since the work function of silicon is about 1.1 eV (electron volt), absorption of photons with wavelengths longer than 1,100 nm would not emit any electron.\n\nQuantum Efficiency\nA key figure of merit in image sensing is the notion of quantum efficiency (QE), which is the ratio between the number of electrons collected and the number of incident photons:\n\\[\n    QE = \\frac{\\#\\text{~of electrons collected}}{\\#\\text{~of incident photons}}.\n\\tag{16.2}\\]\nFigure 16.3 (a) shows the QE spectrum of an image sensor in the Hubble Space Telescope. It might come as a surprise that QE is lower than 1 (even for wavelengths well within the 1,000 nm threshold) and is actually wavelength dependent: shouldn’t every absorbed photon (within the wavelength threshold) always free an electron? There are two reasons.\n\n\n\n\n\n\nFigure 16.3: (a): quantum efficiency of a sensor on the Hubble Space Telescope; from Eric Bajart (2010) with data from Biretta and McMaster (2008, fig. 4.2). (b): silicon absorption coefficient (left axis) and mean free path (right axis) as a function of wavelength; data from Green and Keevers (1995).\n\n\n\nFirst, the denominator in the QE definition is the number of incident photons, not the number of absorbed photons. Not all photons that hit the photodetector will be absorbed. Figure 16.3 (b) shows the spectral absorption coefficient \\(\\sigma\\) (unit 1/cm) of silicon on the left \\(y\\)-axis, and the right \\(y\\)-axis shows the corresponding mean free path \\(l\\) (i.e., the expected length a photon can travel within silicon before being absorbed) at different wavelengths; recall from Equation 12.7 that \\(l = 1/\\sigma\\). We can see that absorption is strongest for the blue-ish lights but decays very rapidly toward the longer wavelengths. This definition of QE is different from how QE is defined in human vision. Recall from Section 3.1; there, QE is the probability of pigment excitation once the pigment actually absorbs a photon; there, the QE of photopigment is roughly two-thirds and is not wavelength-sensitive.\nSecond, the nominator in the QE definition is the number of collected, not emitted, electrons: even if an electron is freed by an absorbed photon, that electron might not actually be collected and contribute to the electrical signal. Depending on where the electrons are freed, some of them need to go through a random walk (think of it as Brownian motion) before being collected, and you can imagine some electrons can be recombined with the holes during the walk. \nGiven QE, the total number of emitted electrons after an exposure time \\(t_{exp}\\) is given by:\n\\[\n    N = \\int_{\\lambda} QE(\\lambda) Y(\\lambda) \\text{d}\\lambda,\n\\tag{16.3}\\]\nwhere \\(Y(\\lambda)\\) is the number of photons incident on a photodiode at a particular wavelength \\(\\lambda\\) at time \\(t\\) during the exposure time \\(t_{exp}\\), assuming that \\(Y\\) is invariant during \\(t_{exp}\\) here.\nAccording to the Planck’s relation (Equation 16.1), \\(Y(\\lambda)\\) is related to the spectral power distribution (SPD) of the incident light \\(\\Phi(\\lambda)\\) by: \\(Y(\\lambda) = \\frac{\\Phi(\\lambda, t) t_{exp} \\lambda}{hc}\\), where \\(\\Phi(\\lambda) t_{exp}\\) is spectral energy distribution. Therefore, we have:\n\\[\n    N = \\int_\\lambda QE(\\lambda) \\frac{\\Phi(\\lambda) t_{exp} \\lambda}{hc}  \\text{d}\\lambda.\n\\tag{16.4}\\]\nNote that we define QE for the photodiode itself: the denominator in Equation 16.2 refers to the number of photons incident on the photodiode, not those that enter the camera system. This is an important distinction, because many photons that enter the camera would not even make their way to the photodiode; some of them are reflected at the lens surfaces, and others are absorbed by the various filters (Section 16.4). In many contexts, the QE is reported with respect to the entire camera system, where the denominator is the number of photons entering the camera, in which case the QE would be lower than that of the photodiode. Always ask what the precise definition of a QE is when reading the literature.\n\n\n\n16.2.2 Measuring Charges\n\nBasic Principle\nNow that we have turned photons to charges — the freed electrons move to the n region and the holes move to the p region of the p-n junction — the next step is to measure the charges. The basic principle of doing so is using a capacitor: we use the electrons to discharge a capacitor with a known capacitance; by measuring the voltage difference before and after the discharge, we can then estimate the number of emitted electrons:\n\\[\n    \\Delta V = \\frac{\\mathcal{Q}_{sig}}{C_{FD}} \\times g = \\frac{N q}{C_{FD}} \\times g,\n\\tag{16.5}\\]\nwhere \\(\\mathcal{Q}_{sig}\\) is the charge in the signal used to discharge the capacitor, which is usually a floating diffusion (see later) that has a capacitance of \\(C_{FD}\\), and \\(g\\) is the voltage gain of whatever device is used to read out the voltage, usually a source follower (see later). \\(\\mathcal{Q}_{sig}\\) itself is the product of \\(N\\), the number of charges in the signal, and \\(q\\), the elementary charge.\n\\(\\frac{q}{C_{FD}}\\) is also called the conversion gain (CG) of the pixel. CG has a unit of \\(\\text{Volt}/\\text{e}^-\\) and can be interpreted as the amount of voltage change per charge. CG is a very important quantity. A high CG means the output voltage change is very sensitive to small amount of input light change, which is good for improving the signal-to-noise ratio (SNR). In contrast, a small CG means the output voltage change is small given the same amount of light change, and that small voltage change becomes very difficult to detect in the presence of noises, resulting in a low SNR. While desirable from a noise perspective, a high CG necessarily means a smaller capacitor, which is easier to fill up (saturate). We will get back to this point when discussing dynamic range (Section 16.2.4).\nWe can see that once we can measure \\(\\Delta V\\), we can get an estimate of \\(N\\). Why do we care about \\(N\\)? Intuitively, the incident light luminance is positively related to \\(N\\): more incident photons means higher luminance. Luminance \\(L\\), if we are interested in only grayscale, monochromatic imaging, is ultimately what we want to estimate.\nIt is important to realize that the actual relationship between \\(L\\) and \\(N\\) is not linear. We know that luminance is defined as:\n\\[\n    L = \\int_{\\lambda} V(\\lambda) \\Phi(\\lambda) \\text{d}\\lambda,\n\\tag{16.6}\\]\nwhere \\(V(\\lambda)\\) is the luminance efficiency function (LEF) and \\(\\Phi(\\lambda)\\) is the SPD of the incident light. Taking Equation 16.6 and Equation 16.4 together, we can see that given \\(N\\), we cannot quite estimate \\(L\\), because \\(L\\) depends on \\(\\Phi(\\lambda)\\), but estimating \\(\\Phi(\\lambda)\\) from \\(N\\) is an under-determined problem, as Equation 16.4 shows. To be exact, \\(L\\) does not necessarily scale linearly with \\(N\\) — it does not even necessarily scale positively with \\(N\\), but it is perhaps not terribly wrong to informally say a higher charge count means a higher luminance in the scene.\n\n\n4T Design\nThe photodiode (PD) technically acts as a capacitor itself (the n-side neutral region holds electrons and the p-side neutral region holds holes), so we could simply use the PD for that purpose. This is indeed how an earlier pixel design works, which we will return to shortly. Modern pixels actually transfer the charges from the PD to a separate measurement node, which we focus on here.\nFigure 16.4 (a) shows the circuit diagram of a typical pixel design that detects and measures the charges. The design has a PD and four transistors, so it is usually called the 4T design. The M-TX switch controls the transfer of the charges accumulated in the PD to the Floating Diffusion (FD)1, another capacitive area, and is sometimes called the measurement node, the sense node, or the conversion node, because that is where the charges are actually being measured. The FD is connected to the NMOS Source Follower (SF) transistor M-SF, where the gate terminal is its input and is connected to the FD voltage, the drain is connected to the supply voltage, and the source is the output that faithfully follows/transfers the input with a gain of about 0.9 (\\(g\\) in Equation 16.5).\n\n\n\n\n\n\nFigure 16.4: (a): circuit diagram of a typical 4T pixel design; adapted from Ma (2024, fig. 2.5(a)). (b): timing diagram of operating a 4T pixel.\n\n\n\nThe sequence of operation goes roughly like the following, and Figure 16.4 (b) shows the corresponding timing diagram:\n\nBefore the exposure, we turn on the M-RST switch and the M-TX to drain the charges (electrons) at the PD, which will also, as a byproduct, drain the charges in the FD, resetting their voltage potentials both to \\(V_{RST}\\). Resetting the FD voltage at this step is of no functional use, as we will shortly see.\nWe then turn off M-RST and M-TX, and the exposure begins, during which the charges are collected inside the PD. We can see from Equation 16.5 that in order to measure the charges we need to measure the voltage difference at the FD node before and after the charges are transferred. So toward the end of the exposure, we turn on the M-RST switch again while, importantly, keeping the M-TX switch off. This would allow us to reset the FD voltage to \\(V_{rst}\\), which will be measured through M-SF as \\(V_1\\) in Figure 16.4 (b)2.\nWe then turn on the M-TX switch, which transfers the charges from the PD to the FD. After that, we turn off M-TX and read the voltage from M-SF for the second time, this time for the voltage at FD after the charge transfer. This is the \\(V_2\\) in Figure 16.4 (b). The difference between \\(V_1\\) and \\(V_2\\) is the \\(\\Delta V\\) in Equation 16.5.\n\nAs we can see, we read the voltage of the FD twice to obtain the voltage difference caused by the charges collected during the exposure. This is called Correlated Double Sampling (CDS), which turns out to also be very important to mitigate many noise sources, which we will discuss later.\nTo read out the voltage from the SF, the M-SEL switch needs to be turned on, which is omitted from Figure 16.4 (b) for simplicity. As we will shortly see in Section 16.3, in most cases (although not all), pixels are read out row by row, so the M-SEL switches of all pixels in the same row are connected to the same signal, usually called the row select signal.\nThe timing diagram in Figure 16.4 (b) is illustrative of the major operations (omitting M-SF) but not drawn to scale. The exposure time is usually at the tens of milliseconds scale (e.g., 30 FPS means roughly a 33.3 ms exposure time), but the timescale to operate the transistors/switches is at the microsecond level. Also observe, in Figure 16.4 (b), that during the exposure the voltage at the FD (\\(V_{FD}\\)) slowly reduces from \\(V_{rst}\\) after the first reset — because of the charge leakage in the FD, just like how DRAM cells leak. This is why we need the second reset to bring the voltage at FD back to \\(V_{rst}\\) before charge transfer. This is also why we say the first reset is of no functional use to the FD (but of course very important to the PD because we want the PD to collect only electrons emitted from the current exposure).\n\n\n4T APS vs. 3T APS vs. PPS\nThe (4T) pixel design described above is called an Active Pixel Sensor (APS) design, first conceived by Noble (1968) (see Fossum (1993) for a more modern perspective). An APS has a per-pixel SF (a common-drain amplifier) that “actively” reads out the signal for each pixel by turning its charges to voltage. We briefly discuss the other, older pixel designs that are less commonly used now. See El Gamal and Eltoukhy (2005) for a more detailed discussion and visual comparisons.\n\n\n\n\n\n\nFigure 16.5: Left: 3T APS vs. 4T APS. Top right: Passive Pixel Sensor (PPS). Bottom right: Digital Pixel Sensor (DPS). Adapted from El Gamal and Eltoukhy (2005, figs. 5, 10, 11).\n\n\n\nA simpler and earlier version of the APS design uses only three transistors (3T) without the gate. Figure 16.5 (left) compares the 4T APS with the 3T APS. Without the transfer gate, the PD is used as the measurement/sensor node itself, so the \\(C_{FD}\\) in Equation 16.5 is effectively the capacitance of the PD itself. The 3T APS simplifies the pixel design and, thus, increases the fill factor (without the microlenses). It, however, generally suffers from a lower signal-to-noise ratio (SNR) for a variety of reasons. For instance, the PD has a large inherent photodiode capacitance, so the signal (\\(\\Delta V\\) in Equation 16.5) read from the PD is low, making it more vulnerable to noise. In contrast, we get to control the FD in the 4T APS, which can be made to have a much lower capacitance, leading to a higher SNR. The CDS for 3T APS is also much less effective in suppressing noise, as we will discuss later.\nA precursor to APS was the Passive Pixel Sensor (PPS), first suggested in Weckler (1967) and Dyck and Weckler (1968). A PPS has only one transistor, as shown in the top-right panel in Figure 16.5. The PPS has no SF that reads out voltage from the PD charges. Instead, the charges (not voltage) in the PD “passively” flow through a column bus and are turned to voltage there through a charge amplifier (Aoki et al. 1982).  The PPS design is simpler (as only one transistor is needed) but leads to a much worse noise profile because of the large (parasitic) capacitance of the column bus. The SF in APS acts as an active amplifier, which isolates the sense node (whether it is the PD or the FD) from the large column bus capacitance, providing a much higher output current and lower output impedance than a PD does and, thus, improving the SNR (Kozlowski et al. 1998; Ohta 2020, chap. 2.5).\n\n\nElectronic Shutter\nIdeally, when we are not capturing light, the photodiodes should not be exposed to lights. This is achieved by a shutter. Mechanical shutters do so by physically blocking lights. The sensor is not exposed to light normally, blocked by the shutter. The shutter then mechanically opens to expose the sensor to light. There are many types of mechanical shutters, of which the most popular one is the focal plane shutter shown in Figure 16.6 (a). The shutter has two curtains that move in sync with a gap that allows lights in. The size of the shutter opening and the speed of the movement dictate the exposure time: a larger opening and slower speed mean longer exposure time. This is called a focal plane shutter because the shutter is located in front of the focal plane (sensor). There is also the leaf shutter, which is usually located at the aperture plane with the lenses.\n\n\n\n\n\n\nFigure 16.6: (a): a mechanical focal-plane shutter, which is inherently a rolling shutter; adapted from Ommnomnomgulp (2008). (b): rolling shutter artifact; from BrayLockBoy (2018).\n\n\n\nThe 4T pixel design above essentially implements an electronic shutter (ES). With an ES, we expose photodiodes to lights all the time. The way we mark the start of the exposure is through the M-RST switch, which resets the PDs, and the way we mark the end of the exposure is through the M-TX switch, which transfers the PD charges for measurement. The time difference between these two steps dictates the exposure time. As you can imagine, the shutter speed (inverse of the exposure time) of an electronic shutter can be much faster than that of a mechanical shutter, since there are no mechanical moving parts.\n\n\n\n16.2.3 Read-out Circuitry\nFollowing the pixel circuitry is the read-out circuitry, which usually has two main components: the programmable-gain amplifier and the analog-to-digital Converter (ADC). Figure 16.7 illustrates the common, simplified designs of the two components.\n\n\n\n\n\n\nFigure 16.7: (a): analog CDS and programmable amplifier; from Ma (2024, fig. 2.5(b)). (b): a single-slope ADC typically used in image sensors; adapted from Ma (2024, fig. 2.5(c)).\n\n\n\nThe amplifier is there to amplify the voltage read from the pixel, and the gain of the amplifier is programmable. A programmable gain is useful in imaging and photography to artificially shorten or extend the exposure time (e.g., through the ISO setting in a digital camera). The particular design shown in Figure 16.7 (a) combines CDS with a classical amplifier design with two capacitors. Specifically, the two voltages read out from the FD (one right after the reset and the other right after the charge transfer) are sampled by the \\(C_{in}\\) capacitor sequentially, which essentially performs an analog-domain subtraction that is required by CDS. The voltage difference is then amplified with a gain \\(\\frac{C_{in}}{C_{feedback}}\\). \\(C_{feedback}\\) is usually programmable, allowing us to control the gain.\nThe amplified voltage difference then goes through an ADC to obtain the digital value. There is a huge amount of ADC designs (Murmann 2014). The design that is commonly used in image sensors is the single-slope (SS) design, whose simplified diagram is shown in Figure 16.7 (b). An SS ADC consists of a comparator, a ramp signal generator, and a counter. The ramp generator provides a monotonically increasing or decreasing ramp signal, which is compared with the to-be-quantized analog signal (output of the amplifier). At every clock cycle, the comparator compares the two inputs while the counter increments. When the two input signals cross, the counter value is recorded and represents the quantized digital value of the analog signal.\nThe designs in Figure 16.7 perform CDS in the analog domain (through \\(C_{in}\\)). In many image sensors today, the CDS is performed in the digital domain after the ADC (Nitta et al. 2006). You would think that such a design might require twice the ADC overhead plus the additional digital subtraction overhead. In reality, the design is quite clever. The ADC would first quantize the first sample (before reset), and the resulting counter value represents the digital value of the first sample. For the second sample, instead of counting from scratch, we would simply turn the counter around so that it counts backward. At the end, the counter value is naturally the digital difference of the two samples.\n\n\n16.2.4 Dynamic Range\nWe can intuitively think of each pixel as a well (a pixel well) that collects electrons. Equation 16.4 indicates that there are two main factors that determine the number of electrons going into a particular pixel well: the incident light power and the exposure time. A pixel cannot indefinitely collect electrons. The full-well capacity (FWC) is the max amount of electrons that can be held by a pixel’s photodiode. More electrons than the FWC would saturate the well, at which point no charges will be stored by the pixel. When a pixel well is saturated, photographers call that pixel “over-exposed”. This is illustrated in Figure 16.8, where, ordinarily, the number of charges collected is proportional to the incident light luminance until the pixel well is full.\n\n\n\n\n\n\nFigure 16.8: Illustration of dynamic range, which is the ratio of the FWC and the noise floor; adapted from Axel Jacobs (2006). Incident luminance higher than the FWC saturates a pixel, leading to over-exposure.\n\n\n\nA larger FWC leads to a higher sensor dynamic range, which, informally, refers to the range of scene luminance that a sensor can capture. Formally, the dynamic range is defined as the ratio between the highest and the lowest luminance level that can be faithfully captured. The highest level is the FWC, but what about the lowest level? Wouldn’t that simply be 0 and, if so, wouldn’t the dynamic range of any image sensor be infinity?\nThe answer is that at very low light levels the charges collected by a pixel are dominated by noise. We call the charges collected when there is no incident light the “noise floor”, which can be measured by taking an image when the camera is in dark. The dynamic range is thus the ratio between the FWC and the noise floor (Nakamura 2006, chap. 3.4.2.1):\n\\[\n    \\text{DR} = \\frac{\\text{FWC}}{\\text{Noise Floor}}\n\\]\nWe discuss noise in detail in Chapter 17 and will not get into it too much here, but briefly, the noise floor is dominated by “dark noise”, which is caused by the thermally dislodged electrons, and the “read noise”, which is the noise introduced by the read-out circuitry.\nNot only can saturation occur at a PD’s well, it can also occur when transferring the charges from the PD to the FD during read-out. As we have briefly alluded to when discussing the conversion gain (CG) in Section 16.2.2, when the CG is low, the SNR is high but we need to use a small FD, whose capacity could sometimes be smaller than that of the PD, in which case the charge transfer might saturate the FD. Alternatively, a large FD will not saturate (during charge transfer) but will lead to a low CG and, thus, lower SNR.\nA technique that many image sensors use is called dual conversion gain (DCG), where a pixel’s charges can be read-out twice, once with a high conversion gain (HCG) and the second time with a low conversion gain (LCG) (Solhusvik et al. 2019; Willassen et al. 2015; Huggett et al. 2009; Miyauchi et al. 2020; Takayanagi et al. 2018). To support the LCG read-out, we need an (or sometimes multiple) extra capacitive node, e.g., an additional FD (let’s call that \\(FD_2\\)), that is connected in parallel with the original FD (let’s call that \\(FD_1\\)) so as to increase the effective \\(C_{FD}\\) in Equation 16.5.\n\nIn the first HCG read-out, we use only \\(FD_1\\) but not \\(FD_2\\). This reading has a high HCG and high SNR, which is especially important for dark parts of the scene. For the bright areas, however, \\(FD_1\\) saturates and the readings are useless. Importantly, however, the left-over charges are not discarded; they still stay in the PD.\nThen in the subsequent LCG read-out, the extra \\(FD_2\\) is switched in. Now all the charges, including the left-over ones in the PD and the charges in \\(FD_1\\), are then re-distributed to \\(FD_1\\) and \\(FD_2\\), which collectively will not saturate, so highlights are captured at the cost of low SNR.\n\n\n\nHigh Dynamic Range Imaging\nThe goal of high-dynamic-range (HDR) imaging is to design imaging systems such that the scene luminance can be faithfully reconstructed from pixel values. Two things are in the way: noise at low-luminance regions in the scene and saturation at high-luminance regions in the scene. A common strategy for HDR is called exposure bracketing, which can be implemented in two ways, both involving taking multiple shots of the scene and then fuse them.\n\nEach shot has the same, short exposure time so no pixel is over-exposed, but pixels for low-luminance regions are noisy. We then average multiple shots; averaging is a form denoising (Chapter 17). This is the approach that Google’s HDR+ system takes (Hasinoff et al. 2016).\nEach shot has a different exposure time. Long-exposure shots are used to capture details in low-luminance regions, and short-exposure shots capture details in high-luminance regions.\n\nEither way, the issue with exposure bracketing is the longer capturing time, which makes the resulting image more susceptible to motion blur. We ideally would like “single-shot” HDR. There are multiple methods, and they usually require co-designing the image sensor/pixel with the post-processing algorithms (aside from modern deep learning approaches that rely semantics information, which we will not discuss).\nOne strategy is to use split pixels or dual PDs, an emerging technology that sensor companies are exploring. The idea is to use split a pixel into two PDs, each with a different “sensitivity” to light (Iida et al. 2018; Solhusvik et al. 2019; Willassen et al. 2015; J. Xu et al. 2022). The sensitivity is usually controlled by PD size (and the corresponding microlens size): the larger PD (LPD) can collect more charges at the same light intensity (quantified by photons/area) than the small PD (SPD)—simply because of the large photon collection area—and, thus, saturate faster. The two groups of PDs are interleaved on the sensor plane, so they each perform a uniform sampling of the scene (preceded by a spatial integration over the pixel area of course).\n\n\n\n\n\n\nFigure 16.9: Illustration of how the split-pixel architecture, where a pixel has a large PD (LPD) and a small PD (SPD) and LOFIC extend the dynamic range (DR). Dual conversion gain (DCG), in this example, is applied to the LPD. Drawn based on J. Xu et al. (2022).\n\n\n\nThe way that split pixels extend dynamic range is illustrated in Figure 16.9. The LPD, with a FWC of \\(S_3\\), saturates at a low luminance level \\(L_1\\), so only those (large) pixels that image low-luminance regions in the scene do not saturate; as a result, LPDs provide a good sampling of the low-luminance information. In contrast, the SPD, with a lower intrinsic FWC of \\(S_2\\), saturate at a high luminance level \\(L_2\\), so SPDs provide a good sampling for high-luminance information in the scene. Note that even though the SPD has a smaller intrinsic FWC than that of the LPD, the SPD’s sensitivity to light is even lower3, so the SPD still saturates at a higher intensity level.\nIf we increase the FWC of the small pixels, they take even longer/higher luminance to saturate. The way we increase the FWC is by adding a lateral overflow integration capacitor (LOFIC), which holds the overflow charges from the PD during exposure (Sugawa et al. 2005; Akahane et al. 2006; Takayanagi et al. 2019; Ikeno et al. 2022). In almost all cases, the FD itself participates in collecting the overflow charges, too. In this way, the FWC of the small pixels, \\(S_4\\), is effectively the total capacity of the photodiode, the LOFIC, and the FD. This further extends the small pixel’s saturation level to \\(L_3\\), shown in Figure 16.9.\nLOFIC can be used in conjunction with DCG. For instance, in the HCG read-out we would use only the FD as the measurement node, and in the LCG read-out we would use both the FD and LOFIC (Takayanagi et al. 2019). Of course we can also add additional FDs to lower the conversion gain even more (Iida et al. 2018). \nWe could also combine the split-pixel architecture with DCG (Iida et al. 2018; Solhusvik et al. 2019; Willassen et al. 2015), where usually the large pixels are read-out with twice with DCG and the small pixels are read-out with only LCG; this is because large pixels are meant to sample low-luminance information so they benefit more from HCG. This is shown in Figure 16.9, where \\(S_1\\) is the capacity of the LPD’s FD node, which saturates at a lower intensity than \\(L_1\\) and is the measurement node in the HCG read-out. The LCG read-out can read all the charges in the FWC (with the help of an additional FD) at the cost of a lower conversion gain.\nAnother approach is the time-to-saturation (TTS) technology (Stoppa et al. 2002), which uses a counter to measure the time it takes for each pixel to saturate and use that time to extrapolate the information given the actual exposure time:\n\\[\n    Q_{\\text{act}} = Q_{\\text{sat}} \\frac{T_{\\text{exp}}}{T_{\\text{sat}}},\n\\]\nwhere \\(Q_{\\text{act}}\\) is the actual number of charges a pixel would have collected without saturation, \\(Q_{\\text{sat}}\\) is the FWC, \\(T_{\\text{exp}}\\) is the exposure time, and \\(T_{\\text{sat}}\\) is the saturation time. One could combine TTS with DCG and LOFIC (Ikeno et al. 2022; Liu et al. 2020, 2022).",
    "crumbs": [
      "Imaging",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Image Sensor Architecture</span>"
    ]
  },
  {
    "objectID": "imaging-sensor.html#sec-chpt-imaging-sensor-arch",
    "href": "imaging-sensor.html#sec-chpt-imaging-sensor-arch",
    "title": "16  Image Sensor Architecture",
    "section": "16.3 Global Architecture",
    "text": "16.3 Global Architecture\nWe have discussed the individual building blocks that are needed for a pixel to turn lights into digital values, but how are they put together in an actual image sensor supporting tens of millions pixels? This chapter talks about the global architecture of an image sensor. We will start with a common architecture followed by other variants.\n\n\n\n\n\n\nFigure 16.10: (a): the block diagram of a typical rolling-shutter image sensor with column-level amplifiers and ADCs, where pixels in the same column share the same amplifier and ADC; pixels are exposed and read out row by row under the control of the RST signal (connecting to the M-RST switches) and the SEL signal (connecting to the M-SEL switches) (for simplicity, we omit the per-row TX signal, which connects to all the M-TX switches in the same row); (b): timing diagram operating the image sensor in (a) with a rolling shutter; technically the FD reset should be overlapped with the exposure time but is lumped into the readout box for simplicity. (c): comparison of column-level ADC used in (a) with pixel-level ADC and array/chip-level ADC. (d): timing diagram operating the image sensor in (a) with a global shutter.\n\n\n\n\n16.3.1 Column-Parallel Readout\nFigure 16.10 (a) shows a typical arrangement, where pixels are organized as a 2D array, just like a (DRAM/SRAM) memory array, and each column has an amplifier and ADC shared by all the pixels in that column. That is, the Output pin in Figure 16.4 of all the pixels in the same column are connected to the same amplifier and ADC. The read-out circuit is then connected to digital processing circuitry, which could potentially perform simple image-space operations such as downsampling, scaling, rotation, etc. There is also an I/O unit that transfers the pixels to the host processor, usually through the MIPI-CSI interface, and transfers commands/configuration data from the host processor, usually through the I2C interface, which has a much lower bandwidth than MIPI (Kb/s vs. Gb/s).\nThe pixels in the pixel array are addressed row by row through a row scanner logic, shown on the left of Figure 16.10 (a). Pixels in the same row share three external signals: a reset signal RST, which is connected to all the M-RST transistors in the row, a row-select signal SEL, which is connected to all the M-SEL transistors of the same row, and a transfer signal TX (omitted in the figure) connected to all the M-TX switches in the same row.\nThe operating sequence of the pixel rows is shown in Figure 16.10 (b); the times are not drawn to scale. Each row of pixels goes through the PD reset, exposure, and readout phases under the control of the three external signals (RST, SEL, and TX). Importantly, the three phases are pipelined across rows. That is, while the first row is being exposed, we can start resetting the PDs for the subsequent rows and preparing them for exposure. For instance, in the concrete example of Figure 16.10 (a), the first row is starting the read-out sequence, the nth row is starting the exposure, while all other rows in-between are currently under exposure. While the exposure times of different rows can overlap, their readout sequences cannot — pixels in the same column but different rows share the same the read-out circuitry.\nWe can see that the way the pixel array is addressed and operated is similar to how a memory array (e.g., SRAM/DRAM) is, where the data in an entire row is accessed at once. However, since the pixel rows are operated strictly sequentially (unless random sampling is needed (Feng et al. 2024)), the row scanner logic does not need a decoder, which supports random accesses that a typical memory array would need. Instead, one can usually use parallel shift registers to generate the three external signals row by row.\n\n\n16.3.2 Rolling vs. Global Shutter\nThe timing diagram suggests that pixels in different rows technically have slightly shifted exposure times, inherently using a rolling shutter. The mechanical focal-plane shutter shown in Figure 16.6 (a) is inherently a rolling shutter. Rolling shutters introduce noticeable artifacts; one such example is shown in Figure 16.6 (b), where the photo was taken by a camera traveling in a car driving at about 50 mph. As a result, the fence and gate appear slanted because vertical parts of these objects are taken at different times. Such an artifact is much less visible for more distant objects, such as the cliff (can you reason about why?).\nGlobal shutters address the rolling shutter artifacts by exposing all pixels at the same time. Figure 16.6 (d) shows the timing diagram of a global shutter sensor; compare that with that of the rolling shutter sensor in Figure 16.6 (a). All the PDs are reset at the same time and have the same exposure duration.\nThe pixels are still read out row by row due to the column-level design of the read-out circuitry. This means the pixel values have to be temporarily held in some form of analog buffer after exposure and before they are read out. One could certainly use the FD for this analog buffer — with the caveat that the this prevents the PD from starting a new exposure cycle. This is because starting a new exposure requires resetting the PD, which would also reset the corresponding FD, as shown in Figure 16.4 (a). For that reason, it is common to implement an additional analog buffer inside each pixel. The buffer can be implemented either in the charge domain before the FD (Yasutomi, Itoh, and Kawahito 2011; Sakakibara et al. 2012; Tournier et al. 2018; Y. Kumagai et al. 2018; Yokoyama et al. 2018; Kobayashi et al. 2017) or implemented in the voltage domain after the FD (Kondo et al. 2015; Stark et al. 2018; Miyauchi et al. 2020). \n\n\n16.3.3 Pixel-Parallel and Chip-Level Readout\nWe can also arrange the read-out circuitry differently, as illustrated in Figure 16.10 (c). For instance, we could have a per-pixel (gain-controllable) amplifier and ADC and, consequently, a per-pixel digital memory. This essentially allows each pixel to directly output digital values, giving rise to the so called Digital Pixel Sensor (DPS) design, which was first reported in Fowler, El Gamal, and Yang (1994) and is recently gaining tractions (Liu et al. 2019), where the in-pixel memory can is a 6T SRAM cell and the entire pixel array acts almost like an SRAM array. The bottom-right panel in Figure 16.5 shows the pixel design diagram of a DPS, where the in-pixel memory can be, for instance, a 6T SRAM cell. In this case, the entire pixel array is indeed like an SRAM array.\nDPS increases the pixel design complexity and pixel sizes, which, without microlenses, reduces the fill factor. This can, however, be alleviated with a stacked design, which we will get to in Section 16.3.5. The main advantage of the DPS is that it massively increases the readout bandwidth due to pixel-parallel ADCs, which could shorten the frame latency when using a global shutter (see Figure 16.10 (d)), especially when short exposure time is desirable (e.g., high frame rate or “snap-shot” photography).\nYet another read-out arrangement is to have a single gain-controllable amplifier and ADC for the entire pixel array. This is shown in Figure 16.11 (b). In this case, we not only need logic to scan rows one by one but also to scan columns one by one (e.g., through shift registers). This arrangement is not common (thus omitted in Figure 16.10 (c)) due to its slow read-out speed but is the only option for sensors based on the Charge-coupled Devices (CCD), a design that is different from all the designs we have discussed so far and is our focus next.\n\n\n\n\n\n\nFigure 16.11: (a) charge-shifting read-out architecture for CCDs. (b) read-out architecture for CMOS image sensors with a global, array-level amplifier. Adapted from Nakamura (2006, fig. 3.5).\n\n\n\n\n\n16.3.4 CMOS vs. CCD Sensor\nAll the sensor designs we have covered so far are called Complementary Metal-Oxide-Semiconductor (CMOS) sensors, because they heavily rely on circuitries implemented using the CMOS techonlogies. CCD sensor is the other major category of sensor design, first reported in Boyle and Smith (1970). Both CCD and CMOS sensors use silicon to implement the PDs, although the specific implementations can differ (Nakamura 2006, chap. 3.1.2). The main difference lies in how the charges generated by the PDs are read out. See Fossum (1993), Fossum (1997), El Gamal and Eltoukhy (2005), and more recently, Fossum, Teranishi, and Theuwissen (2024) for the historical background and comparisons.\nA CCD sensor directly reads out charges from pixels by shifting the collected charges row by row. When a row reaches the bottom of the pixel array, we then shift the charges column by column to a single, array-level SF amplifier (and potentially a gain-controllable amplifier and ADC afterwards). This architecture is shown in Figure 16.11 (a). In CMOS sensors, in contrast, the charges are converted to voltages within the pixels, and it is the voltage potentials that are being read out from the pixel array by addressing, rather than shifting across, individual rows. The CMOS architecture is shown in Figure 16.11 (b).\nThe key to a CCD sensor is the charge-coupled devices themselves. A CCD is a set of connected MOS capacitors that store and transfer, between them, charges (Hu 2009, chap. 5), invented by Willard Boyle and George E. Smith (Boyle and Smith 1970)4. In a CCD image sensor, the CCDs are connected to the PDs. After the exposure, all the PDs simultaneously transfer their charges to the corresponding vertical CCDs. The vertical CCDs in the same column then act as a shift register, transferring the charges downward to the horizontal CCD at the bottom of the chip. When a row of charges reaches the horizontal CCDs, the charges are then transferred horizontally (again, in a shift-register fashion) to the SF amplifier, which turns charges to voltage.\nGiven this signal read-out architecture, it is perhaps unsurprising to see that CCD sensors inherently support global shutters: the CCDs used for shifting charges naturally store the charges temporarily during the read-out.\nCCDs are fabricated using process technologies that are optimized for charge transfer and that are incompatible with the CMOS technologies. In contrast, the read-out architecture of the CMOS sensors can be fabricated using CMOS technologies. This is a huge advantage because non-imaging logics such as control (e.g., clock generation) and analog/digital processing (e.g., ADC, image processing, computer vision tasks) are also based on CMOS technologies. Such logics, in CCD sensors, need to be implemented on a separate chip that interfaces with the CCD chip, rather than integrated with the pixel array on the same chip in a CMOS image sensor.\nAs modern CMOS technologies mature and gradually take over the semiconductor industry, CMOS image sensors have become more appealing. The main advantage of the CCD sensors is their high SNRs. CCD sensors do not have active devices during read-out and, thus, avoid/minimize many sources of noise that CMOS sensors are vulnerable to, a point we will return to when discussing noise modeling5. Because of that, while consumer cameras today mostly use CMOS sensors, CCD sensors are still use widely used in many scenarios where imaging quality is critical, e.g., scientific imaging. For instance, many telescopes for astrophysics (e.g., Sloan Digital Sky Survey) still use CCD sensors.\n\n\n16.3.5 Computational and Stacked CMOS Image Sensors\nBecause the imaging circuitries and the logic processing circuitries both use the CMOS process technologies, a clear trend in CMOS Image Sensor (CIS) design is to move into the sensor computations that are traditionally carried out outside the sensor, which gives rise to the notion of Computational CIS.\n\nCIS Scaling Trends\nFigure 16.12 (a) shows the percentage of computational CIS papers in International Solid-State Circuits Conference (ISSCC) and International Electron Devices Meeting (IEDM), two premier venues for semiconductor circuits and devices, from Year 2000 and Year 2022 with respect to all the CIS papers during the same time range. The trend is clear: increasingly more CIS designs integrate compute capabilities.\n\n\n\n\n\n\nFigure 16.12: (a) Percentage of conventional CIS, computational CIS, and stacked computational CIS designs from surveying all ISSCC and IEDM papers published between the year 2000 and 2022. Increasingly more CIS designs are computational. (b) CIS process node always lags behind conventional CMOS process node. This is because CIS node scaling tracks the pixel size scaling, which does not shrink aggressively due to the fundamental need of maintaining photon sensitivity. From Ma et al. (2023, figs. 1, 3).\n\n\n\nA key reason why we could integrate processing/computational capabilities into the CIS chip is because of the advancements in the CMOS technologies that, for instance, have significantly shrunk the feature size, which is the smallest physical dimension that can be reliably fabricated on a semiconductor chip and is proportional to the transistor size. At the same time, however, the PD size itself has not shrunk proportionally, meaning adding CMOS logic to the sensor increases the total chip area minimally in the grand scheme of things.\nThis is shown in Figure 16.12 (b), where triangle markers show the pixel sizes in CIS designs from all ISSCC papers appeared during Year 2000 and Year 2022, which include leading industry CIS designs at different times. We overlay a trend line regressed from these CIS designs to better illustrate the pixel size scaling trend. As a comparison, the blue line at the bottom represents the standard CMOS technology node scaling laid out by the International Roadmap for Devices and Systems (IRDS) (IRDS 2024). We can see that the gap between the pixel size and the standard CMOS feature size steadily increases. In fact, the pixel size scaling stagnates at around 5 \\(\\mu m\\), which has long been seen as the practical pixel size limit (Fossum 1997). As semiconductor manufacturers keep pulling rabbits out of a hat, the CMOS feature size is still, miraculously, shrinking (TSMC/Samsung are shipping products with a 2 nm process node in 2025), so the gap would still exist, at least for quite a while.\n\n\nComputational CIS Architectures\nThe computations inside a CIS could take place in both the analog and the digital domain. Figure 16.13 (b) illustrates one example where analog computing is integrated into a CIS chip before the ADC. Analog operations usually implement primitives for feature extraction (Bong, Choi, Kim, Kang, et al. 2017; Bong, Choi, Kim, Han, et al. 2017), object detection (Young et al. 2019), and DNN inference (Hsu et al. 2020; H. Xu et al. 2021). Figure 16.13 (c) illustrates another example that integrates digital processing, such as ISP (Murakami et al. 2022), image filtering (Kim et al. 2005), and DNN (Bong, Choi, Kim, Han, et al. 2017).\n\n\n\n\n\n\nFigure 16.13: (a) Traditional 2D imaging CIS with the PD array and the ADCs. (b) Computational CIS with analog processing capabilities (before the ADCs). (c) Computational CIS with digital processing. (d) Stacked computational CIS with digital processing in a separate layer. Adapted from Ma et al. (2023, fig. 2).\n\n\n\nAs the processing capabilities become more complex, CIS design has embraced 3D stacking technologies, as is evident by the increasing number of stacked CIS in Figure 16.12. Figure 16.13 (d) illustrates a typical stacked design, where the processing logic is separated from, and stacked with, the pixel array layer. The different layers communicate through the hybrid bond or the micro Through-Silicon Via (\\(\\mu\\)TSV) (Liu et al. 2022; Tsugawa et al. 2017). The processing layer typically integrates digital processors, such as ISP (Kwon et al. 2020), image processing (Hirata et al. 2021; O. Kumagai et al. 2018), and DNN accelerators (Eki et al. 2021; Liu et al. 2022).\nThree-layer stacked designs have also been proposed. Sony IMX 400 (Haruta et al. 2017) is a 3-layer design that integrates a pixel layer, a DRAM layer (1 Gbit), and a logic layer with an Image Signal Processor (ISP). The DRAM layer buffers high-rate frames before streaming them out to the host. This enables super slow motion (960 FPS); otherwise, the bandwidth of the MIPI CSI-2 interface limits the capturing rate of the sensor. Meta conceptualizes a three-layer design (Liu et al. 2022) with a pixel array layer, a per-pixel ADC layer, and a digital processing layer that integrates a DNN accelerator — using DPS. Stacking makes it easier to implement DPS: the main disadvantage of DPS is the complexity of the pixel design, but with stacking, the additional pixel processing circuitry (gain amplifier, ADC, etc.) can be “hidden” on a separate layer than the pixel array layer (Liu et al. 2022, 2020).\n\n\nChallenges of CIS\nMoving computation inside a CIS, however, is not without challenges. Most importantly, processing inside the sensor is far less efficient than that outside the sensor. This is because, while the CIS is implemented using the CMOS technologies, it uses significantly older process nodes than that of the conventional CMOS.\nThis is shown in Figure 16.12 (b), where the square markers show the process node used in each CIS paper surveyed. As a reference, the IRDS standard CMOS process node scaling line is also shown. At around the year 2000, the CIS process node started lagging behind that of the conventional CMOS node, and the gap is increasing. CIS designs today commonly use 65 nm and older process nodes. This gap is not an artifact of the CIS designs we pick; it is fundamental: there is simply no need to aggressively scale down the process node because the pixel size does not, and can not, shrink much. In fact, from Figure 16.12 (b) we can see that the slope of CIS process node scaling almost exactly follows that of the pixel size scaling. The reason that pixel size does not shrink much is to ensure light sensitivity: a small pixel reduces the number of photons it can collect, which directly reduces the dynamic range and the SNR6.\nInefficient in-sensor processing can be mitigated through 3D stacking technologies, which allow for heterogeneous integration: the pixel layer and the computing layer(s) can use their respective, optimal process node. Stacking, however, could increase power density, especially when future CIS integrates more processing capabilities. Therefore, harnessing the power of (stacked) computational CIS requires exploring a large design space and is still an active area of research (Ma 2024; Feng et al. 2024; Ma et al. 2023).",
    "crumbs": [
      "Imaging",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Image Sensor Architecture</span>"
    ]
  },
  {
    "objectID": "imaging-sensor.html#sec-chpt-imaging-sensor-optics",
    "href": "imaging-sensor.html#sec-chpt-imaging-sensor-optics",
    "title": "16  Image Sensor Architecture",
    "section": "16.4 In-Sensor Optics",
    "text": "16.4 In-Sensor Optics\nThe on-chip optics serve a few purposes: blocking lights in the IR/UV ranges, boosting photon collection efficiency, anti-aliasing, and filtering for color reproduction.\n\n16.4.1 IR/UV Cut-Off Filters\nMany cameras have cut-off filters for infrared (IR) and ultraviolet (UV) lights. Their goals are to remove/block IR or UV lights, as much as possible, from the incident light. These filters are transparent in that they predominantly absorb light while scattering very little light. So their optical behaviors can be adequately captured by their transmittance spectra. Figure 16.14 (left) shows the transmittance spectrum of the cut-off filter on the Nikon D200, where light below 400 nm and above 700 nm is essentially blocked from hitting the sensor.\n\n\n\n\n\n\nFigure 16.14: Left: transmittance spectrum of the on-chip cut-off optics on Nikon D200; from Kolarivision Melentijevic (2015). Right: IR thermal imaging uses light power in the IR range to estimate temperature; from Arno / Coen (2006).\n\n\n\nThe reason most photographic cameras want to remove IR and UV lights is because the human visual system is not sensitive to IR and UV lights (recall our earlier discussions about the spectra of the cone fundamentals, which drop to 0 beyond roughly the 380 \\(\\text{nm}\\) and 780 \\(\\text{nm}\\) range). So for a camera to accurately reproduce the color of an object as if the object is directly viewed by the human eyes, the sensor’s sensitivity ideally needs to mimic that of the human eyes. Cutting IR and UV lights, to which our photoreceptors are not sensitive, is just the first step. We will discuss in detail in Section 16.7 what other mechanisms are in place for accurate color reproduction in image sensors.\nInterestingly, thermographic cameras detect optical power in the IR range to estimate object temperature. Any object above absolute zero radiates, and this is call the blackbody radiation. Planck’s law governs the electromagnetic power emitted at a particular wavelength at a particular temperature. It turns out that at room temperature (about 300 K), most of the radiation power is in the IR range; very little radiation comes from the visible range. That is why thermal cameras use IR radiation for temperature estimation. Figure 16.14 (right) shows an example of an IR image visualized as a heatmap, a real heatmap. \n\n\n16.4.2 Microlenses\nAn important figure of merit of image sensors is the fill factor (FF), which is defined as the ratio of the photosensitive area of a pixel to the actual pixel area. Usually the photosensitive area is much smaller than the pixel area. This is because in addition to the actual photodiode, a pixel contains many other electrical components (capacitors, transistors, and other complex logic gates) that take up the area. This is illustrated in Figure 16.15 (a), where many incident lights will not reach the PD, leading to a low FF. Given a fixed pixel area, a low FF means the pixel collects fewer photons during exposure, which translates to a higher signal-to-noise ratio, so it is almost always desirable to have a higher FF.\n\n\n\n\n\n\nFigure 16.15: (a): without a microlens, the photosensitive area of a pixel is the PD area; many incident lights will not hit the PD, leading to a low fill factor. (b): microlenses increase the effective fill factor of an image sensor.\n\n\n\nOne common way to increase the FF that is prevalent in almost all image sensors is through microlenses. This is illustrated in Figure 16.15 (b). Every pixel has a convex lens, which we call a microlens, sitting on top of it. The job of the microlens is to, ideally, direct all the photons hitting the pixel to the photodiode, in which case the FF would effectively be 100%, which contemporary image sensors are very close to.\n\n\n16.4.3 Anti-Aliasing Filters\nMany image sensors also have anti-aliasing (AA) filters, especially photographic sensors. Recall that pixels perform spatial sampling of the optical image, which is continuous, thus introducing aliasing. The classic anti-aliasing method is to pre-filter the continuous signal using a low-pass filter, essentially blurring the signal and reducing its peak frequency. Pharr, Jakob, and Humphreys (2023, chap. 8) and Glassner (1995, Unit II) provide great technical discussions of signal sampling and reconstruction, which we will omit here.\nIn some sense, the photodiodes themselves and the microlenses act as pre-filters already: they inherently perform spatial 2D box convolutions over the continuous signal impinging upon them. Take the photodiode as an example: each photodiode integrates all the incident photons, as we have seen in Section 16.2, and integration is equivalent to convolving/filtering the signal with a 2D box filter.\nHowever, the support of the filter carried by the microlens and the photodiode is small: the microlens filter has a size of the pixel area, and the photodiode filter support is even more compact. To more aggressively pre-filter the signal, we need a filter with a wide support. To that end, AA filters use birefringent material, as shown in Figure 16.16 (a), which essentially splits a ray into two rays, each with a different polarization and, thus, takes a slightly different path (recall that the refractive index depends on the polarization of light). If we cascade two such materials, a ray gets split into four rays; this is called a 4-dot beam splitting. This is done by, e.g., the Nikon D800e, as shown in Figure 16.16 (b).\n\n\n\n\n\n\nFigure 16.16: (a): a birefringent material that, through double refraction, splits a ray into two; adapted from APN MJM (2011). (b): many anti-aliasing filters are made by cascading two birefringent materials that, collectively, split a ray into four; they are called 4-dot AA filters. (c): MTF of a 4-dot AA filter.\n\n\n\nThe birefringent material acts as a low-pass filter. The intuition is that if an incident ray is spread over, say, 4 sensor-plane points, then each sensor-plane point, equivalently, integrates information from 4 incident rays, each coming from a distinct scene point (assuming a pinhole aperture). We know integration is essentially low-pass filtering.\nThe way to understand the effect of the AA filter is to analyze its Point Spread Function (PSF) and Modulation Transfer Function (MTF), which we have seen in Section 15.5.3. Assuming a pinhole aperture, a 4-dot beam-splitting AA filter essentially imposes a PSF where a scene point is spread over 4 sensor-plane points. The PSF is the sum of 4 Dirac Delta functions placed on a regular grid with an offset \\(d\\) between adjacent grid points (which depends on the difference in refractive indices and the relative positions between the two splitting planes):\n\\[\n    f(x, y) = \\frac{1}{4}[\\delta(x, y) + \\delta(x-d, y) + \\delta(x, y-d) + \\delta(x-d, y-d)].\n\\]\nWith a little math, which we omit here, we can show that the MTF of this PSF is:\n\\[\n    MTF(f_x, f_y) = |\\cos(\\pi d f_x)||\\cos(\\pi d f_y)|.\n\\]\nAn example of this MTF is shown in Figure 16.16 (c), where the \\(x\\)-axis and \\(y\\)-axis are the two spatial frequencies \\(f_x\\) and \\(f_y\\), and the \\(a\\)-axis is the MTF. We can see that this particular MTF passes low frequencies and cuts off at a frequency of, in the case where \\(d=1\\), 0.5. Interestingly, the MTF also passes high frequencies, which is generally not a huge concern because power at high frequencies is usually already attenuated by the PSFs of other optical elements (e.g., the main imaging lens). Of course, in reality the aperture is not a pinhole, so the PSF is not simply a sum of four Delta functions but can nevertheless still be similarly analyzed.\n\n\n\n\n\n\nFigure 16.17: (a): a birefringent material that, through double refraction, splits a ray into two; adapted from APN MJM (2011). (b): many anti-aliasing filters are made by cascading two birefringent materials that, collectively, split a ray into four; they are called 4-dot AA filters. (c): MTF of a 4-dot AA filter.\n\n\n\nFigure 16.17 (a) and Figure 16.17 (b) compare the images taken of the same scene by Nikon D800e, which lacks an AA filter, and Nikon D800, which has a 4-dot AA filter. Look at the AC’s condenser coil; the AA image is more blurred but has much less objectionable aliasing effect.",
    "crumbs": [
      "Imaging",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Image Sensor Architecture</span>"
    ]
  },
  {
    "objectID": "imaging-sensor.html#sec-chpt-imaging-sensor-optics-monomodel",
    "href": "imaging-sensor.html#sec-chpt-imaging-sensor-optics-monomodel",
    "title": "16  Image Sensor Architecture",
    "section": "16.5 Monochromatic, Noise-Free Sensor Model",
    "text": "16.5 Monochromatic, Noise-Free Sensor Model\nEach in-sensor optical element adds its own spectral transmittance, so the overall transmittance of the in-sensor optics is the product of them. We will simply use \\(T(\\lambda)\\) to represent the overall transmittance. Given what we have discussed so far, we can build an analytical model for a monochromatic, noise-free image sensor. The raw pixel value, also known as the Digital Number, \\(n\\) of a pixel \\(p\\) of size \\(A_p\\) and is exposed for a duration of \\(t_{exp}\\) is given by:\n\\[\n\\begin{aligned}\n    N &= \\int_{\\lambda} \\int^{t_{exp}} \\int^{A_p} Y(p, \\lambda, t) T(\\lambda) QE(\\lambda) \\text{d}p \\text{d}t \\text{d}\\lambda, \\\\\n    \\Delta V &= \\frac{Nq}{C_{FD}} \\times g, \\\\\n    n &= \\lfloor \\frac{\\Delta V}{V_{max}} (2^{L} - 1) \\rfloor,\n\\end{aligned}\n\\tag{16.7}\\]\nwhere \\(Y(p, \\lambda, t)\\) is the number of photons incident on position \\(p\\) at a particular wavelength \\(\\lambda\\) at a particular time \\(t\\), so it is a quantal counterpart of the spectral irradiance; \\(T(\\lambda)\\) is the overall spectral transmittance of the in-sensor optics, \\(QE(\\lambda)\\) is the quantum efficiency, and \\(q\\) is the elementary charge.\nThe first equation in Equation 16.7 models \\(N\\), the total amount of charges collected at the particular pixel, where we integrate spatially, temporally, and spectrally. The second equation in Equation 16.7 is essentially Equation 16.5, and models the voltage difference sensed before and after the exposure. The last equation in Equation 16.7 is a crude ADC model, assuming that the voltage range \\([0, v_{max}]\\) is quantized into \\(L\\) bits, and the output of the ADC model is the digital number, a.k.a., the raw pixel value.\nHow do we express \\(Y(p, \\lambda, t)\\), the quantal counterpart of irradiance? The spectral irradiance at position \\(p\\) and time \\(t\\) is:\n\\[\n    E(p, \\lambda, t) = \\int^{\\Omega(p, V)} L(p, \\omega, \\lambda, t) \\cos\\theta~\\text{d}\\omega,\n\\tag{16.8}\\]\nwhere \\(\\Omega(p, V)\\) is the solid angle subtended by \\(p\\) and the aperture \\(V\\); \\(L(p, \\omega, \\lambda, t)\\) is the radiance with a wavelength \\(\\lambda\\) incident on \\(p\\) from the direction \\(\\omega\\) at time \\(t\\), and \\(\\theta\\) is the polar angle subtended by \\(\\omega\\) and the pixel normal vector.\nGiven Planck’s equation (Equation 16.1), we can turn irradiance \\(E\\) (energy per unit area per unit time) into the quantity \\(Y\\) (photon quantity per unit area per unit time):\n\\[\n    Y(p, \\lambda, t) = \\frac{E(p, \\lambda, t) \\lambda}{hc}.\n\\tag{16.9}\\]\nPlugging Equation 16.8 and Equation 16.9 into the \\(N\\) expression in Equation 16.7, we have:\n\\[\n    N = \\int_{\\lambda} \\int^{t_{exp}} \\int^{A_p} \\int^{\\Omega(p, V)} \\frac{L(p, \\omega, \\lambda, t) \\cos\\theta \\text{d}\\omega T(\\lambda) QE(\\lambda) \\lambda}{hc} \\text{d}p \\text{d}t \\text{d}\\lambda.\n\\tag{16.10}\\]\nRearranging the terms a bit we get:\n\\[\n    N = \\int_{\\lambda} \\Big( \\int^{t_{exp}} \\int^{A_p} \\int^{\\Omega(p, V)} L(p, \\omega, \\lambda, t) \\cos\\theta \\text{d}\\omega \\text{d}p \\text{d}t \\Big) T(\\lambda) QE(\\lambda) \\frac{\\lambda}{hc} \\text{d}\\lambda.\n\\tag{16.11}\\]\nRecall from Section 9.1, the inner four integrals in Equation 16.11 collectively form the so-called camera measurement equation, which calculates \\(Q(\\lambda)\\), the energy at wavelength \\(\\lambda\\) collected by the pixel during the exposure7. Therefore, we get:\n\\[\n    N = \\int_{\\lambda} Q(\\lambda) T(\\lambda) QE(\\lambda) \\frac{\\lambda}{hc} \\text{d}\\lambda.\n\\tag{16.12}\\]\nWe have implicitly assumed here that the effects of the in-sensor optics can simply be modeled by the spectral transmittance \\(T(\\lambda)\\). This is largely reasonable because 1) in-sensor optics are mostly transparent and 2) they are very close to the pixels, so we can ignore rays that are incident on the edge of the optics and, after refractions, miss the pixels.\n\n16.5.1 Spectral Sensitivity Function\nWe can make a few assumptions to simplify our discussion. First, we assume the ADC quantization error is negligible. Second, we assume that the irradiance within a pixel is spatially and temporally uniform during a short exposure time. The raw pixel value \\(n\\) in Equation 16.7 is then simplified to:\n\\[\n    n \\approx k \\int_{\\lambda} Y(p, \\lambda, t) T(\\lambda) QE(\\lambda) \\text{d}\\lambda,\n\\tag{16.13}\\]\nwhere \\(Y(p, \\lambda, t)\\) is the (average) number of incident photons at wavelength \\(\\lambda\\) hitting position \\(p\\) at time \\(t\\); \\(k = uvt_{exp}\\frac{qg}{C_{FD}}\\frac{2^N-1}{V_{max}}\\) is a constant.\nLet’s define a convenient term: Spectral Sensitivity Function (SSF), which is the product of \\(T(\\lambda)\\) and \\(QE(\\lambda)\\). Therefore, we can rewrite \\(n\\) as:\n\\[\n    n \\approx k \\int_{\\lambda} Y(p, \\lambda, t) SSF_{quantal}(\\lambda) \\text{d}\\lambda.\n\\tag{16.14}\\]\nSSF is the only spectral (wavelength-dependent) term in Equation 16.14 other than the incident light itself; it represents the phenomenological light sensitivity of the sensor over wavelength. SSF is sometimes also called the camera response function.\nThe SSF defined in Equation 16.14 is an “equal-quantal” function because it tells us the relative responses between different wavelengths under the same amount of incident photons. We can turn it into an “equal-energy” or “equal-power” function that operates on energy or power. We first express the raw pixel value \\(n\\) in terms of the spectral power distribution \\(\\Phi(\\lambda)\\) rather than the spectral quantity distrubition \\(Y(\\lambda)\\) and rewrite Equation 16.14 as:\n\\[\n    n \\approx k \\int_{\\lambda} \\frac{\\Phi(p, \\lambda, t)}{t_{exp}\\frac{hc}{\\lambda}} SSF_{quantal}(\\lambda) \\text{d}\\lambda,\n\\tag{16.15}\\]\nwhere \\(\\Phi(p, \\lambda, t)\\) denotes the spectral power distribution of the light hitting position \\(p\\) at time \\(t\\). Now let’s absorb \\(t_{exp}hc\\) into \\(k\\) and define \\(k' = uv\\frac{qg}{C_{FD}}\\frac{2^N-1}{V_{max}}\\frac{1}{hc}\\) and \\(SSF_{power}(\\lambda) = SSF_{quantal}(\\lambda)\\lambda\\), we get:\n\\[\n    n \\approx k' \\int_{\\lambda} \\Phi(p, \\lambda, t)SSF_{power}(\\lambda) \\text{d}\\lambda.\n\\tag{16.16}\\]\n\\(SSF_{power}(\\lambda)\\) is the equal-power SSF. The subscript is usually omitted in the literature because it is usually clear what SSF is being used (e.g., from the quantity that is being multiplied with the SSF). Also note that in some literature, the SSF is used interchangeably with QE, so be very careful.",
    "crumbs": [
      "Imaging",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Image Sensor Architecture</span>"
    ]
  },
  {
    "objectID": "imaging-sensor.html#sec-chpt-imaging-sensor-optics-pixel",
    "href": "imaging-sensor.html#sec-chpt-imaging-sensor-optics-pixel",
    "title": "16  Image Sensor Architecture",
    "section": "16.6 What is a Pixel?",
    "text": "16.6 What is a Pixel?\nGiven the model of how a pixel value is generated, we can develop a more fundamental understanding of pixels. Perhaps the first question to ask is: what is a pixel? There are at least three forms of pixel that are relevant to us: a pixel on an image sensor, a pixel in a digital image, and a pixel on a display. They participate in the processes of filtering, sampling, and reconstructing the underlying light signal.\n\n\n\n\n\n\nFigure 16.18: (a): the continuous optical image impinging on the sensor plane; (b): each image sensor pixel integrates the photon energy incident on the pixel surface; (c): this is equivalent to filtering the optical image with a Box (average) filter and then sampling the filtered image only at the pixel centers; (d): the resulting array of samples is essentially a digital image (after some in-sensor processing); (e): a display takes that array of samples and reconstructs a continuous signal using a Box filter, equivalent to a nearest neighbor interpolation; (f): the chain of signal processing summarized.\n\n\n\nDuring an exposure period, the photons in the scene, after going through the various optics, rain down on the sensor plane. This is illustrated in Figure 16.18 (a), where for illustration purpose shows arbitrarily only a handful of photons but in reality every point on the sensor receives photons from all directions. The spatial energy distribution on the sensor plane is what we call an optical image \\(OI(p)\\), a 2D continuous signal that tells us the energy at any point \\(p\\) on the sensor plane. In radiometry, the energy of an infinitesimal point is called radiant exposure (whose unit is \\(\\text{J}/\\text{m}^\\text{2}\\)), equivalent to the irradiance of the point integrated over the exposure time.\nAs we have discussed above and seen in the measurement equation (Equation 9.2), each sensor pixel spatially integrates the energy across its surface area, shown in Figure 16.18 (b). This integration is equivalent to a cascade of two operations:\n\nfiltering the optical image using a 2D Box (average) filter \\(B_i\\) with a support equivalent to the pixel size: \\(FOI(p) = (OI \\star B_i)(p)\\), and\nsampling the filtered signal at the center of each pixel: \\(I(p) = FOI(p) \\mathop{\\mathrm{III}}(p)\\), where \\(\\mathop{\\mathrm{III}}(p)\\) is the 2D Dirac Comb function that is only non-zero at the pixel centers.\n\nFiltering with a Box filter is essentially integration, and filtering/convolution followed by sampling is equivalent to computing the convolution only at the sampled positions. The result is an array of samples, shown in Figure 16.18 (c). Each sample then is processed inside the sensor (e.g., turned into charges by Equation 16.12) and eventually read out as a digital number (\\(n\\) in Equation 16.7), i.e., a pixel value in the final digital image, shown in Figure 16.18 (d).\nThat is, the digital image we get is nothing more than a (processed) array of samples of the filtered optical image \\(FOI\\). Ignoring the ADC quantization error and noise, the value of an image pixel is proportional to the corresponding sample in \\(FOI\\).\nNow to display the image, we send that array to the display. For simplicity, let’s assume that we are dealing with monochromatic displays, in which case each image pixel drives a single display pixel. A display pixel, like a sensor pixel, is small but has a non-zero area. Each point on the display pixel also has a radiant exposure, which ideally is proportional to the image pixel value and is uniform across the entire pixel area.\nTherefore, ignoring gaps between display pixels, ultimately what we get from the display is another 2D, continuous signal \\(DOI\\), shown in Figure 16.18 (e). This is essentially reconstructing a continuous signal \\(DOI\\) from the digital image (an array of samples) by applying, again, a Box filter \\(B_d\\): \\(DOI(p) = (FOI \\star B_d)(p)\\). This filtering is equivalent to a nearest neighbor interpolation. The entire chain of signal processing from sensor pixels to digital image pixels and display pixels is summarized in Figure 16.18 (f).\nIt is worth noting that our discussion above greatly simplifies what the display pixels actually do. Most importantly, the signal ultimately coming out of the display is not 2D but actually a light field: every point on the display emits lights across a range of directions, each of which has a spectral power distribution (that gives rise to color) that might change over time. How a display pixel turns a single image pixel value into a light field very much depends on the actual display design, which we will discuss in more detail in Chapter 19.",
    "crumbs": [
      "Imaging",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Image Sensor Architecture</span>"
    ]
  },
  {
    "objectID": "imaging-sensor.html#sec-chpt-imaging-sensor-color",
    "href": "imaging-sensor.html#sec-chpt-imaging-sensor-color",
    "title": "16  Image Sensor Architecture",
    "section": "16.7 Color Sensing",
    "text": "16.7 Color Sensing\nThere is one main piece of the on-chip optics we have not discussed: the color filters, which are critical for color sensing and deserve their own section.\n\n16.7.1 Goal of Color Sensing\nWhat does it mean for an image sensor to capture color? We know that colors are subjective sensations caused by cone photoreceptor responses to light; a color can be expressed as a point in a 3D space formed by the L, M, and S cone responses, i.e., the LMS cone space. Ideally, if we can build an image sensor in such a way that it also possesses three kinds of pixels, each of which has a spectral sensitivity matching exactly that of a cone class (i.e., cone fundamental), the sensor would be able to accurately capture and reproduce the color information.\nIn fact, it is even sufficient for the sensor responses to be just a (linear) transformation away from the cone responses, as long as we can pre-calibrate the transformation matrix offline. This idea is illustrated in Figure 16.19. We emphasize linear transformation here simply because it is computationally cheaper; nothing prevents you from designing a sensor sensitivity profile that requires a sophisticated transformation from the cone space.\n\n\n\n\n\n\nFigure 16.19: The goal of color sensing is to form a color space from the raw pixel values and for there to exist a (preferably linear) transformation between the sensor color space and a standard color space, typically the CIE XYZ space. Adapted from Blume and Garbazza and Spitschan (2019), Thorseth (2015), and ajay_suresh (2021)}.\n\n\n\nWhere do the three classes of spectral sensitivities come? Examine our monochromatic sensing model in Equation 16.14; it appears that all the pixels share the same response function and, thus, have the same spectral sensitivity: every pixel has the same quantum efficiency and the same optical elements sitting above them (so the same spectral transmittance of the optics).\nThere are a variety of ways to introduce sensitivity differences across pixels, which we will discuss shortly in Section 16.7.2. Assuming, for now, that we have somehow introduced the three classes of SSFs, denoted \\(SSF_R(\\lambda)\\), \\(SSF_G(\\lambda)\\), and \\(SSF_B(\\lambda)\\). Given an incident light with an SPD \\(\\Phi(\\lambda)\\), the camera responses are:\n\\[\n    [\\int_{\\lambda} \\Phi(\\lambda)SSF_{R}(\\lambda) \\text{d}\\lambda, \\int_{\\lambda} \\Phi(\\lambda)SSF_{G}(\\lambda) \\text{d}\\lambda, \\int_{\\lambda} \\Phi(\\lambda)SSF_{B}(\\lambda) \\text{d}\\lambda].\n\\]\nThis is a direct invocation of Equation 16.16 with the constant omitted. The color of the light expressed in the LMS cone space is:\n\\[\n    [\\int_{\\lambda} \\Phi(\\lambda)L(\\lambda) \\text{d}\\lambda, \\int_{\\lambda} \\Phi(\\lambda)M(\\lambda) \\text{d}\\lambda, \\int_{\\lambda} \\Phi(\\lambda)S(\\lambda) \\text{d}\\lambda].\n\\]\nIf the cone responses form a 3D cone space, the camera raw responses also form a color space, which is sometimes called the camera’s native color space. We provide an interactive tutorial that allows you to interactively explore and compare the native color spaces of various cameras and the LMS cone space. Figure 16.20 (left) shows the SSFs of iPhone 11 (solid lines) and the cone fundamentals. The SSFs are normalized so that \\(SSF_G\\) is peaked at unity, and the cone fundamentals are each normalized to peak at unity, so you could compare the relative sensitivity between the three SSFs in iPhone 11 but could not between the cone classes. Usually the SSF of a camera depends on a variety of factors such as the materials of the optical elements and the photodiodes as well as the pixel design, so it is almost impossible for the three SSFs to match exactly the cone fundamentals. Figure 16.20 (right) shows the spectral locus in iPhone 11’s native color space and in the cone space; they evidently do not overlap.\n\n\n\n\n\n\nFigure 16.20: Left: Spectral sensitivity functions of iPhone 11 (the RGB filters; solid lines) in comparison with the LMS cone fundamentals (dashed lines). Right: the spectral locus in the LMS space and in the camera’s native color space. Adapted from Zhu (2022).\n\n\n\nA major task in sensor calibration is to identify a transformation matrix \\(M\\) such that the following (approximately) holds:\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n    \\int_{\\lambda} \\Phi(\\lambda)SSF_{R}(\\lambda) \\text{d}\\lambda\\\\\n    \\int_{\\lambda} \\Phi(\\lambda)SSF_{G}(\\lambda) \\text{d}\\lambda\\\\\n    \\int_{\\lambda} \\Phi(\\lambda)SSF_{B}(\\lambda) \\text{d}\\lambda\n\\end{bmatrix}\n\\times M =\n\\begin{bmatrix}\n    \\int_{\\lambda} \\Phi(\\lambda)L(\\lambda) \\text{d}\\lambda\\\\\n    \\int_{\\lambda} \\Phi(\\lambda)M(\\lambda) \\text{d}\\lambda\\\\\n    \\int_{\\lambda} \\Phi(\\lambda)S(\\lambda) \\text{d}\\lambda\n\\end{bmatrix}\n\\end{aligned}\n\\]\nThe transformation matrix is then applied in the post-processing pipeline of the raw pixels to turn raw pixel responses into a color value. We will discuss the calibration and the post-processing pipeline in greater details in Chapter 18.\n\n\n16.7.2 Implementing Three “Classes of Pixels”\nPerhaps the most straightforward method to introduce varying SSF is to apply a spectral filter to different pixels. A spectral filter is just a transparent optical element with a wavelength-selective transmittance. We need only three filters to emulate the three cone classes, but ideally each pixel should get all three simultaneously, which is difficult if you think about it, since at any given time you can physically have only one filter sitting on a pixel.\n\nThree-Shot and Three-Chip Methods\nThere are two ways to go about addressing this issue. We can take three images of the same scene, each with a different filter, and then combine the together. This approach is believed to be pioneered by Sergey Prokudin-Gorsky, who conducted a breathtaking “photographic survey” of the early 20th-century Russia using this method (Prokudin-Gorsky 1948). This is called the “three-shot” approach. Alternatively, one could split the incident lights and send each of them to a different sensor, each with a different filter. This approach would obviously increase the form factor of the camera but avoids having to register and align the three separate shots, which is subjective to object motion. These camera are called “three-chip” or “three-CCD/COMS” cameras, which are still very widely used today in broadcasting, film studios, etc.\n\n\nColor Filter Array (CFA)\nBoth the three-shot and the three-chip approach allow each incident light to be transformed to three responses needed for color reproduction — at the cost of capturing overhead or bulky system design. A much simpler approach, and the most commonly used approach today, is called Color Filter Array (CFA), which assigns each pixel only one filter.\n\n\n\n\n\n\nFigure 16.21: Left: the Bayer color filter array; from Cburnett (2006). Middle and Right: a Bayer-domain image where each pixel generates only one response and a full-color image assuming each pixel generates three responses; adapted from Cmglee (2018).\n\n\n\nFigure 16.21 shows the most commonly used CFA, where the three classes of filters are tiled in what is called the Bayer filter mosaic, named after Bryce Bayer, who invented this pattern while working for Eastman Kodak in Rochester, NY (Bayer 1976). Each of the three filters has a transmittance spectrum that peaks at, roughly, red-ish, green-ish, and blue-ish wavelengths, similar to the spectra shown in Figure 16.20 (left).\nThe three filter classes are organized in \\(2\\times 2\\) tiles, where each tile has two green filters. Bayer did so because he wanted to mimic human vision, where the photopic Luminance Efficiency Function (LEF) is most sensitive to green-ish lights (Sharpe et al. 2005, 2011) (see Figure 4.9). We can see that the CFA approach is actually more similar to human color vision than the three-shot or three-chip approach. In human vision, each cone photoreceptor has a particular sensitivity spectrum, and generates one of the three responses needed to form color vision.\nA necessary consequence of using the CFA is that each pixel gets only one color channel information. Figure 16.21 (middle) shows a raw image captured using a CFA, where each pixel evidently has only one color channel. The overall image looks overwhelmingly green because of the sheer amount of green filters. An important step in the post-processing pipeline is to reconstruct the two other missing channels, a process called demosaicing, i.e., removing the Bayer mosaic artifacts. An example of the reconstructed image is shown in Figure 16.21 (right).\nWe will have more to say about the demosaicing process when we get to Chapter 18, but for now, let’s just observe that demosaicing is nothing more than a signal sampling and reconstruction problem. The CFA allows each pixel to sample only one channel of the three channels of response. So the green-filter response, for instance, is sampled by half of the pixels8, and the other two responses are sampled by one quarter of the pixels each. The job of demosaicing is then to reconstruct the full signal responses from the samples — a well-established problem in signal processing.\n\n\nFoveon Approach\nThe final approach does away with optical color filters altogether. Instead, we will use three photodiodes vertically stacked for each pixel. Figure 16.22 illustrates a pixel in the Foveon X3 sensor, which is perhaps the most famous sensor that uses this architecture.\n\n\n\n\n\n\nFigure 16.22: Illustration of the Foveon X3 pixel, which has three PDs made of the same material (silicon) vertically stacked; adapted from Anoneditor (2007). Each PD receives a different light spectrum (due to the depth-varying absorption), effectively creating three different responses of the same light incident on the pixel surface.\n\n\n\nThe idea is that the silicon absorption spectrum is wavelength sensitive, as shown in the right panel of Figure 16.3. Blue-ish lights have a much shorter mean free length than do green-ish lights, which have a shorter mean free length than do red-ish lights. This means most short-wavelength lights will be absorbed after the first photodiode, leaving mostly medium- to long-wavelength lights. Those lights will go through the second photodiode, which absorbs mostly the medium-wavelength lights, leaving mostly long-wavelength lights to the third photodiode. As a result, each PD actually receives a different light spectrum, effectively creating three different responses for the same light incident on the pixel.\nLet’s assume that the three PDs have a depth of \\(d_B\\), \\(d_G\\), and \\(d_R\\), respectively. The incident light impinging on the pixel (i.e., the first PD surface) has a SPD \\(\\Phi(\\lambda)\\). The light impinging on the second PD then has a spectrum \\(\\Phi(\\lambda)e^{-\\sigma(\\lambda)d_B}\\), where \\(\\sigma(\\lambda)\\) is the silicon’s absorption coefficient spectrum. This is easily derived from the fact that pure absorption (no scattering and emission) leads to an exponential decay of the input signal (Equation 12.4). Similarly, the light impinging on the third PD then has a spectrum \\(\\Phi(\\lambda)e^{-\\sigma(\\lambda)(d_B+d_G)}\\). The responses produced by the three PDs are thus (in the order of R, G, and G):\n\\[\n    [\\int_{\\lambda}\\Phi(\\lambda)\\eta_R(\\lambda)e^{-\\sigma(\\lambda)(d_B+d_G)}, \\int_{\\lambda}\\Phi(\\lambda)\\eta_G(\\lambda)e^{-\\sigma(\\lambda)(d_B)}, \\int_{\\lambda}\\Phi(\\lambda)\\eta_B(\\lambda)],\n\\]\nwhere \\(\\eta_R(\\lambda)\\), \\(\\eta_G(\\lambda)\\), and \\(\\eta_B(\\lambda)\\) are QE spectra of the three PDs (where we consider only photons that reach a PD as the denominator in Equation 16.2 while ignoring photons that are reflected/absorbed before the photons hit the PD), respectively, and \\(\\Phi(\\lambda)\\) is the SPD of the light incident on the pixel surface. The three PDs use identical material (so they share the same silicon absorption spectrum) but can still have different \\(\\eta(\\lambda)\\)s because of the thickness differences — due to the differences in the lengths of the depletion and neutral regions in the PD p-n junctions. Can you guess why the thickness tends to increase for deeper PDs in Figure 16.22 (right)?\nCompared to using the CFA, the vertical PD stacking approach is much more complicated to fabricate and more costly, so it is much less commonly used. It avoids color sampling (and the resulting aliasing) and the need for demosaicing, and in theory could also have a higher overall quantum efficiency (and signal-to-noise ratio) since there are no color filters, so it might find uses in scientific imaging (Chen et al. 2023).\n\n\n\n\najay_suresh. 2021. “iPhone 12 cameras; CC BY-SA 2.0 license.” https://commons.wikimedia.org/wiki/File:Apple_iPhone_12_Pro_-_Cameras_(50535314721).jpg.\n\n\nAkahane, Nana, Shigetoshi Sugawa, Satoru Adachi, Kazuya Mori, Toshiyuki Ishiuchi, and Koichi Mizobuchi. 2006. “A Sensitivity and Linearity Improvement of a 100-dB Dynamic Range CMOS Image Sensor Using a Lateral Overflow Integration Capacitor.” IEEE Journal of Solid-State Circuits 41 (4): 851–58.\n\n\nAnoneditor. 2007. “Illustration of the Foveon X3 sensor; CC BY-SA 3.0.” https://commons.wikimedia.org/wiki/File:Absorption-X3.png.\n\n\nAoki, Masakazu, Haruhisa Ando, Shinya Ohba, Iwao Takemoto, Shusaku Nagahara, Toshio Nakano, Masaharu Kubo, and Tsutomu Fujita. 1982. “2/3-Inch Format MOS Single-Chip Color Imager.” IEEE Transactions on Electron Devices 29 (4): 745–50.\n\n\nAPN MJM. 2011. “A calcite crystal displays the double refractive properties while sitting on a sheet of graph paper; CC BY-SA 3.0 license.” https://commons.wikimedia.org/wiki/File:Crystal_on_graph_paper.jpg.\n\n\nArno / Coen. 2006. “Thermogram of a snake wrapped around a human arm; CC BY-SA 3.0 license.” https://commons.wikimedia.org/wiki/File:Wiki_stranglesnake.jpg.\n\n\nAxel Jacobs. 2006. “Open window with armchair and manequin. Sample scene for HDRI (standard LDR, single image from a set of bracketed exposures); CC BY-SA 2.0 license.” https://commons.wikimedia.org/wiki/File:HDRI_Sample_Scene_Window_-_08.jpg.\n\n\nBayer, Bryce E. 1976. “Color Imaging Array.”\n\n\nBiretta, John A, and Matt McMaster. 2008. Wide Field and Planetary Camera 2 Instrument Handbook v. 10.0. Space Telescope Science Institute.\n\n\nBlume and Garbazza and Spitschan. 2019. “Schematic overview of photorecetors; CC BY-SA 4.0 license.” https://commons.wikimedia.org/wiki/File:Overview_of_the_retina_photoreceptors_(a).png.\n\n\nBong, Kyeongryeol, Sungpill Choi, Changhyeon Kim, Donghyeon Han, and Hoi-Jun Yoo. 2017. “A Low-Power Convolutional Neural Network Face Recognition Processor and a CIS Integrated with Always-on Face Detector.” IEEE Journal of Solid-State Circuits 53 (1): 115–23.\n\n\nBong, Kyeongryeol, Sungpill Choi, Changhyeon Kim, Sanghoon Kang, Youchang Kim, and Hoi-Jun Yoo. 2017. “14.6 a 0.62 mW Ultra-Low-Power Convolutional-Neural-Network Face-Recognition Processor and a CIS Integrated with Always-on Haar-Like Face Detector.” In 2017 IEEE International Solid-State Circuits Conference (ISSCC), 248–49. IEEE.\n\n\nBoyle, Willard S, and George E Smith. 1970. “Charge Coupled Semiconductor Devices.” Bell System Technical Journal 49 (4): 587–93.\n\n\nBrayLockBoy. 2018. “An example of the Rolling shutter effect in action at Afton Down, Isle of Wight, taken by a camera on a car travelling at approximately 50 miles per hour. CC BY-SA 4.0 license.” https://commons.wikimedia.org/wiki/File:Rolling_Shutter_Effect_at_Afton_Down,_21_August_2018.jpg.\n\n\nCburnett. 2006. “A Bayer pattern on a sensor; CC BY-SA 3.0.” https://commons.wikimedia.org/wiki/File:Bayer_pattern_on_sensor.svg.\n\n\nChen, Cheng, Ziwen Wang, Jiajing Wu, Zhengtao Deng, Tao Zhang, Zhongmin Zhu, Yifei Jin, et al. 2023. “Bioinspired, Vertically Stacked, and Perovskite Nanocrystal–Enhanced CMOS Imaging Sensors for Resolving UV Spectral Signatures.” Science Advances 9 (44): eadk3860.\n\n\nCmglee. 2018. “Images of a garden with some tulips and narcissus; CC BY-SA 3.0.” https://commons.wikimedia.org/wiki/File:Colorful_spring_garden_Bayer_%2B_RGB.png.\n\n\n———. 2019. “Comparison of front- vs. back-illuminated sensors; CC BY-SA 4.0 license.” https://commons.wikimedia.org/wiki/File:Comparison_backside_illumination.svg.\n\n\nDyck, Rudolph H, and Gene P Weckler. 1968. “Integrated Arrays of Silicon Photodetectors for Image Sensing.” IEEE Transactions on Electron Devices 15 (4): 196–201.\n\n\nEinstein, Albert. 1905a. “On a Heuristic Point of View about the Creation and Conversion of Light.” Annalen Der Physik 17 (6): 132–48.\n\n\n———. 1905b. “Über Einen Die Erzeugung Und Verwandlung Des Lichtes Betreffenden Heuristischen Gesichtspunkt.” Albert Einstein-Gesellschaft.\n\n\nEki, Ryoji, Satoshi Yamada, Hiroyuki Ozawa, Hitoshi Kai, Kazuyuki Okuike, Hareesh Gowtham, Hidetomo Nakanishi, et al. 2021. “9.6 a 1/2.3 Inch 12.3 Mpixel with on-Chip 4.97 TOPS/w CNN Processor Back-Illuminated Stacked CMOS Image Sensor.” In 2021 IEEE International Solid-State Circuits Conference (ISSCC), 64:154–56. IEEE.\n\n\nEl Gamal, Abbas, and Helmy Eltoukhy. 2005. “CMOS Image Sensors.” IEEE Circuits and Devices Magazine 21 (3): 6–20.\n\n\nEMVA. 2021. “EMVA Standard 1288 Standard for Characterization of Image Sensors and Cameras.” https://www.emva.org/wp-content/uploads/EMVA1288General_4.0Release.pdf.\n\n\nEric Bajart. 2010. “Quantum efficiency of the CCD sensor ‘PC1’ in the Hubble Space Telescope’s Wide Field and Planetary Camera WFPC2; CC BY-SA 3.0.” https://commons.wikimedia.org/wiki/File:Quantum_efficiency_graph_for_WFPC2-en.svg.\n\n\nFeng, Yu, Tianrui Ma, Yuhao Zhu, and Xuan Zhang. 2024. “Blisscam: Boosting Eye Tracking Efficiency with Learned in-Sensor Sparse Sampling.” In 2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA), 1262–77. IEEE.\n\n\nFossum, Eric R. 1993. “Active Pixel Sensors: Are CCDs Dinosaurs?” In Charge-Coupled Devices and Solid State Optical Sensors III, 1900:2–14. SPIE.\n\n\n———. 1997. “CMOS Image Sensors: Electronic Camera-on-a-Chip.” IEEE Transactions on Electron Devices 44 (10): 1689–98.\n\n\nFossum, Eric R, and Donald B Hondongwa. 2014. “A Review of the Pinned Photodiode for CCD and CMOS Image Sensors.” IEEE Journal of the Electron Devices Society.\n\n\nFossum, Eric R, Nobukazu Teranishi, and Albert JP Theuwissen. 2024. “Digital Image Sensor Evolution and New Frontiers.” Annual Review of Vision Science 10 (1): 171–98.\n\n\nFowler, Boyd, Abbas El Gamal, and David XD Yang. 1994. “A CMOS Area Image Sensor with Pixel-Level a/d Conversion.” In Proceedings of IEEE International Solid-State Circuits Conference-ISSCC’94, 226–27. IEEE.\n\n\nGlassner, Andrew S. 1995. Principles of Digital Image Synthesis. Elsevier.\n\n\nGreen, Martin A, and Mark J Keevers. 1995. “Optical Properties of Intrinsic Silicon at 300 k.” Progress in Photovoltaics: Research and Applications 3 (3): 189–92.\n\n\nHaruta, Tsutomu, Tsutomu Nakajima, Jun Hashizume, Taku Umebayashi, Hiroshi Takahashi, Kazuo Taniguchi, Masami Kuroda, et al. 2017. “4.6 a 1/2.3 Inch 20Mpixel 3-Layer Stacked CMOS Image Sensor with DRAM.” In 2017 IEEE International Solid-State Circuits Conference (ISSCC), 76–77. IEEE.\n\n\nHasinoff, Samuel W, Dillon Sharlet, Ryan Geiss, Andrew Adams, Jonathan T Barron, Florian Kainz, Jiawen Chen, and Marc Levoy. 2016. “Burst Photography for High Dynamic Range and Low-Light Imaging on Mobile Cameras.” ACM Transactions on Graphics (ToG) 35 (6): 1–12.\n\n\nHirata, Tomoki, Hironobu Murata, Hideaki Matsuda, Yojiro Tezuka, and Shiro Tsunai. 2021. “7.8 a 1-Inch 17Mpixel 1000fps Block-Controlled Coded-Exposure Back-Illuminated Stacked CMOS Image Sensor for Computational Imaging and Adaptive Dynamic Range Control.” In 2021 IEEE International Solid-State Circuits Conference (ISSCC), 64:120–22. IEEE.\n\n\nHsu, Tzu-Hsiang, Yi-Ren Chen, Ren-Shuo Liu, Chung-Chuan Lo, Kea-Tiong Tang, Meng-Fan Chang, and Chih-Cheng Hsieh. 2020. “A 0.5-v Real-Time Computational CMOS Image Sensor with Programmable Kernel for Feature Extraction.” IEEE Journal of Solid-State Circuits 56 (5): 1588–96.\n\n\nHu, Chenming. 2009. Modern Semiconductor Devices for Integrated Circuits. Prentice Hall.\n\n\nHuggett, Anthony, Chris Silsby, Sergi Cami, and Jeff Beck. 2009. “A Dual-Conversion-Gain Video Sensor with Dewarping and Overlay on a Single Chip.” In 2009 IEEE International Solid-State Circuits Conference-Digest of Technical Papers, 52–53. IEEE.\n\n\nIida, S, Y Sakano, T Asatsuma, M Takami, I Yoshiba, N Ohba, H Mizuno, et al. 2018. “A 0.68 e-Rms Random-Noise 121dB Dynamic-Range Sub-Pixel Architecture CMOS Image Sensor with LED Flicker Mitigation.” In 2018 IEEE International Electron Devices Meeting (IEDM), 10–12. IEEE.\n\n\nIkeno, Rimon, Kazuya Mori, Masayuki Uno, Ken Miyauchi, Toshiyuki Isozaki, Isao Takayanagi, Junichi Nakamura, et al. 2022. “A 4.6-\\(\\mu\\)m, 127-dB Dynamic Range, Ultra-Low Power Stacked Digital Pixel Sensor with Overlapped Triple Quantization.” IEEE Transactions on Electron Devices 69 (6): 2943–50.\n\n\nIRDS. 2024. “International Roadmap for Devices and Systems.” https://irds.ieee.org/.\n\n\nKim, Seong-Jin, Kwang-Hyun Lee, Sang-Wook Han, and Euisik Yoon. 2005. “A 200/Spl Times/160 Pixel CMOS Fingerprint Recognition SoC with Adaptable Column-Parallel Processors.” In ISSCC. 2005 IEEE International Digest of Technical Papers. Solid-State Circuits Conference, 2005., 250–596. IEEE.\n\n\nKobayashi, Masahiro, Yusuke Onuki, Kazunari Kawabata, Hiroshi Sekine, Toshiki Tsuboi, Takashi Muto, Takeshi Akiyama, et al. 2017. “4.5A 1.8e-Rms Temporal Noise over 110 dB Dynamic Range \\(3.4\\mu\\mathrm {m}\\) Pixel Pitch Global-Shutter CMOS Image Sensor with Dual-Gain Amplifiers SS-ADC, Light Guide Structure, and Multiple-Accumulation Shutter.” IEEE Journal of Solid-State Circuits 53 (1): 219–28.\n\n\nKondo, Toru, Yoshiaki Takemoto, Kenji Kobayashi, Mitsuhiro Tsukimura, Naohiro Takazawa, Hideki Kato, Shunsuke Suzuki, et al. 2015. “A 3D Stacked CMOS Image Sensor with 16Mpixel Global-Shutter Mode and 2Mpixel 10000fps Mode Using 4 Million Interconnections.” In 2015 Symposium on VLSI Circuits (VLSI Circuits), C90–91. IEEE.\n\n\nKozlowski, Lester J, J Luo, WE Kleinhans, and T Liu. 1998. “Comparison of Passive and Active Pixel Schemes for CMOS Visible Imagers.” In Infrared Readout Electronics IV, 3360:101–10. SPIE.\n\n\nKumagai, Oichi, Atsumi Niwa, Katsuhiko Hanzawa, Hidetaka Kato, Shinichiro Futami, Toshio Ohyama, Tsutomu Imoto, et al. 2018. “A 1/4-Inch 3.9 Mpixel Low-Power Event-Driven Back-Illuminated Stacked CMOS Image Sensor.” In 2018 IEEE International Solid-State Circuits Conference-(ISSCC), 86–88. IEEE.\n\n\nKumagai, Y, R Yoshita, N Osawa, H Ikeda, K Yamashita, T Abe, S Kudo, et al. 2018. “Back-Illuminated \\(2.74\\mu\\mathrm {m}\\)-Pixel-Pitch Global Shutter CMOS Image Sensor with Charge-Domain Memory Achieving 10k e-Saturation Signal.” In 2018 IEEE International Electron Devices Meeting (IEDM), 10–16. IEEE.\n\n\nKwon, Minho, Seunghyun Lim, Hyeokjong Lee, Il-Seon Ha, Moo-Young Kim, Il-Jin Seo, Suho Lee, et al. 2020. “A Low-Power 65/14nm Stacked CMOS Image Sensor.” In 2020 IEEE International Symposium on Circuits and Systems (ISCAS), 1–4. IEEE.\n\n\nLiu, Chiao, Lyle Bainbridge, Andrew Berkovich, Song Chen, Wei Gao, Tsung-Hsun Tsai, Kazuya Mori, et al. 2020. “A 4.6 \\(\\mu\\)m, 512\\(\\times\\) 512, Ultra-Low Power Stacked Digital Pixel Sensor with Triple Quantization and 127dB Dynamic Range.” In 2020 IEEE International Electron Devices Meeting (IEDM), 16–11. IEEE.\n\n\nLiu, Chiao, Andrew Berkovich, Song Chen, Hans Reyserhove, Syed Shakib Sarwar, and Tsung-Hsun Tsai. 2019. “Intelligent Vision Systems–Bringing Human-Machine Interface to AR/VR.” In 2019 IEEE International Electron Devices Meeting (IEDM), 10–15. IEEE.\n\n\nLiu, Chiao, Song Chen, Tsung-Hsun Tsai, Barbara De Salvo, and Jorge Gomez. 2022. “Augmented Reality-the Next Frontier of Image Sensors and Compute Systems.” In 2022 IEEE International Solid-State Circuits Conference (ISSCC), 65:426–28. IEEE.\n\n\nMa, Tianrui. 2024. “Efficient Data-Driven Machine Vision: A Co-Design of Circuit, Algorithm, and Architecture for Edge Vision Sensors.” PhD thesis, Washington University in St. Louis.\n\n\nMa, Tianrui, Yu Feng, Xuan Zhang, and Yuhao Zhu. 2023. “Camj: Enabling System-Level Energy Modeling and Architectural Exploration for in-Sensor Visual Computing.” In Proceedings of the 50th Annual International Symposium on Computer Architecture, 1–14.\n\n\nMelentijevic. 2015. “DSLR Internal Cut Filter / Lowpass Filter / Hot Mirror Transmission Curves.” https://kolarivision.com/articles/internal-cut-filter-transmission/.\n\n\nMiyauchi, Ken, Kazuya Mori, Toshinori Otaka, Toshiyuki Isozaki, Naoto Yasuda, Alex Tsai, Yusuke Sawai, Hideki Owada, Isao Takayanagi, and Junichi Nakamura. 2020. “A Stacked Back Side-Illuminated Voltage Domain Global Shutter CMOS Image Sensor with a 4.0 \\(\\mu\\)m Multiple Gain Readout Pixel.” Sensors 20 (2): 486.\n\n\nMurakami, Hirotaka, Eric Bohannon, John Childs, Grace Gui, Eric Moule, Katsuhiko Hanzawa, Tomofumi Koda, et al. 2022. “A 4.9 Mpixel Programmable-Resolution Multi-Purpose CMOS Image Sensor for Computer Vision.” In 2022 IEEE International Solid-State Circuits Conference (ISSCC), 65:104–6. IEEE.\n\n\nMurmann, Boris. 2014. “ADC Performance Survey 1997-2024.” https://github.com/bmurmann/ADC-survey.\n\n\nNakamura, Junichi. 2006. Image Sensors and Signal Processing for Digital Still Cameras. CRC press.\n\n\nNitta, Yoshikazu, Yoshinori Muramatsu, Kiyotaka Amano, Takayuki Toyama, K Mishina, Atsushi Suzuki, Tadayuki Taura, et al. 2006. “High-Speed Digital Double Sampling with Analog CDS on Column Parallel ADC Architecture for Low-Noise Active Pixel Sensor.” In 2006 IEEE International Solid State Circuits Conference-Digest of Technical Papers, 2024–31. IEEE.\n\n\nNoble, Peter JW. 1968. “Self-Scanned Silicon Image Detector Arrays.” IEEE Transactions on Electron Devices 15 (4): 202–9.\n\n\nOhta, Jun. 2020. Smart CMOS Image Sensors and Applications. CRC press.\n\n\nOmmnomnomgulp. 2008. “A focal plane shutter firing at 1/500 of a second with the ‘gap’ clearly visible. This shutter is on a Nikon film SLR. CC BY-SA 3.0 license.” https://commons.wikimedia.org/wiki/File:1_500_Sec_Focal_P_Shut.jpg.\n\n\nPharr, Matt, Wenzel Jakob, and Greg Humphreys. 2023. Physically Based Rendering: From Theory to Implementation. 4th ed. MIT Press.\n\n\nProkudin-Gorsky, Sergey. 1948. “Library of Congress Prokudin-Gorskii Collection.” https://www.loc.gov/collections/prokudin-gorskii/about-this-collection/.\n\n\nSakakibara, Masaki, Yusuke Oike, Takafumi Takatsuka, Akihiko Kato, Katsumi Honda, Tadayuki Taura, Takashi Machida, et al. 2012. “An 83dB-Dynamic-Range Single-Exposure Global-Shutter CMOS Image Sensor with in-Pixel Dual Storage.” In 2012 IEEE International Solid-State Circuits Conference, 380–82. IEEE.\n\n\nSharpe, Lindsay T, Andrew Stockman, Wolfgang Jagla, and Herbert Jägle. 2005. “A Luminous Efficiency Function, v*(\\(\\lambda\\)), for Daylight Adaptation.” Journal of Vision 5 (11): 3–3.\n\n\n———. 2011. “A Luminous Efficiency Function, VD65*(\\(\\lambda\\)), for Daylight Adaptation: A Correction.” Color Research & Application 36 (1): 42–46.\n\n\nSolhusvik, Johannes, T Willassent, Sindre Mikkelsen, Mathias Wilhelmsen, Sohei Manabe, Duli Mao, Zhaoyu He, Keiji Mabuchi, and TA Hasegawa. 2019. “1280\\(\\times\\) 960 2.8 \\(\\mu\\)m HDR CIS with DCG and Split-Pixel Combined.” In Proceedings of the International Image Sensor Workshop (IISW), Snowbird, UT, USA, 23–27.\n\n\nStark, Laurence, Jeffrey M Raynor, Frederic Lalanne, and Robert K Henderson. 2018. “A Back-Illuminated Voltage-Domain Global Shutter Pixel with Dual in-Pixel Storage.” IEEE Transactions on Electron Devices 65 (10): 4394–4400.\n\n\nStoppa, David, Andrea Simoni, Lorenzo Gonzo, Massimo Gottardi, and G-F Dalla Betta. 2002. “Novel CMOS Image Sensor with a 132-dB Dynamic Range.” IEEE Journal of Solid-State Circuits 37 (12): 1846–52.\n\n\nSugawa, Shigetoshi, Nana Akahane, Satoru Adachi, Kazuya Mori, Toshiyuki Ishiuchi, and Koichi Mizobuchi. 2005. “A 100 dB Dynamic Range CMOS Image Sensor Using a Lateral Overflow Integration Capacitor.” In ISSCC. 2005 IEEE International Digest of Technical Papers. Solid-State Circuits Conference, 2005., 352–603. IEEE.\n\n\nSwain, PK, and David Cheskis. 2008. “Back-Illuminated Image Sensors Come to the Forefront.” Photonics Spectra 42 (8): 46.\n\n\nTakayanagi, Isao, Ken Miyauchi, Shunsuke Okura, Kazuya Mori, Junichi Nakamura, and Shigetoshi Sugawa. 2019. “A 120-Ke- Full-Well Capacity 160-\\(\\mu\\)v/e- Conversion Gain 2.8-\\(\\mu\\)m Backside-Illuminated Pixel with a Lateral Overflow Integration Capacitor.” Sensors 19 (24): 5572.\n\n\nTakayanagi, Isao, Norio Yoshimura, Kazuya Mori, Shinichiro Matsuo, Shunsuke Tanaka, Hirofumi Abe, Naoto Yasuda, et al. 2018. “An over 90 dB Intra-Scene Single-Exposure Dynamic Range CMOS Image Sensor Using a 3.0 \\(\\mu\\)m Triple-Gain Pixel Fabricated in a Standard BSI Process.” Sensors 18 (1): 203.\n\n\nTeranishi, Nobukazu. 2015. “Effect and Limitation of Pinned Photodiode.” IEEE Transactions on Electron Devices 63 (1): 10–15.\n\n\nTeranishi, Nobukazu, Akiyoshi Kohono, Yasuo Ishihara, Eiji Oda, and Kouichi Arai. 1982. “No Image Lag Photodiode Structure in the Interline CCD Image Sensor.” In 1982 International Electron Devices Meeting, 324–27. IEEE.\n\n\nThorseth. 2015. “Spectral power distribution of a 25 W incandescent light bulb; CC BY-SA 4.0 license.” https://commons.wikimedia.org/wiki/File:Spectral_power_distribution_of_a_25_W_incandescent_light_bulb.png.\n\n\nTournier, Arnaud, F Roy, Y Cazaux, F Lalanne, P Malinge, M Mcdonald, G Monnot, and N Roux. 2018. “A HDR 98dB \\(3.2\\mu\\mathrm {m}\\) Charge Domain Global Shutter CMOS Image Sensor.” In 2018 IEEE International Electron Devices Meeting (IEDM), 10–14. IEEE.\n\n\nTsugawa, H, H Takahashi, R Nakamura, T Umebayashi, T Ogita, H Okano, K Iwase, et al. 2017. “Pixel/DRAM/Logic 3-Layer Stacked CMOS Image Sensor Technology.” In 2017 IEEE International Electron Devices Meeting (IEDM), 3–2. IEEE.\n\n\nWeckler, Gene P. 1967. “Operation of Pn Junction Photodetectors in a Photon Flux Integrating Mode.” IEEE Journal of Solid-State Circuits 2 (3): 65–73.\n\n\nWillassen, Trygve, Johannes Solhusvik, Robert Johansson, Sohrab Yaghmai, Howard Rhodes, Sohei Manabe, Duli Mao, et al. 2015. “A 1280\\(\\times\\) 1080 4.2 \\(\\mu\\)m Split-Diode Pixel Hdr Sensor in 110 Nm Bsi Cmos Process.” In Proceedings of the International Image Sensor Workshop, Vaals, the Netherlands, 8–11.\n\n\nXu, Han, Ningchao Lin, Li Luo, Qi Wei, Runsheng Wang, Cheng Zhuo, Xunzhao Yin, Fei Qiao, and Huazhong Yang. 2021. “Senputing: An Ultra-Low-Power Always-on Vision Perception Chip Featuring the Deep Fusion of Sensing and Computing.” IEEE Transactions on Circuits and Systems I: Regular Papers 69 (1): 232–43.\n\n\nXu, Jiangtao, Liuqin Shu, Zhiyuan Gao, Quanmin Chen, and Kaiming Nie. 2022. “Analysis and Parameter Optimization of High Dynamic Range Pixels for Split Photodiode in CMOS Image Sensors.” IEEE Sensors Journal 22 (7): 6748–54.\n\n\nYasutomi, Keita, Shinya Itoh, and Shoji Kawahito. 2011. “A Two-Stage Charge Transfer Active Pixel CMOS Image Sensor with Low-Noise Global Shuttering and a Dual-Shuttering Mode.” IEEE Transactions on Electron Devices 58 (3): 740–47.\n\n\nYokoyama, Toshifumi, Masafumi Tsutsui, Yoshiaki Nishi, Ikuo Mizuno, Veinger Dmitry, and Assaf Lahav. 2018. “High Performance \\(2.5\\mu\\mathrm {m}\\) Global Shutter Pixel with New Designed Light-Pipe Structure.” In 2018 IEEE International Electron Devices Meeting (IEDM), 10–15. IEEE.\n\n\nYoung, Christopher, Alex Omid-Zohoor, Pedram Lajevardi, and Boris Murmann. 2019. “A Data-Compressive 1.5/2.75-Bit Log-Gradient QVGA Image Sensor with Multi-Scale Readout for Always-on Object Detection.” IEEE Journal of Solid-State Circuits 54 (11): 2932–46.\n\n\nZhu, Yuhao. 2022. “Exploring Camera Color Space and Color Correction.” https://horizon-lab.org/colorvis/camcolor.html.",
    "crumbs": [
      "Imaging",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Image Sensor Architecture</span>"
    ]
  },
  {
    "objectID": "imaging-sensor.html#footnotes",
    "href": "imaging-sensor.html#footnotes",
    "title": "16  Image Sensor Architecture",
    "section": "",
    "text": "For the charges collected in PD to be transferable to the FD, the photodiode needs to be “pinned”, which means there is another layer of p+ implant above the p-n junction pinned to the ground (0 V). Such a PD is also called the Pinned Photodiode, or PPD (Teranishi et al. 1982; Teranishi 2015; Fossum and Hondongwa 2014).↩︎\n\\(V_1\\) and \\(V_{rst}\\) technically are ever so slightly different because the charges might be leaking between resetting and read out.↩︎\nFor instance in Solhusvik et al. (2019), the sensitivity ratio between the LPD and SPD is over 100\\(\\times\\), but the FWC of the SPD is less than three times smaller than that of the LPD.↩︎\nThey shared the Nobel Prize in Physics in 2009.↩︎\nIt is worth noting, however, that it is difficult for the CCD sensor to perform CDS because of its read-out architecture (shifting charges to a single SF amplifier).↩︎\nIt is interesting to note the fact that there is a fundamental pixel size limit negates one advantage of the CCD sensors, where the pixel design is simpler so one can theoretically make the pixel size smaller, but that is countered by the limit to which the PDs can shrink (Fossum 1997).↩︎\nDon’t be confused by the two similar notations that represent different quantities: \\(N\\) for the number of charges at a pixel and \\(Q\\) for the energy at a pixel.↩︎\nIf we want to be pedantic, each green pixel has a small, but non-infinitesimal, area, so it first performs a low-pass filtering using a box filter whose extent is the pixel area, followed by sampling at the center of the pixel.↩︎",
    "crumbs": [
      "Imaging",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Image Sensor Architecture</span>"
    ]
  },
  {
    "objectID": "imaging-noise.html",
    "href": "imaging-noise.html",
    "title": "17  Noise",
    "section": "",
    "text": "This chapter reviews different sources of noise during imaging. We will study the physical mechanisms through which the noises trickle in, build computational models for different noise sources, and discuss basic denoising strategies. Before the chapter is fully developed, check our lecture slides on imaging noise.",
    "crumbs": [
      "Imaging",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Noise</span>"
    ]
  },
  {
    "objectID": "imaging-isp.html",
    "href": "imaging-isp.html",
    "title": "18  Camera Signal Processing",
    "section": "",
    "text": "18.1 General Pipeline\nThis chapter reviews basics camera signal processing algorithms. The output of an image sensor is what we usually call the raw pixels. The raw pixels are not the usual RGB images we are used to see. For starters, if we use the common CFA approach for color sensing, each raw pixel has only one color channel response — the two missing channel responses must be recovered. We have also ignored noises, which are introduced every step along the signal transduction chain from incident lights to raw values. Therefore, the raw pixels usually go through a post-processing pipeline before raw sensor can be consumed, whether it is by a human observer or, increasingly, machine vision algorithms.\nThat pipeline in modern cameras is implemented by a special hardware accelerator called the Image Signal Processor (ISP), which is an Intellectual Property (IP) block in a mobile System-on-a-Chip (SoC). Implementing the post-processing algorithms in dedicated hardware makes a lot of sense from an efficiency perspective: when you press a button to capture an image, you certainly do not want to wait for a long time or burn a lot of energy before the image is shown to you. As many mobile vendors do not actually control the optics and the sensor, the ISP increasingly has become the key product differentiator. As a result, many companies have their custom ISP designs; for instance, Qualcomm’s Snapdragon SoC has their own Spectra ISP.\nMany texts exist on the general ISP algorithm (Ramanath et al. 2005; Karaimer and Brown 2016) and the hardware design (Hegarty et al. 2014), which we refer you to. We also have a pedagogical ISP written in Python (Zhu 2022a) that is a good reference, too. The goal of this section is to walk through the general pipeline and point out main ideas. One thing worth emphasizing here is that the ISP design is strongly influenced by the downstream task that consumes the output of the ISP. The two main consumers are human vision and machine vision. The former cares about visually pleasing images while the latter does not — as long as the key semantic information is retained and can be extracted.\nFigure 18.1 shows a general ISP pipeline and how it fits into the entire imaging pipeline. The ISP takes the raw pixels generated by the sensor and generates two types of output: the finished image, usually encoded in sRGB color space and compressed, and statistics of the image that are used to drive the so-called “3A” algorithms, i.e., auto white balance (AWB), auto exposure (AE), and auto focus (AF). We will not have much time to discuss the 3A algorithms, but they can be thought of as feedback controls over the rest of the imaging system: AWB controls the white balancing stage in the ISP, AE controls the exposure time of the image sensor, and AF controls the lens movement in the optics. The 3A algorithms usually run on the host CPU or an MCU because they are relatively simple computationally.\nThe ISP pipeline shown here is a general architecture that covers roughly what an ISP has to do. Keep in mind that the exact stages and their arrangements are proprietary and vary by vendors. Regardless, all ISPs operate on a set of basic principles.",
    "crumbs": [
      "Imaging",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Camera Signal Processing</span>"
    ]
  },
  {
    "objectID": "imaging-isp.html#general-pipeline",
    "href": "imaging-isp.html#general-pipeline",
    "title": "18  Camera Signal Processing",
    "section": "",
    "text": "Figure 18.1: A general ISP pipeline. The exact stages and arrangement of the stages are proprietary. The ISP pipeline outputs both the RGB (or other color spaces) images and image statistics; the latter is used to drive the so-called “3A” algorithms, which are feedback controls over the lens, sensor, and the ISP.\n\n\n\n\n\nRecall that the raw pixel values output by the sensor should ideally be proportional to the scene luminance, but this is hardly the case in reality. The first thing an ISP does is to recover luminance-proportional values from the raw sensor output; this includes three main steps.\n\nThe first step is called dark signal subtraction (DSS) or back-light subtraction. This is necessary because even when pixels receive no light at all, their raw values are usually not zero. This is because of “dark current”, formed by thermally dislodged electrons even in the absence of incident photons. Measuring the raw values of “optical black” pixels (Kameda 2012) per frame and subtracting those values would allow us to eliminate the effect of the dark current1.\nThe second step is called flat-field correction or lens-shading correction (LSC). It accounts for the fact that the raw pixel values (with dark current subtracted) are spatially non-uniform, a phenomenon called “vignetting”, where even under uniform (flat-field) illumination peripheral pixels receive fewer photons. It is caused by a variety of reasons: the mechanical design (including microlenses) of the camera blocks more lights toward the edge of the pixel array, the radiance fall-off (the \\(\\cos\\theta\\) term) when rays incident in an oblique angles, etc. We can pre-calibrate this non-uniformity, store it in an image, and compensate for the non-uniformity for each frame.\nThe final step is denoising. Many excellent discussions of noise sources exist, such as Boukhayma (2018), Nakamura (2006, chap. 3.3), and Rowlands (2020b, chaps. 3.8–3.9), which we refer you to. Regardless of the noise source, the general strategy of denoising is low-pass filtering: blurs are subjectively less objectionable than noises.\n\nIt is important that these steps are taken at the very beginning of an ISP: if the pixel values are noisy, any subsequent manipulations on the pixels also manipulate, sometimes amplify, the noise.\nAfter that, we can assume that the raw pixels carry physical meanings: they are proportional to luminance, but of course because of the CFA, the color information is spatially sampled. The raw pixels before demosaicing are usually called pixels in the Bayer domain. The next stage is demosaicing, which essentially reconstructs the color information (all three channels) from the single-channel samples. While many reconstruction filters/kernels exist, the easiest and most commonly used filter is the bilinear filter.\nThe demosaiced color information is encoded in the sensor’s native color space, because the sensor’s SSFs almost certainly do not match the cone fundamentals. So the next stage is to transform color from the sensor’s native color space to a device-independent color space such as the CIE XYZ space. We build an interactive tutorial to walk you through this correction process that you are invited to play with (Zhu 2022b). White balancing usually is implemented along with color correction, because both involve linear transformations of colors (Rowlands 2020a; Zhu 2021).\nUsually there is a tone mapping stage in the ISP. The dynamic range of the raw pixels is usually (but not always) larger than that of a typical output medium (e.g., a display or a print). Tone mapping operators map signals between the two dynamic ranges so that the output image is visually appealing. Tone mapping participates in the overall Opto-Optical Transfer Function (OOTF) of an end-to-end display pipeline, which we discuss in detail in Chapter 21.\nThe final output is usually compressed, either through an image compression algorithm (e.g., JPEG) or, in the case of video capturing, a video compression algorithm (e.g., H264).",
    "crumbs": [
      "Imaging",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Camera Signal Processing</span>"
    ]
  },
  {
    "objectID": "imaging-isp.html#two-trends",
    "href": "imaging-isp.html#two-trends",
    "title": "18  Camera Signal Processing",
    "section": "18.2 Two Trends",
    "text": "18.2 Two Trends\nFirst, it has become increasingly common to co-design ISP algorithms, along with optics and image sensor design, with the downstream tasks. This is particularly important for machine vision, which is not concerned with the traditional goal of an ISP, i.e., generating visually pleasing images. A co-design between the ISP and the machine vision algorithms could potentially improve both task quality and efficiency.\nSecond, a huge amount of recent efforts have been spent on exploring the notion of “neural ISP”, which is nothing more than replacing part, or the entirety, of the ISP pipeline with deep neural networks (DNNs). The learning paradigm has two main advantages: it replaces some of the heuristics in traditional ISP designs, and it allows the algorithm to be more easily updated without having to wait until the next generation of the product. The latter point is possible because a neural ISP pipeline can run on a DNN accelerator that almost all modern mobile SoCs have, and updating the algorithm is nothing more than updating the model weights.\nThe key issue with neural ISP is speed and efficiency: a neural ISP model executed on a generic DNN accelerator is likely much slower and more energy hungry than traditional ISPs. So it is more likely that neural ISPs will find their main uses in offline image processing and photo finishing rather than in the real-time imaging pipeline.\n\n\n\n\nBoukhayma, Assim. 2018. Ultra Low-Noise CMOS Image Sensors. Springer.\n\n\nHegarty, James, John S Brunhaver, Zachary DeVito, Jonathan Ragan-Kelley, Noy Cohen, Steven Bell, Artem Vasilyev, Mark Horowitz, and Pat Hanrahan. 2014. “Darkroom: Compiling High-Level Image Processing Code into Hardware Pipelines.” ACM Trans. Graph. 33 (4): 144–41.\n\n\nKameda, Shinjiro. 2012. “Solid State Image Pickup Device Having Optical Black Pixels with Temperature Characteristics Above and Below Temperature Characteristics of Aperture Pixels.” Google Patents.\n\n\nKaraimer, Hakki Can, and Michael S Brown. 2016. “A Software Platform for Manipulating the Camera Imaging Pipeline.” In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, the Netherlands, October 11–14, 2016, Proceedings, Part i 14, 429–44. Springer.\n\n\nNakamura, Junichi. 2006. Image Sensors and Signal Processing for Digital Still Cameras. CRC press.\n\n\nRamanath, Rajeev, Wesley E Snyder, Youngjun Yoo, and Mark S Drew. 2005. “Color Image Processing Pipeline.” IEEE Signal Processing Magazine 22 (1): 34–43.\n\n\nRowlands, D Andrew. 2020a. “Color Conversion Matrices in Digital Cameras: A Tutorial.” Optical Engineering 59 (11): 110801–1.\n\n\n———. 2020b. Physics of Digital Photography. 2nd ed. IOP Publishing.\n\n\nZhu, Yuhao. 2021. “Principles and Practices of Chromatic Adaptation.” https://yuhaozhu.com/blog/chromatic-adaptation.html.\n\n\n———. 2022a. “A pedagogical ISP pipeline in Python.” https://github.com/horizon-research/isp.\n\n\n———. 2022b. “Exploring Camera Color Space and Color Correction.” https://horizon-lab.org/colorvis/camcolor.html.",
    "crumbs": [
      "Imaging",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Camera Signal Processing</span>"
    ]
  },
  {
    "objectID": "imaging-isp.html#footnotes",
    "href": "imaging-isp.html#footnotes",
    "title": "18  Camera Signal Processing",
    "section": "",
    "text": "Note, however, that DSS does not eliminate the effect of dark current shot noise, which results from fluctuations in dark current during the exposure time (so the dark current is in theory different for different pixels even if they have the exact temperature), and the effect of dark current non-uniformity, which results from spatial differences in dark current across pixels (because of, for instance, the spatial temperature differences).↩︎",
    "crumbs": [
      "Imaging",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Camera Signal Processing</span>"
    ]
  },
  {
    "objectID": "display.html",
    "href": "display.html",
    "title": "Display",
    "section": "",
    "text": "This part of the book discusses the principles and practical implementations of display systems. Fundamentally, displays convert electrical signals, i.e., image pixels, into optical signals, i.e., light emitted from the display. Ultimately, the goal of this conversion is to present information to humans as if humans were directly looking at a real-world scene. The scene could actually exist in the real world, or could be a virtual environment and simulated/rendered by a rendering system. Either way, the intended scene information is encoded as a set of images, which are then turned into light by the display system. The quality of a display system is, thus, assessed by how closely matched is the viewing experience on the display vs. that of the intended, real-world scene.\nI keep using the word “display system” rather than simply “display”; this is to emphasize that a display is a system, including not only the actual devices that emit light (Chapter 19  Optical Mechanisms) but also the electrical driving circuits (Chapter 20  Driving Circuits) that use image pixels to drive the emissive devices as well as the display signal processing pipeline (Chapter 21  Display Signal Processing) that generates the pixel values sent to the driving circuits.",
    "crumbs": [
      "Display"
    ]
  },
  {
    "objectID": "display-optics.html",
    "href": "display-optics.html",
    "title": "19  Optical Mechanisms",
    "section": "",
    "text": "19.1 Light Emission Mechanisms\nWe will first discuss the fundamental mechanisms that produce light (Section 19.1), followed by various mechanisms to produce color (Section 19.2) and to control luminance (Section 19.3). Finally, we will take a look at two smartphone displays and their gamuts (Section 19.4).\nDisplays fundamentally transform electrical signals, i.e., image pixels, to optical signals, i.e., light. The most common device used for this electrical-to-optical signal transduction is an Light Emitting Diode (LED), is a semiconductor device that emits light when an electric current passes through it.\nApplying an external voltage across the p–n junction injects electrons from the n-type side and holes from the p-type side into the junction. When electrons recombine with holes, they release energy as photons. The photon’s wavelength is determined by the semiconductor’s band gap, which depends on the material composition. Common semiconductors used for LEDs include AlGaInP (aluminum gallium indium phosphide, for red, orange, or yellow LEDs) and InGaN (indium gallium nitride, for green, blue, or white LEDs).\nFigure 19.1 (a) shows the emission spectra of two InGanN and two AlGaInP LEDs, and Figure 19.1 (b) shows the chromaticities of the four LEDs in the CIE 1931 xy-chromaticity diagram. Generally, the emission spectra of InGanN and AlGaInP are pretty sharp, leading to saturated colors.\nInGanN and AlGaInP are inorganic materials. They are usually very large, millimeter in size, so they cannot be used as individual display pixels. OLED (Organic LED) displays use organic (i.e., carbon-based) molecules/polymers, which can be small and act as individual pixels or sub-pixels (see later). MicroLED displays, which many believe are the future of displays, use miniatured inorganic LEDs as individual pixels. These tiny inorganic LEDs have better (optical) properties than organic LEDs, e.g., higher luminance, higher electrical to optical conversion efficiency, and longer lifetime, but are more difficult to manufacture.\nFinally, there is also the Quantum-Dot LED (QLED), which is another kind of LED material that uses quantum dots, tiny nanoparticles of semiconductor, to emit light. QLEDs rely on the quantum effects of the quantum dots to produce color: generally small dots emit higher energy (shorter wavelength, bluer) light, and larger dots emit lower energy (longer wavelength, redder) light1. Commercial QLEDs operate based on fluorescence, where light emission is driven by an external light (Agarwal, Rai, and Mondal 2023). The cutting edge research, however, focuses on using quantum dots as “conventional”, electrical driven devices that emit lights under external current/voltage (Mashford et al. 2013; Shirasaki et al. 2013; Qian et al. 2011).",
    "crumbs": [
      "Display",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Optical Mechanisms</span>"
    ]
  },
  {
    "objectID": "display-optics.html#sec-disp-optics-mechanism",
    "href": "display-optics.html#sec-disp-optics-mechanism",
    "title": "19  Optical Mechanisms",
    "section": "",
    "text": "Figure 19.1: (a): emission spectra of four different LEDs; (b): the corresponding colors in the xy-chromaticity diagram. Adapted from Thomson, Stuart (2018, figs. 3, 4).\n\n\n\n\n\n\nOther Mechanisms\nAnother common light emission mechanism is fluorescence, of which we have seen one use in QLEDs above. Generally, fluorescing materials absorb a photon at a shorter wavelength, which excite an electron across the band gap. When the electron relaxes and recombines with the hole, a photon is emitted, usually at a longer wavelength. The shift between the absorption and emission wavelength is called the Stokes shift. Usually blue/UV lights are absorbed and green lights are emitted — that is why fluorescence is usually green. Since the luminance efficiency function (LEF) peaks at 555 \\(\\text{nm}\\), green-ish light (Section 4.3.2), fluorescence usually makes object appear brighter/more conspicuous, even though the emitted energy is almost always lower than the incident energy.\nIn fluorescent light blubs, phosphors are commonly used as fluorescent material. Usually we use UV light (generated by applying current to excite mercury vapor) to excite the phosphor coating inside a lamp, which then fluoresces. What enters you eyes is the combination of the fluorescent light and UV light directly emitted by the mercury vapor.\n\n\n\n\n\n\nFigure 19.2: (a) the emission spectrum of a typical fluorescent light bulb; from Daniel Smith (2016); (b): the spectrum of a phosphor-converted white LED; adapted from Deglr6328 (2018).\n\n\n\nFigure 19.2 (a) shows the emission spectrum of a typical fluorescent light blub. The two shorter-wavelength peaks are likely from the UV lights themselves, and the rest of the peaks are likely resulted from the phosphor emissions.\nOther than fluorescent blubs, many white LEDs also make use of fluorescence. They are so-called phosphor-converted white LEDs. Figure 19.2 (b) shows the spectrum of one such LED, where the peak at the shorter wavelength is caused by the regular InGaN LED emission and the longer wavelength peak is emitted by the Ce:YAG phosphor (cerium-doped yttrium aluminium garnet). These two spectra combined together give a relatively broad-band, white-ish emission spectrum.",
    "crumbs": [
      "Display",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Optical Mechanisms</span>"
    ]
  },
  {
    "objectID": "display-optics.html#sec-disp-optics-color",
    "href": "display-optics.html#sec-disp-optics-color",
    "title": "19  Optical Mechanisms",
    "section": "19.2 Color Production",
    "text": "19.2 Color Production\nThe trichromaticy theory of color tells us that reproduce a color we need only three primary colors. This is how all displays produce colors: each image pixel’s color is produced by combining three primary colors. There are two main strategies to implement the three primaries, one relies on the spatial integration of the human visual system, and the other relies on the temporal integration.\n\n19.2.1 Subpixels\nIn many displays, each display pixel is implemented by three sub-pixels, each of which has an implementation-specific emission spectrium and acts as a primary light. The retina then spatially integrates the lights from the three sub-pixels, i.e., mixing the three primary colors. Figure 19.3 shows two subpixels structures in iPhone 6 and iPhone 14 Pro. Different phones usually use slightly different subpixel structures, often times to avoid patent infringement.\n\n\n\n\n\n\nFigure 19.3: Two subpixels structures in iPhone 6 (LCD) and iPhone 14 Pro (OLED display).\n\n\n\nWhile three subpixels are the absolute minimal for color production, there are also displays that use more than three primaries. At an area and resolution cost, more primaries means a larger color gamut and provides more flexibility. For instance, if the color of one of the primaries is slightly off (e.g., due to aging), the additional primaries can be used to maintain the overall color reproduction accuracy.\nIn particular, there are four-primary displays that have an additional white subpixel. The white subpixels are useful in two ways. First, we can use the white subpixels for actually producing white rather than mixing the other three subpixels. This generally improves the power efficiency. Second, we can add white to artificially boost the luminance of a pixel, but this comes at the cost of sacrificing the color reproduction accuracy: the resulting color becomes more desaturated. This is commonly exploited in projectors.\n\nFiltering vs. Emissive Displays\nThere are two ways to realize the subpixels. Liquid-Crystal Display (LCD) is a light filtering display that uses a white backlight made from a few large inorganic LEDs, which is then filtered by per-pixel color filters to produce subpixel colors. Emissive displays, such as OLED and MicroLED displays, directly use emissive LEDs as individual pixels.\n\n\n\n\n\n\nFigure 19.4: The architectural comparison between an OLED and a LCD.\n\n\n\nFigure 19.4 compares the architectures between the OLED display and the LCD. In a LCD, the backlight is usually broadband (white) produced either by the phosphor-converted white LEDs or a mixture of RGB LEDs, all are conventional large inorganic LEDs. Each sub-pixel is associated with a color filter with a particular transmittance spectrum that, together with the backlight spectrum, determines the color of the sub-pixel.\nEach sub-pixel has a liquid crystal (LC) cell sandwiched between a rear polarizer and a front polarizer. The rear polarizer rotates the backlight to a particular polarization direction, say vertical. The front polarizer is set to allow light with only the horizontal polarization to go through. Without the LCs in-between, backlight passing through the rear polarizer is blocked by the front polarizer.\nWhen no voltage is applied, the LC cells are in their default twist state, which rotates the polarization of the light by 90 degrees so that it pass the second polarizer, making the pixel transparent and producing the highest luminance. When a voltage is applied, the LC cells align with the electric field and untwist. This alignment causes no change to the polarization of the light leaving the rear polarizer, so the light is blocked by the front polarizer, producing the lowest luminance.\nThis relationship between voltage and transmittance (i.e., low voltage leads to high transmittance) is characteristic of Twisted Nematic (TN) LC cells. There are other LC technologies such as In-Plane Switching (IPS) and Vertical Alignment (VA) (which are better alternatives to TN (Trussell and Vrhel 2008, chap. 11.2)) where the relationship is inverted.\n\n\n\n19.2.2 Field-Sequential Color\nOther displays use the Field-Sequential Color (FSC) mechanism to produce color. In FSC, each image is presented by three sequential fields, each of which produces light only using on primary color. FSC then relies on the temporal integration of our visual system to create colors.\nIn fact, early color TV was delivered using the FSC mechanism. CBS debuted the FSC TV system in 1940s; it is obviously obsolete now, but you can still watch a video of its operation here. Figure 19.5 (a) shows a modern replication of the CBS’ FSC color wheel, which has 2 set of three filters, so there are 6 filters in total per rotation. The wheel spins 24 rotations per second, which amounts of 144 filters per second.\n\n\n\n\n\n\nFigure 19.5: (a) a color wheel similar to the one used by CBS’ FSC TV; (b) the integrated frame shows color. From LabGuy’s World (2014).\n\n\n\nThe TV itself is a Cathode-Ray Tube (CRT) display, which scans the entire display 144 times a second, producing 144 frames. Each frame, received from the broadcaster, is broadband and contains only luminance information. The frame presentation and the color wheel rotation must be perfectly in sync such that as a frame is scanned on the TV, the corresponding filter is placed in front of the TV, displaying a fully red, green, or blue field. Figure 19.5 (b) shows an effective color image of the TV taken by a camera.\n\n\n\n\n\n\nFigure 19.6: (a) the light path inside a DLP projector: light from a lamp (covered inside so not seen here) goes through the color wheel, reflected off a mirror on the left to the DMD, which controls the light intensity and sends to the light through the projection lens to the screen; adapted from DMahalko (2009c), DMahalko (2009a), and DMahalko (2009b); (b) the operating principle of a DMD, where every pixel can be mechanically turned on (oriented to reflect the light to a target of interest) or off (oriented to reflect the light away from the target); from Allen (2017, fig. 1).\n\n\n\n\nDLP and LCoS\nToday, the most common example of an FSC display is the Digital Light Processing (DLP) projectors. Figure 19.6 (a) shows the interior view of a typical DLP. Light comes from a light source on the right (blocked by the projector housing in the figure and not seen), which could be a broadband lamp or a mixture of different LEDs. The light first passes through the color wheel, which in this case has four filters, red, gree, blue, and transparent. This is equivalent to a four-primary system as discussed above. The light then is reflected by a mirror on the left to a Digital Micromirror Device (DMD) at the bottom, which has an array of pixels/tiny mirrors, each of which can be mechanically turned by a yoke.\nFigure 19.6 (b) illustrates the basic structure of a DMD. Each pixel can be turned either “on”, which directs the incident light to the projection lens, or turned “off”, which directs the incident light away from the projection lens. The light passing through the project lens then leaves the system and enters the scene. The DMD controls the luminance of each pixel by the ratio of the on-time to the off-time. This luminance-control mechanisms is essentially pulse-width modulation, which we will discuss shortly.\nAnother application of FSC is in Liquid Crystal on Silicon (LCoS) displays, which, in princple, combine DLP projectors and LCDs. Like DLPs, LCoS displays also use the FSC and use a per-pixel mirror to reflect light to the scene. Unlike DLPs, the mirror is always “on”. The mechanism that determines the pixel luminance is, instead, similar to that in LCDs.\nLike LCDs, there are two polarizers. The light source, originally unpolarized, is polarized by the first polarizer before hitting the LC array. The voltage applied to each (per-pixel) LC rotates the polarization state of the light, which then reflects off the pixel’s mirror and goes through the second polarizer, which transmits light with a polarization state that is rotated 90 degrees from that of the first polarizer. Essentially like LCDs, the voltage determines the amount of light emitted from each pixel.",
    "crumbs": [
      "Display",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Optical Mechanisms</span>"
    ]
  },
  {
    "objectID": "display-optics.html#sec-disp-optics-lum",
    "href": "display-optics.html#sec-disp-optics-lum",
    "title": "19  Optical Mechanisms",
    "section": "19.3 Luminance Control",
    "text": "19.3 Luminance Control\nNow we know how displays can produce (at least) three primaries, the next question is how do we control their luminance. We keep using “luminance” in this chapter: if we know the emission spectral of the sub-pixels, determining/knowing the luminance is equivalent to knowing the color. We have already seen some of the ways different display architectures use to control luminance before. In general, there are two ways to control the luminance: pulse width modulation (PWM) or pulse amplitude modulation (PAM).\n\n19.3.1 PAM\nIn PAM, we (directly or indirectly) control the voltage or current supplied to a pixel, which then changes the amount of photons emitted (and thereby luminance).\nFor LEDs (whether organic or inorganic), the luminance is proportional to the current that flows through the diode. Recall in LEDs the photons are emitted through the creation of electron-hole pairs, so the amount of photon emission is naturally proportional to the number of electrons injected (through an external voltage), which is proportional to current.\nMore formally, we can define quantum efficiency \\(\\eta(\\lambda)\\) of an LED (Schubert 2006, chap. 5): \\[\n    \\eta(\\lambda) = \\frac{N_{\\text{photons}}}{N_{\\text{electrons}}} = \\frac{\\Phi(\\lambda)/(h f)}{I/e},\n\\tag{19.1}\\]\nwhere \\(N_{\\text{photons}}\\) is the number of photons emitted to space per second, \\(N_{\\text{electrons}}\\) is the number of electrons injected to the LED per second, \\(\\Phi(\\lambda)\\) is the emitted power at wavelength \\(\\lambda\\), \\(h\\) is the Planck’s constant, \\(f\\) is the frequency of the photons, \\(I\\) is the injection current (total charges per second), and \\(q\\) is the elementary charge.\nIn reality, however, the relationship between current and luminance can be non-linear because \\(\\eta\\) is not a constant. For instance, the quantum efficiency reduces as current increase (Dai et al. 2010; Deng et al. 2017).\nThe relationship between the voltage across an LED and the current through the LED, known as the I-V curve, is non-linear. Generally, the relationship is exponential (Geffroy, Le Roy, and Prat 2006; Tsujimura 2017, chap. 4.2; Miller 2019, chap. 6.1.2), governed by the Shockley diode equation (Shockley 1949):\n\\[\n    I = I_0 (e^{V/nV_T} - 1),\n\\tag{19.2}\\]\nwhere \\(I\\) is the diode forward current, \\(V\\) is the voltage across the diode, \\(I_0\\) is the saturation current, \\(n\\) is diode ideality factor, and \\(V_T\\) is a temperature dependent thermal voltage. \nImportantly, however, we do not get to directly control the voltage or current across the LED. The exact driving circuit will be discussed in Chapter 20.\n\n\n\n\n\n\nFigure 19.7: Relationship between the transmittance and voltage applied to (a) a Twisted Nematic (TN) LC cell; from Gauza et al. (2007, fig. 5) and (b) an In-Plane Switching (IPS) LC cell; from Jeon et al. (2009, fig. 2b).\n\n\n\nFor LCDs, the luminance depends on the LC transmittance, which depends on the voltage that is applied on the LC. Figure 19.7 (a) shows the relationship between transmittance and voltage of a TN LC cell. Overall, the relationship is not linear: the transmittance is almost invariant to voltage at very low or very high voltage levels, but in the mid-voltage range the relationship is close to linear (Trussell and Vrhel 2008, chap. 11.2; Gauza et al. 2007; Lee, Lee, and Kim 1998; Jeon et al. 2009; Hong, Shin, and Chung 2007). Figure 19.7 (b) shows the transmittance vs. voltage relationship for an IPS cell, where the relationship is largely inverted. Generally, the LCD luminance does not scale linearly with the voltage.\n\n\n19.3.2 PWM\nIn PWM, the voltage or current supplied to a pixel is fixed but we control the duty cycle, the period in which the voltage or current is active. For instance, with PWM the LC in the LCD is either fully twisted or not twisted, and we control the time during which the LC is in each state. Similarly for emissive displays, each subpixel either emit no light or maximum amount of light. What changes is the time during which the subpixels emit light.\nDuty cycle is always between 0 and 1. Assuming there is a display that refreshes 60 times a second, a duty cycle of 0.5 would mean that the pixels emit light in about \\(1\\text{s}/60\\times 0.5\\approx 8.3\\text{ms}\\) during each refresh cycle. Figure 19.8 shows two PWM examples with a 25% and 75% duty cycle, respectively.\n\n\n\n\n\n\nFigure 19.8: Two PWM examples with a 25% and 75% duty cycle, respectively. Ideally each pulse is a perfect square wave, in which case the luminance is proportional to the duty cycle, but in reality pulses take time to rise and fall, so the luminance is sub-linear w.r.t. duty cycle.\n\n\n\nIn theory, the luminance is proportional to the duty cycle — assuming that the pulse is a perfect square wave (the black curve in Figure 19.8). In reality, it takes time for the pulse to rise and fall (e.g., for a liquid crystal cell to change its orientation), so there is some efficiency loss (the green curve in Figure 19.8). As a result, the luminance is sub-linear with respect to the intended duty cycle. The longer the duty cycle, the more we can amortize this efficiency loss and the closer we get to being linear. Generally, the LEDs respond to current changes in the nanoseconds to microseconds range, while the liquid crystal cells respond much slower, usually at the milliseconds range.",
    "crumbs": [
      "Display",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Optical Mechanisms</span>"
    ]
  },
  {
    "objectID": "display-optics.html#sec-disp-optics-gamut",
    "href": "display-optics.html#sec-disp-optics-gamut",
    "title": "19  Optical Mechanisms",
    "section": "19.4 Display Native Gamut",
    "text": "19.4 Display Native Gamut\nRegardless of how color and luminance are produced, ultimately a display has a set of effective primaries, which make up the display’s native color space, which is most likely not exactly sRGB or any standard color space. The primary colors (and the white point) depend on the emission spectrum of each sub-pixel, which in turn depends on the material used. For instance, inorganic LEDs have a narrower emission spectra than the organic LEDs (Huang et al. 2020), so they tend to be able to generate more saturated colors and, thus, the resulting display gamut is wider. One has to balance multiple trade-offs in a display design, such as invariance of chromaticity vs. luminance, lifetime, power consumption, and cost, so it is difficult to tune the pixel spectra just so that the colors precisely match that of a standard.\n\n\n\n\n\n\nFigure 19.9: Microscope-magnified subpixel images of P3 green and sRGB green primary (both are [0, 255, 0] in their respective color spaces) on a 4th-generation iPad Pro taken from an iPhone 12 Pro (whose image signal processing chain introduces color inaccuracies; the red sub-pixel contributions to the sRGB green are not as strong when seen by naked eye). As a side note, you can also see that when the image is focused on the green sub-pixels, the red (and blue) sub-pixels are out of focus, a result of chromatic aberration.\n\n\n\nAs an example, Figure 19.9 shows the the sub-pixels images of the green primary colors in the P3 and sRGB color space as displayed on a 4th-generation iPad Pro. We can make a few observations. First, the emission patterns of P3 green and sRGB green are different. The P3 green is more “pure”, where the red and blue sub-pixels are contributing very little, whereas the sRGB green requires noticeable contribution from the red sub-pixels. This is not surprising because the P3 green is much more saturated (closer to spectral colors) than the sRGB green, as shown in the right figure in Figure 5.3. The actual contributions of red sub-pixels in sRGB green as seen by my eye are not as strong as seen in this iPhone-taken image; the image signal processing pipeline in the iPhone definitely has introduced its artifacts.\nSecond, even for the P3 green, there are still some contributions from the red sub-pixels. This suggests that the native display gamut is different from, in fact larger than, P3. This makes sense: for a display to support a particular color space, say, the P3 space, the display’s native color space must be no smaller than the P3 space.\nIt is worth noting that the spectrum/color of the light emitted from a sub-pixel is angularly dependent. This is at least partially because some emitted photons might not escape the LED because of the internal reflection at the material-air interface, and this reflection depends on photon arrival angles (Schubert 2006, chap. 5). Display measurement standard usually defines the angles at which a display’s color and luminance must be measured for a full display performance characterization (ICDM 2025, chap. 9).\n\n\n\n\n\nAgarwal, Kushagra, Himanshu Rai, and Sandip Mondal. 2023. “Quantum Dots: An Overview of Synthesis, Properties, and Applications.” Materials Research Express 10 (6): 062001.\n\n\nAllen, John. 2017. “Application of Patterned Illumination Using a DMD for Optogenetic Control of Signaling.” Nature Methods 1.\n\n\nDai, Qi, Qifeng Shan, Jing Wang, Sameer Chhajed, Jaehee Cho, E Fred Schubert, Mary H Crawford, Daniel D Koleske, Min-Ho Kim, and Yongjo Park. 2010. “Carrier Recombination Mechanisms and Efficiency Droop in GaInN/GaN Light-Emitting Diodes.” Applied Physics Letters 97 (13).\n\n\nDaniel Smith. 2016. “Calculating the Emission Spectra from Common Light Sources.” https://www.comsol.com/blogs/calculating-the-emission-spectra-from-common-light-sources/.\n\n\nDeglr6328. 2018. “Spectrum of a \"white\" LED showing blue light directly emitted by the GaN LED and the stokes shifted yellowish light emitted by the Ce:YAG phosphor; CC BY-SA 3.0 license.” https://commons.wikimedia.org/wiki/File:White_LED.png.\n\n\nDeng, Xiong, Yan Wu, AM Khalid, Xi Long, and Jean-Paul MG Linnartz. 2017. “LED Power Consumption in Joint Illumination and Communication System.” Optics Express 25 (16): 18990–9003.\n\n\nDMahalko. 2009a. “InFocus LP425z Single Chip DLP - 4-segment color wheel - Green Blue; CC BY-SA 3.0 license.” https://commons.wikimedia.org/wiki/File:InFocus_LP425z_Single_Chip_DLP_-_4-segment_color_wheel_-_Green_Blue.JPG.\n\n\n———. 2009b. “InFocus LP425z Single Chip DLP - 4-segment color wheel - Red Gray; CC BY-SA 3.0 license.” https://commons.wikimedia.org/wiki/File:InFocus_LP425z_Single_Chip_DLP_-_4-segment_color_wheel_-_Red_Gray.JPG.\n\n\n———. 2009c. “InFocus LP425z Single Chip DLP - DMD Light Path; CC BY-SA 3.0 license.” https://commons.wikimedia.org/wiki/File:InFocus_LP425z_Single_Chip_DLP_-_DMD_Light_Path.jpg.\n\n\nGauza, Sebastian, Xinyu Zhu, Wiktor Piecek, Roman Dabrowski, and Shin-Tson Wu. 2007. “Fast Switching Liquid Crystals for Color-Sequential LCDs.” Journal of Display Technology 3 (3): 250–52.\n\n\nGeffroy, Bernard, Philippe Le Roy, and Christophe Prat. 2006. “Organic Light-Emitting Diode (OLED) Technology: Materials, Devices and Display Technologies.” Polymer International 55 (6): 572–82.\n\n\nHong, HyungKi, HyunHo Shin, and InJae Chung. 2007. “In-Plane Switching Technology for Liquid Crystal Display Television.” Journal of Display Technology 3 (4): 361–70.\n\n\nHuang, Yuge, En-Lin Hsiang, Ming-Yang Deng, and Shin-Tson Wu. 2020. “Mini-LED, Micro-LED and OLED Displays: Present Status and Future Perspectives.” Light: Science & Applications 9 (1): 105.\n\n\nICDM. 2025. Information Display Measurement Standard, v 1.3. Society for Information Display.\n\n\nJeon, Eun Jeong, Anoop Kumar Srivastava, Miyoung Kim, Kwang-Un Jeong, Jeongmin Choi, Gi-Dong Lee, and Seung Hee Lee. 2009. “Temperature Dependence of the Electro-Optic Characteristics in the Liquid Crystal Display Switching Modes.” Journal of Information Display 10 (4): 175–79.\n\n\nLabGuy’s World. 2014. “Goldmark 1 - A Field Sequential Color TV Project.” https://labguysworld.com/Goldmark1_001.htm.\n\n\nLee, SH, SL Lee, and HY Kim. 1998. “Electro-Optic Characteristics and Switching Principle of a Nematic Liquid Crystal Cell Controlled by Fringe-Field Switching.” Applied Physics Letters 73 (20): 2881–83.\n\n\nMashford, Benjamin S, Matthew Stevenson, Zoran Popovic, Charles Hamilton, Zhaoqun Zhou, Craig Breen, Jonathan Steckel, et al. 2013. “High-Efficiency Quantum-Dot Light-Emitting Devices with Enhanced Charge Injection.” Nature Photonics 7 (5): 407–12.\n\n\nMiller, Michael E. 2019. Color in Electronic Display Systems. Springer.\n\n\nQian, Lei, Ying Zheng, Jiangeng Xue, and Paul H Holloway. 2011. “Stable and Efficient Quantum-Dot Light-Emitting Diodes Based on Solution-Processed Multilayer Structures.” Nature Photonics 5 (9): 543–48.\n\n\nSchubert, E Fred. 2006. Light-Emitting Diodes. 2nd ed. Cambridge University Press.\n\n\nShirasaki, Yasuhiro, Geoffrey J Supran, Moungi G Bawendi, and Vladimir Bulović. 2013. “Emergence of Colloidal Quantum-Dot Light-Emitting Technologies.” Nature Photonics 7 (1): 13–23.\n\n\nShockley, William. 1949. “The Theory of p-n Junctions in Semiconductors and p-n Junction Transistors.” Bell System Technical Journal 28 (3): 435–89.\n\n\nThomson, Stuart. 2018. “Determination of Chromaticity Coordinates and Bandgaps of III-V LEDs Using Electroluminescence Spectroscopy.” https://www.edinst.com/wp-content/uploads/2018/08/Electroluminescence-spectroscopy-of-LEDs.pdf.\n\n\nTrussell, H Joel, and Michael J Vrhel. 2008. Fundamentals of Digital Imaging. Cambridge University Press.\n\n\nTsujimura, Takatoshi. 2017. OLED Display Fundamentals and Applications. 2nd ed. John Wiley & Sons.",
    "crumbs": [
      "Display",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Optical Mechanisms</span>"
    ]
  },
  {
    "objectID": "display-optics.html#footnotes",
    "href": "display-optics.html#footnotes",
    "title": "19  Optical Mechanisms",
    "section": "",
    "text": "2023 Nobel Prize in Chemistry is awarded to the discovery, characterization, and synthesis of quantum dots.↩︎",
    "crumbs": [
      "Display",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Optical Mechanisms</span>"
    ]
  },
  {
    "objectID": "display-electronics.html",
    "href": "display-electronics.html",
    "title": "20  Driving Circuits",
    "section": "",
    "text": "20.1 Backplane\nNow that we understand how displays produce color and control luminance, let’s take a step back and ask: since all we get from an image is array of pixels, how do we actually control the display color and luminance from image pixel values? This requires a combination of display signal processing algorithms that turn image pixel values to actual digital values sent to the display, which is the topic of next chapter, and the driving circuits that use the digital values to actually drive the emissive devices, which we focus on in this chapter.\nThere are two main components in the driving circuits: 1) the backplane that sits directly at the back of a display panel and that controls the emissive devices (Section 20.1), and 2) the driver integrated circuit (IC) that delivers the driver signals to the backplane (Section 20.2).\nMechanically, a display panel has an emissive layer at the front that contains the optical devices we have seen in the previous chapter and a backplane that sits right behind and delivers electrical signals to the emissive layer. The main component of the backplane is the driving circuit, which delivers electrical signals to drive the LEDs/LC cells. The driving circuit differs between LCDs and (O)LED displays, and can use either an active matrix (AM) architecture of a passive matrix (PM) architecture.",
    "crumbs": [
      "Display",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Driving Circuits</span>"
    ]
  },
  {
    "objectID": "display-electronics.html#sec-disp-bp",
    "href": "display-electronics.html#sec-disp-bp",
    "title": "20  Driving Circuits",
    "section": "",
    "text": "20.1.1 LCD vs. OLED\nAs far as driving is concerned, the main difference between an LED and an LC cell is that LED is a current-driven device and an LC cell is a voltage-driven device. An LED emits photons because of the injected current that flows through it. LC cells do not emit photons themselves — the backlight does. The LC cells change their optical properties (the ability to rotate the polarization of incident light) in response to external voltage, and there is very little current that flows through the LC cells.\nTherefore, to drive an LC cell, we need to maintain an external voltage. In contrast, to drive an LED, we need to maintain a flow of current, which will cause a voltage potential difference across the LED but that is the by-product of the injected current.\nFigure 20.1 compares the driving circuit between an LC cell and an OLED pixel. In the LC case, each pixel has an LC cell, a storage capacitor \\(C_{storage}\\), and a switching transistor, usually a thin-film transistor (TFT). When \\(V_{Select}\\) is activated, the TFT allows \\(C_{storage}\\) to be charged by \\(V_{Data}\\). The voltage stored in \\(C_{storage}\\) then drives the LC cell. In PAM, the voltage value is determined by the intended luminance based on the transmittance-vs-voltage curve (Section 19.3.1). In PWM, the voltage is fixed but duration at which \\(V_{Select}\\) is activated is luminance dependent.\n\n\n\n\n\n\nFigure 20.1: Comparison the driving circuit inside (a) an LCD pixel and (b) an OLED pixel. Both use the active matrix design, where there are in-pixel TFTs and capacitors to store data and control each pixel. Adapted from Ma (2016, fig. 1).\n\n\n\nThe in-pixel driving circuit for an OLED is slightly more complicated because of its current driven nature. As shown in Figure 20.1 (b), each pixel has two TFTs and a storage capacitor, hence the name 2T1C design. The switching TFT acts similarly to that in the LC case, allowing \\(C_{storage}\\) to be charged. The voltage across \\(C_{storage}\\) is then \\(V_{DD} - V_{Data}\\).\nTo deliver a current to the OLED, however, having only the voltage in \\(C_{storage}\\) is not enough; we need another TFT, the driving TFT. The driving TFT (like any transistor) has three terminals: the source terminal is connected to one side of the capacitor and \\(V_{DD}\\), the gate terminal is connected to the other side of the capacitor, and the drain terminal is connected to the OLED1. The gate-source voltage of the TFT, \\(V_{gs}\\), is equivalent to the voltage across \\(C_{storage}\\). Given \\(V_{gs}\\), the current that flows through the drain terminal \\(I_d\\), which is also the current injected to the OLED \\(I_{OLED}\\) (because of Kirchhoff’s first law), is given by (Ma 2016, p. 1829):\n\\[\n\\begin{aligned}\n    I_{OLED} = I_d = k(V_{gs} - V_{th})^2, \\\\\n    V_{gs} = V_{DD} - V_{Data},\n\\end{aligned}\n\\tag{20.1}\\]\nwhere \\(V_{th}\\) is the threshold voltage of the TFT and k is a constant that depends on inherent properties of the transistor. \\(V_{Data}\\) is properly set so that the resulting \\(I_{OLED}\\) gives us the desired luminance (according to Equation 19.1).\nWe can see that we do not directly control the voltage across the OLED. As the current \\(I_d\\) flows through the OLED, there is naturally a voltage difference across the OLED (given by Equation 19.2), which is also the voltage at the drain terminal (since the other side of the OLED is connected to ground). The only thing we have to make sure of is \\(V_{ds} \\geq V_{gs} - V_{th}\\) (where \\(V_{ds}\\) is the voltage across the drain and source terminals) so that the TFT operates in the saturation region where Equation 20.1 holds.\nGiven the driving circuit of individual pixels, it is natural to extend it to drive an array of pixels, shown in Figure 20.2, which uses OLEDs as an example. The OLED pixels are organized similar to image sensor pixels and memory cells in a memory array. Each time only one row of pixels is activated (through \\(V_{Select}\\)). Each row has a dedicated \\(V_{Data}\\) signal, which delivers the necessary voltage to the corresponding pixel.\n\n\n\n\n\n\nFigure 20.2: An array of active matrix OLED pixels and the driving circuit. Each pixel uses the 2T1C design in Figure 20.1 (b).\n\n\n\nIf a display’s refresh rate is, say, 120 Hz, each row is selected 120 times a second. Each time, a new voltage is effectively programmed into the storage capacitor. Critically, even when a row is not selected, the charges in \\(C_{storage}\\) are still there and can continuously drive the LED. Driving circuit that has in-pixel control and storage uses the so-called “active matrix” addressing scheme. You might have heard of AMOLED, which is essentially OLED displays that use the AM driving circuit.\n\n\n20.1.2 Active Matrix vs. Passive Matrix\nThe driving circuit we have seen above uses the active matrix design. In the passive matrix (PM) design, there is no in-pixel TFTs or storage capacitor (Blankenbach, Hudak, and Jentsch 2016). Figure 20.3 compares the PM and AM design.\n\n\n\n\n\n\nFigure 20.3: Comparing the passive matrix (a) and active matrix (b) driving architecture; from (Ma 2016, figs. 4.20, 4.41). The AM architecture has in-pixel control (TFTs) and storage (capacitor) that are absent in the PM design.\n\n\n\nLike the AM addressing scheme, in PM we still address pixels row by row. For LCDs, once a row is activated, we set the per-column voltage so that we get the proper voltage difference (\\(V_{row} - V_{col}\\)) to drive an LC cell. For OLED displays, we address a row of pixels by connecting the row signal to ground, and the column signal needs to act as a current source so as to deliver a current through the OLED.\nCritically, the pixel (whether LCD or OLED) is “off” whenever its row is not selected. Therefore, for the most part of a refresh cycle the pixels are off. As a result, we need to deliver a large voltage or current during the “on” period in order to get a desired luminance level. This increases the power consumption and reduces the device life time.",
    "crumbs": [
      "Display",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Driving Circuits</span>"
    ]
  },
  {
    "objectID": "display-electronics.html#sec-disp-dic",
    "href": "display-electronics.html#sec-disp-dic",
    "title": "20  Driving Circuits",
    "section": "20.2 Driver IC",
    "text": "20.2 Driver IC\nThe backplane itself is more or less just the matrix shown in Figure 20.2, whose row and column signals (\\(V_{Select}\\) and \\(V_{Data}\\), or scan and data drivers in Figure 20.3) need to be set externally. These driver signals come from an external driver IC. Blankenbach (2016) and Cristaldi, Pennisi, and Pulvirenti (2009, vol. 2, chap. 6) provide general descriptions of LCD driver ICs up until 10 years ago. Today’s driver ICs are more integrated and have more advanced features, but the general functionalities and principles remain.\n\n\n\n\n\n\nFigure 20.4: Two driver ICs: (a) Raydium’s RM68090 driving LCDs (Raydium (2011, p. 13)) and (b) OmniVision’s OD6631 driving AMOLED displays (OmniVision (n.d.)).\n\n\n\nFigure 20.4 shows two examples of driver ICs; one is Raydium’s RM68090 meant to drive LCDs and the other is OmniVision’s OD6631 meant to drive AMOLED displays. Perhaps the most important function of a display driver IC is to set the row (gate) and the column (source) driver signals. In the case of RM68090, it interfaces with a display backplane that has 320 rows, each of which has 720 columns2. Therefore, RM68090 has 320 gate driver signals (G[320:1]) and 720 source driver signals (S[720:1]). Only one gate driver signal is ON at a time, since only one row is selected at a time, so the gate driver signals effectively are a 320-stage shifter register. Assuming the display refreshes at a rate of 60 Hz, the shift register must shift at a rate of 60 \\(\\times\\) 320 = 19.2 KHz.\nOD6631 interfaces with AMOLED displays that use the gate-in-panel (GIP) technology where the shift registers are inside the backplane. The AMOLED can have up to 2800 rows, but the driver IC delivers only 20 gate driver signals (GIP[19:0]) to the panel. This implies that each gate signal controls 140 rows. With a 144 Hz refresh rate, shift register inside the backplane has to operate a rate of 144 \\(\\times\\) 2800 = 403.2 KHz, but the gate signals from the driver IC only have to shift at 1/20 of that rate (20.16 KHz).\nAnother difference between gate and source driver signals is that the former is a binary signal controlling whether a row is selected whereas the latter needs to have many digital levels to control the pixel luminance. Therefore, there are Digital-to-Analog Convertors (DACs) before the source driver signals. Both the gate and source driver signals are generated from level shifters, because the input voltage of the driver IC is usually much lower (e.g., below 3.3 \\(\\text{V}\\)) than that of the gate/source drivers; for instance, the gate driver signal in RM68090 is above 10 \\(\\text{V}\\) in order to select a row.\nFor mobile devices like smartphones, the driver IC is typically connected to the Systems-on-a-Chip (SoC), which has a display controller IP block. The display controller uses the DMA engine to pass frame data from the memory to the MIPI Display Serial Interface (DSI) transmitter (Tx) block, which serializes the data (since MIPI DSI is a serial interface) and transmits the data to the MIPI DSI receiver (Rx) on the driver IC — this is how frame data is passed between the host and the driver IC.\nThe driver IC then needs to unpack the data to extract pixels. For mobile displays usually integrate the timing controller (TCON), whose main main job is to decide which row/column drivers get which pixels at which time. For large displays like TVs, the TCON usually sits inside the display itself and is a separate IC.\nMany driver ICs also include a small amount of built-in memory, or a frame buffer, which temporarily stores an entire frame of image data. This offloads the constant data transfer burden from the main processor, allowing it to send a new image to the driver IC only when a change is needed, while the driver handles the continuous task of refreshing the screen.\n\n\n\n\nBlankenbach, Karlheinz. 2016. “Active Matrix Driving.” In Handbook of Visual Display Technology, 2nd ed., 645–64. Springer.\n\n\nBlankenbach, Karlheinz, Andreas Hudak, and Michael Jentsch. 2016. “Direct Drive, Multiplex, and Passive Matrix.” In Handbook of Visual Display Technology, 2nd ed., 621–44. Springer.\n\n\nCristaldi, David JR, Salvatore Pennisi, and Francesco Pulvirenti. 2009. Liquid Crystal Display Drivers: Techniques and Circuits. Vol. 2. Springer.\n\n\nMa, Ruiqing. 2016. “Active Matrix for OLED Displays.” In Handbook of Visual Display Technology, 2nd ed., 1821–41. Springer.\n\n\nOmniVision. n.d. “OD6631 AMOLED Driver Brief.”\n\n\nRaydium. 2011. “RM68090 Data Sheet: Single Chip Driver with 262K color for 240RGBx320 a-Si TFT LCD.”\n\n\nTsujimura, Takatoshi. 2017. OLED Display Fundamentals and Applications. 2nd ed. John Wiley & Sons.",
    "crumbs": [
      "Display",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Driving Circuits</span>"
    ]
  },
  {
    "objectID": "display-electronics.html#footnotes",
    "href": "display-electronics.html#footnotes",
    "title": "20  Driving Circuits",
    "section": "",
    "text": "Other configurations are possible depending on whether we use an NMOS or a PMOS transistor and whether we use a source-follower circuit or a constant-current circuit (Tsujimura 2017, chap. 4.4.2). What we describe here uses PMOS + constant-current circuit.↩︎\n240 pixel columns and 3 sub-pixels per pixel column, so 720 effective columns↩︎",
    "crumbs": [
      "Display",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Driving Circuits</span>"
    ]
  },
  {
    "objectID": "display-signalprocessing.html",
    "href": "display-signalprocessing.html",
    "title": "21  Display Signal Processing",
    "section": "",
    "text": "21.1 The Big Picture\nFrom Chapter 20, we know that ultimately it is the \\(V_{Data}\\) signals that the display has to set in order to get a desired response on the pixels. How do set \\(V_{Data}\\)? This question can only be answered by positioning display in an end-to-end workflow that involves imaging, image processing, and display. We will first give a big-picture view, showing that the central task of this signal processing chain is called tone mapping, which is realized by a chain of signal processing steps (Section 21.1). We will then walk through this chain step by step (Section 21.2), and discuss practical issues in realizing tone mapping (Section 21.3). Finally, we will discuss color management, a framework that makes everything we discuss in this chapter much more consistent and reliable across software and hardware platforms (Section 21.4).\nConsider a typical workflow where you capture the scene as an image and then view it on a display. Figure 21.1 illustrates the chain of signal processing that takes place in this workflow. At the beginning of this chain is the luminance in the physical scene; at the end is the luminance emitted by the display. The transformation from the former to the latter can be abstracted as the Opto-Optical Transfer Function (OOTF), ① in Figure 21.1. In practice, the OOTF is indirectly realized through a long processing chain (① through ⑩ in Figure 21.1) that spans imaging hardware, image processing algorithms, and display hardware. Each step can be represented as a transfer function, and together these functions collectively constitute the OOTF.\nIdeally, the OOTF should be an identity function, which, one could argue, is the Holy Grail of an imaging-display workflow: faithfully capturing and reproducing the actual luminance in the scene. The former is dealt with by HDR imaging (Section 16.2.4), and the latter is the job of display signal processing and display hardware design.",
    "crumbs": [
      "Display",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Display Signal Processing</span>"
    ]
  },
  {
    "objectID": "display-signalprocessing.html#sec-disp-sp-ov",
    "href": "display-signalprocessing.html#sec-disp-sp-ov",
    "title": "21  Display Signal Processing",
    "section": "",
    "text": "Figure 21.1: In an end-to-end workflow, the mapping in the scene luminance to the display luminance is the effective OOTF of the system. This mapping, commonly known as tone mapping, is realized by cascading a sequence of processing stages involving the imaging system (②), image processing algorithms (⑤), and the display (⑩). Information is encoded and decoded when passing between stages (③, ④, ⑥, and ⑧). The image processing algorithms are what we have the most control over in the end-to-end workflow.\n\n\n\n\n\n21.1.1 Luminance Dynamic Range\nIt would be amazing if a display could accurately reproduce the scene luminance (assuming it is accurately captured). It is hardly possible for a variety of reasons. \n\n\nFirst, the peak display luminance of a display is usually lower than that of the real world.\nSecond, the real world has a much larger luminance dynamic range (DR) than that is afforded by the display. The luminance DR of the scene is the ratio between the maximum and minimum luminance in the scene, and the luminance DR of the display is the ratio between the maximum and minimum luminance producible by the display.\n\nThe definitions are concerned with luminance (a photometric metric) rather than illuminance (a radiometric metric) because we care about the perceived power not the radiant power in the scene.\nWe use “luminance DR” rather than simply DR to emphasize its difference from the DR of a sensor (Section 16.2.4), which is concerned with the ratio of peak measurable luminance in the scene to the noise floor. For simplicity we will use DR when it is clear what it refers to in a given context. Luminance DR is also often referred to as the contrast ratio while the sensor DR can be thought of as a form of signal-to-noise ratio (Mantiuk et al. 2015).\n\nThird, the luminance levels in a real-world scene are continuous whereas the luminance levels in digital displays are quantized (e.g., 256 levels in an 8-bit encoding), so there are quantization errors.\n\n\n\n\n\n\n\nFigure 21.2: Luminance dynamic range comparison between a real-world scene and various output devices. Adapted from Lang (2007, figs. 3,5).\n\n\n\nThe difference between the scene luminance range and that of various output devices is illustrated in Figure 21.2.\n\nThe DR of a real-world scene usually spans 4-5 log units.\nThe DR of a typical display is usually limited to about 3 log units, which is slightly higher than prints.\nModern high-dynamic-range (HDR) displays have luminance DRs that might match that of a scene, but the peak luminance still falls far short of that of what a real-world scene can produce1.\n\nTo enhance display DR, we not only need to be able to produce a high peak luminance but also a very low, ideally 0, luminance when the pixel value is 0. There are many industry standards/certifications for HDR displays, almost all of which include metrics such as minimum peak luminance, maximum black-level luminance, contrast ratio, and bit depth (VESA 2024).\n\n\n21.1.2 Tone Mapping\nGiven that it is unlikely that a display can fully reproduce the scene luminance, the next best question to ask is: how do we accurately reproduce the perceptual experience of the intended scene? To achieve this, the knob we have is the mapping of the intended luminance of each pixel in the image to a new luminance that is within the range afforded by the display (assuming of course the chromaticity is maintained throughout this mapping through appropriate color space transformations).\nThe mapping from scene luminance to displayed luminance is the OOTF in Figure 21.1, and is the central task of tone mapping. As noted earlier, the OOTF is not directly controlled; instead, it emerges from a cascade of transfer functions within the signal processing chain. Some of these transfer functions are determined by the design of the imaging and display hardware (② and ⑩), others by image encoding and decoding format (e.g., ③, ④ ⑥, ⑦), and the remainder by the image processing algorithm (⑤) abstracted as Electro-Electrical Transfer Function (EETF).\nEETF is the component over which we have the greatest (or sometimes the only) control and, thus, where tone mapping/OOTF can be practically influenced. For that reason, the entire EETF can also be thought of as a tone mapping operator (TMO).\nThe key to tone mapping is to preserve contrast. Recall that the human visual system has a contrast sensitivity function (Section 2.4.2), which tells us the minimal contrast necessary at each frequency for the pattern to be detectable. Let’s use a simple 1D example to explain how tone mapping might affect contrast and, consequently, perceptual quality.\nAssume that we have a 1D signal \\(y = A_0 + A\\sin(x)\\) (where \\(0 &lt; A &lt; A_0\\)) with only two frequencies, a 0 Hz mode with an amplitude of \\(A_0\\) and a 1 Hz mode with a amplitude of \\(A\\).\n\nThe luminance DR of the signal is \\([A_0 - A, A_0 + A]\\).\nThe Michelson contrast at 1 Hz is \\(\\frac{A}{A_0}\\).\n\nNow assume that we need to display this signal on a display with a luminance DR of \\([\\frac{A_0 - A}{k}, \\frac{A_0 + A}{k}]\\), where \\(k &gt; 1\\). Perhaps the easiest TMO would be to simply linearly scale the signal by a factor of \\(k\\) to \\(y' = \\frac{A_0}{k} + \\frac{A}{k}\\sin(x)\\). The Michelson contrast at 1 Hz for this new signal is still \\(\\frac{A}{A_0}\\), which seems to indicate that this simple linear-scaling TMO works well.\nThis TMO has two potential issues.\n\nFirst, while the scene luminance is continuous or is encoded with a high bit precision by the imaging system, the display could have a lower bit depth, leading to quantization errors. If the vast majority of the scene is limited to a relatively a narrow luminance range, the quantization errors would make the displayed image look “dull”.\nSecond, the average luminance is reduced by a factor of \\(k\\). The contrast sensitivity reduces as the mean background level reduces (Wandell 1995, fig. 5.26; Barten 2003, fig. 7; Ashraf et al. 2024, figs. 8, 9, 10). This means we could need to increase, not just maintain, the contrast to make up for the sensitivity loss.\n\nIn practice, another challenge in implementing a good TMO is that the EETF has limited visibility of the end-to-end workflow: it receives information only from ⑭ rather than the scene directly and sends information only to ⑯ rather than directly to the display. So any information loss before or after the EETF would hurt the tone mapping quality. We will refer you to Reinhard (2010) and Mantiuk et al. (2015) for surveys of tone mapping techniques. Section 21.3 discusses challenges and solutions of tone mapping in practice.",
    "crumbs": [
      "Display",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Display Signal Processing</span>"
    ]
  },
  {
    "objectID": "display-signalprocessing.html#sec-disp-sp-proc",
    "href": "display-signalprocessing.html#sec-disp-sp-proc",
    "title": "21  Display Signal Processing",
    "section": "21.2 The Chain of Processing",
    "text": "21.2 The Chain of Processing\nLet’s now walk through the chain of processing from luminance in the scene to the luminance emitted from a display.\n\n21.2.1 Hardware-Intrinsic OETF\nAn imaging system fundamentally performs a signal transduction from the optical domain to the electrical domain. This transduction can be abstracted as Equation 16.7, where the scene power is converted to RAW pixels. Equation 16.7 can be thought of as the intrinsic Opto-Electrical Transfer Function (OETF) of the imaging system (② in Figure 21.1). Barring noise and ADC quantization errors, the RAW pixel values are roughly proportional to the scene luminance. A RAW pixel value can be expressed as:\n\\[\n    P_{cam} =\n    \\begin{bmatrix}\n        hOETF_R(\\Phi_s(\\lambda)) \\\\\n        hOETF_G(\\Phi_s(\\lambda)) \\\\\n        hOETF_B(\\Phi_s(\\lambda))\n    \\end{bmatrix},\n\\tag{21.1}\\]\nwhere \\(P_{cam}\\) is the pixel color in the camera RAW space, \\(hOETF_R(\\cdot)\\), \\(hOETF_G(\\cdot)\\), and \\(hOETF_B(\\cdot)\\) represent the hardware-intrinsic OETF for the red, green, and blue channel, respectively, and \\(\\Phi_s(\\lambda)\\) represents the SPD of the incident light in the scene. We have three OETFs here because there are three different spectral sensitivity functions (Section 16.7.1). If using a 10-bit encoding, each channel in \\(P_{RAW}\\) is bounded between 0 and 1023. The \\(hEOTF\\)s are defined accordingly.\nIn a rendering system, the image pixel values are rendered/simulated rather than captured, but the same principle applies, where the rendered pixels should ideally be proportional to or, ideally, directly encode the absolute luminance information of the rendered scene. Two main differences exist between rendering and imaging. First, in rendering we generally we do not intentionally model sensor noise. Second, numerically solving the (volume) rendering equation leads to inaccuracies (Pharr, Jakob, and Humphreys 2018, chaps. 2, 13, 14), whereas the rendering equations are effectively “solved by nature” in imaging. Of course, we still pay the ADC quantization error in rendering. Therefore, the rendered pixels are usually not perfectly proportional to luminance.\n\n\n21.2.2 Reference OETF\nWhen savings RAW pixels as an image file (such as JPEG and PNG), we usually have a low bit budget. For instance, RAW pixels are typically encoded using 10 or 12 bits, but usual RGB images use 8 bits per color channel. Recall from Section 5.3.2 that when quantizing luminance-linear signals into digital values, we use a gamma-based encoding strategy, which attempts to encode the perceived brightness, rather than the physical luminance, uniformly. Gamma encoding makes better use of the limited bit budget by reducing the perceptual quantiztion errors for the low luminance range.\nThe encoding function \\(f_{L \\mapsto V}\\) that maps a luminance-linear signal \\(L\\) to a digital value \\(V\\) that is actually saved in an image file is also called the OETF (③ in Figure 21.1). This OETF, however, purely represents an encoding strategy, and is clearly different from the hardware-intrinsic OETF of the imaging system.\n\nThe hardware-intrinsic OETF represents an actual signal transduction, but the encoding OETF here purely manipulates information, both \\(L\\) and \\(V\\), in the electrical domain, except \\(L\\) represents luminance information and \\(V\\) represents a digital value.\nThe two need not be, and mostly will not be, the same. When people say OETF without any qualifier, what they refer to is the encoding OETF \\(f_{L \\mapsto V}\\), a convention we will follow. We will explicitly use hardware-intrinsic OETF when referring specifically to the transfer function intrinsic to the signal transduction process.\n\nAfter \\(f_{L \\mapsto V}\\), pixel values are roughly proportional to perceived brightness. A good OETF should be designed based on models of human brightness perception. Over the years, many reference OETFs have been defined in TV/broadcast standards, such as Rec. 601 (ITU-R 2011b), Rec. 709 (ITU-R 2015b), and, more recently, Rec. 2020 (ITU-R 2015a) and Rec. 2100 (ITU-R 2025)—all published by ITU-R2. In contrast, sRGB (Anderson et al. 1996; Stokes et al. 1996; IEC 1998) and Display P3 are color space standards (not defined by ITU-R). sRGB shares the same primaries and white point chromaticities as Rec. 709 but uses a different OETF. Display P3 offers a wider gamut than sRGB while using the same OETF.\nAll of the standards above, except Rec. 2100, use relative luminance as input, where L=1 is given by some form of maximum luminance measure manually determined for a particular setting and, therefore, usually does not correspond to a fixed, absolute luminance level. That maximum luminance could be, for instance, the absolute luminance that just saturates the sensor in the imaging system. In theory, though, the brightness-vs-luminance model should take absolute luminance into account. The sRGB standard does specify a recommended display luminance of 80 nits, but that is just a recommendation and nothing prevents you from displaying an sRGB on a dimmer or brighter display, in which case the brightness model underlying the OETF in sRGB technically would not apply.\nIn practice, the OETF is applied after a color space conversion (CSC) from the raw camera space to a standard color space such as sRGB or Display P3 (which we cover in Zhu (2022)), each of which specifies a reference OETF. The OETF is applied to each of the three color channels. Mathematically:\n\\[\n\\begin{aligned}\n    P_{XYZ} &= T_{cam\\_to\\_XYZ} \\times \\text{diag}^{-1}(1024) \\times P_{cam}, \\\\\n    P_{sRGB\\_linear} &= T_{XYZ\\_to\\_sRGB} \\times P_{XYZ}, \\\\\n    P_{sRGB} &= \\text{diag}(255) \\times\n    \\begin{bmatrix}\n        OETF(P_{sRGB\\_linear}(R))\\\\\n        OETF(P_{sRGB\\_linear}(G))\\\\\n        OETF(P_{sRGB\\_linear}(B))\n    \\end{bmatrix},\n\\end{aligned}\n\\tag{21.2}\\]\nwhere:\n\n\\(\\text{diag}^{-1}(1024) \\times P_{cam}\\) (\\(\\mathbb{R}^3 \\in [0, 1]^3\\)) is a color in the camera RAW space normalized to the [0, 1] range (assuming 10-bit RAW encoding).\n\\(T_{cam\\_to\\_XYZ}\\) is the transformation matrix from the normalized camera RAW space to the CIE 1931 XYZ space; it is usually illuminant dependent and normalized such that when the illuminant is normalized to have a Y value of 1, one of the RGB channels saturates (Rowlands 2020).\n\\(P_{XYZ}\\) is the color in the XYZ space.\n\\(T_{XYZ\\_to\\_sRGB}\\) is the transformation from the XYZ space to a color space, say sRGB, used to encode the image file; the matrix is usually normalized such that [1, 1, 1] in the linear sRGB space translates to Y=1.\n\\(P_{sRGB\\_linear}\\) (\\(\\mathbb{R}^3 \\in [0, 1]^3\\)) is the color in the linear sRGB space.\n\\(OETF(\\cdot)\\) is the OETF of the encoding space (the OETF for sRGB in this example is Equation 5.1).\n\\(P_{sRGB}\\) (\\(\\mathbb{Z}^3 \\in [0, 255]^3\\)) is the color in the sRGB space.\n\n\n\n21.2.3 EETF\nWhen an OETF-encoded image is later processed, we can use OETF-1 to recover the original luminance (④ in Figure 21.1). This is important because any further image processing should ideally be operating in the luminance-linear space, where operates correspond to physical units; forgetting this can lead to many subtle bugs in code (Chen, Chang, and Zhu 2024)!\nThe image processing pipeline can be abstracted as Electro-Electrical Transfer Function (EETF), as it processes digital pixels (⑤ in Figure 21.1). EETF is usually the part of the entire processing pipeline that we get to control, so it is where we can impact the overall tone mapping and OOTF. Mathematically: \\[\n\\begin{aligned}\n    P_{sRGB\\_linear} = OETF^{-1}(\\text{diag}^{-1}(255) \\times P_{sRGB}), \\\\\n    \\mathcal{M}_{sRGB \\rightarrow sRGB}: P_{sRGB\\_linear} \\mapsto P'_{sRGB\\_linear},\n\\end{aligned}\n\\tag{21.3}\\]\nwhere we first recover \\(P_{sRGB\\_linear}\\), the luminance-linear signals in the sRGB space, in which the tone mapping operator \\(\\mathcal{M}_{sRGB \\rightarrow sRGB}\\) operates in; the result \\(P'_{sRGB\\_linear}\\) is the tone-mapped pixel value in the luminance-linear space. Since tone mapping is primarily concerned with manipulating luminance information (assuming chromaticity remains unchanged), recovering luminance-linear signals first is important. If \\(\\mathcal{M}\\) depends only on the value of \\(P_{sRGB}\\), the TMO is a global operator. In contrast, local TMOs can apply different transformations to pixels that share the same color but appear at different spatial locations.\nEven though ideally we would want to manipulate absolute luminance, as discussed in Section 21.1, the tone mapping operator here has to work to relative luminance. This is because usually the processing stages before EETF do not keep the absolute luminance information because of all the normalizations. Section 21.3 discusses challenges facing implementing a good EETF and typical solutions.\nEquation 21.3 assumes that we are given a digital image to begin with. In reality, we can also implement tone mapping at the end of an rendering pipeline or a camera signal processing pipeline before a digital image is saved (Chapter 18). At this stage, we have direct access to analog or high-precision (e.g., 10 bits) luminane before it is quantized to a lower (e.g., 8) bit depth in an image.\n\n\n21.2.4 Reference EOTF\nAfter EETF, each image pixel is mapped to an intended (relative) luminance. Now comes the time to display the image. We have to again turn luminance back to digital values3. Minimizing perceptual quantization error is still the key, since these digital values will eventually be decoded back to luminance. This requires, again, modeling human brightness perception, but this time the luminance range is limited by what the display can afford to produce so the model would be somewhat different than that used to define OETF on the imaging side.\nWe need a function \\(f_{V \\mapsto L}\\) that maps a digital value \\(V\\) to a luminance \\(L\\). \\(f_{V \\mapsto L}\\) is called the Electro-Optical Transfer Function (EOTF). This is potentially confusing: why do we not construct the function to map luminance to digital value, like how we have done on the imaging side, but the other way around? Mathematically, this is somewhat a moot point because the function is constructed to be monotonic and, thus, invertible. In practice, we use EOTF, rather than OETF, on the display side simply to signify the fact that a display converts electrical signals to optical signals.\nOver the years, there have been a set of reference EOTFs defined in various standards. Rec. 1886 (ITU-R 2011a) is meant to give a good approximation of the hardware-intrinsic EOTF of CRT displays, and Rec. 2100 is meant to be used for HDR workflows, where absolute luminance is tracked. Rec. 709, Rec. 2100, sRGB, and Display P3 define both an OETF and an EOTF, which are inversions of each other. Note that OETF and EOTF need not be an inversion of each other. Both are designed with a good model of human brightness perception in mind. The difference in the underlying model is that at the imaging side the luminance is dictated by the scene whereas as the display side the luminance is dictated by the display hardware; the two do not match, which, in turn, impacts the EOTF and OETF design. Therefore, while we use OETF to encode scene luminance to a file (and OETF-1 to recover the scene luminance from the file), the display EOTF might not necessarily be OETF-1.\nGiven an intended luminance \\(L\\) we want to display, we use EOTF-1 to obtain the corresponding digital value \\(V\\) to be sent to the display (⑥ in Figure 21.1). This is usually carried out in a CSC. For example, if the input image is encoded in sRGB, the pixels remain in the sRGB space after applying the EETF. If the display operates in the Display P3 color space, a CSC must be performed from linear sRGB to linear Display P3, after which the P3 EOTF is applied to obtain the digital pixel values. These EOTF-encoded digital pixels will then be transmitted through the MIPI DSI interface to the driver IC, as discussed in Section 20.2. Mathematically, the sequence of processing is:\n\\[\n\\begin{aligned}\n    P_{XYZ} &= T^{-1}_{XYZ\\_to\\_sRGB} \\times P'_{sRGB\\_linear}, \\\\\n    P_{P3\\_linear} &= T_{XYZ\\_to\\_P3} \\times P_{XYZ}, \\\\\n    P_{P3} &= \\text{diag}(1024) \\times\n    \\begin{bmatrix}\n        EOTF^{-1}(P_{P3\\_linear}(R)) \\\\\n        EOTF^{-1}(P_{P3\\_linear}(G)) \\\\\n        EOTF^{-1}(P_{P3\\_linear}(B))\n    \\end{bmatrix},\n\\end{aligned}\n\\tag{21.4}\\]\nwhere:\n\n\\(T_{XYZ\\_to\\_P3}\\) is the transformation from the XYZ space to the linear Display P3 space; the matrix is usually normalized such that [1, 1, 1] in the linear P3 space translates to an XYZ value where Y=1.\n\\(P_{P3\\_linear}\\) is the color in the linear P3 space.\n\\(P_{P3}\\) (\\(\\mathbb{Z}^3 \\in [0, 1023]^3\\)) is the color in the P3 space, assuming 10-bit encoding.\n\nEven though the TMO in Equation 21.3 operates completely within the sRGB space, by cascading Equation 21.3 and Equation 21.4 we can see that an EETF-based TMO effectively maps pixels from the color space where the input image is encoded (\\(P_{sRGB}\\) here) to the color space where the tone-mapped image is to be displayed (\\(P_{P3}\\) here), so the TMO can also be thought of as:\n\\[\n    \\mathcal{M}_{sRGB \\rightarrow P3}: P_{sRGB\\_linear} \\mapsto P_{P3\\_linear},\n\\]\n\n\n21.2.5 Hardware-Intrinsic EOTF\nThe driver IC will then turn the P3-encoded pixel values to the DAC inputs. Can we directly use the former for the latter? Most likely not.\nTo see why, let’s assume that we are dealing with an AMOLED display; using Equation 20.1 and Equation 19.1, we know that to emit a particular power spectrum \\(\\Phi_d(\\lambda)\\) from the display, the following must hold:\n\\[\n    e\\frac{\\Phi_d(\\lambda)/(h f)}{\\eta(\\lambda)} = k(V_{DD} - V_{Data} - V_{th})^2,\n\\tag{21.5}\\]\nTherefore, the desired \\(V_{Data}\\) is given by4:\n\\[\n    V_{Data} = \\sqrt{e\\frac{P/(h f)}{\\eta k}}  + V_{th} - V_{DD}.\n\\tag{21.6}\\]\nWith a DAC, we can convert a digital value to an analog voltage. Using an ideal DAC transfer function, the digital value \\(D\\) to be sent to the DAC is then:\n\\[\n\\begin{aligned}\n    D = \\frac{V_{Data} - V_{min}}{\\Delta}, \\\\\n    \\Delta = \\frac{V_{max} - V_{min}}{2^N - 1},\n\\end{aligned}\n\\tag{21.7}\\]\nwhere \\([V_{min}, V_{max}]\\) is the DAC output range and \\(N\\) is the resolution.\nThe relationship between the digital value \\(D\\) and the emitted power spectrum \\(\\Phi_d(\\lambda)\\) or the corresponding luminance is what we call the display-intrinsic EOTF. From the theoretical analysis we can see that the relationship is non-linear. Figure 21.3 shows examples for four inorganic LEDs. In practice, the hardware-intrinsic EOTF is affeced by many factors (such as variation in manufacturing, the particular driving circuit design, etc.), and is usually measured offline rather than modeled analytically.\n\n\n\n\n\n\nFigure 21.3: Hardware-intrinsic EOTFs for four inorganic LEDs (R, G, B, and W). From Miller (2019, fig. 7.2).\n\n\n\nWe want to unequivocally differentiate between the display-intrinsic EOTF and the reference EOTF defined in a standard.\n\nThe former maps digital values sent to the DAC to the luminance emitted: it is an inherent property of the display hardware (both the driving circuits and the emissive devices) and represents an actual signal transduction. The latter is purely a theoretical construction that is meant for efficient and effective digital encoding (based on human brightness perception); it operates completely within the electrical domain, except the input \\(V\\) represents a digital value and the output \\(L\\) represents (relative) luminance.\nThe two EOTFs do not have to match and most definitely do not match5. When people say EOTF without any qualifier, they mean \\(f_{V \\mapsto L}\\). We will specifically use display-intrinsic EOTF to refer to the actual EOTF that maps DAC values to emitted luminance.\n\nGiven the display-intrinsic EOTF, converting from P3-encoded pixels to DAC inputs requires two steps.\n\nFirst, we use the reference EOTF (in this case part of the Display P3 standard) to decode the actual luminance intended to be displayed (⑦ in Figure 21.1).\nSecond, we perform a CSC from the Display P3 space to the display native space, after which we invert the display-intrinsic EOFT to obtain the digital value to send to the DACs (⑧ in Figure 21.1). This CSC is necessary because it is unlikely that the display primaries and white point exactly matches that of a color space standard (e.g., Display P3), but we can measure them offline and construct a transformation matrix from the P3 space to the display native.\n\nMathematically, this is:\n\\[\n\\begin{aligned}\n    P_{P3\\_linear} &= EOTF(\\text{diag}^{-1}(1024) \\times P_{P3}), \\\\\n    P_{disp} &= T_{P3\\_to\\_disp} \\times P_{P3\\_linear}, \\\\\n    D &=\n    \\begin{bmatrix}\n        hEOTF^{-1}_R(P_{disp}(R)) \\\\\n        hEOTF^{-1}_G(P_{disp}(G)) \\\\\n        hEOTF^{-1}_B(P_{disp}(B))\n    \\end{bmatrix}, \\\\\n    \\Phi_d(\\lambda) &= \\frac{k h \\eta(\\lambda) (V_{DD} - (D\\Delta + V_{min}) - V_{th})^2}{e\\lambda},\n\\end{aligned}\n\\tag{21.8}\\]\nwhere:\n\n\\(T_{P3\\_to\\_disp}\\) is the transformation matrix from the linear Display P3 space to the display native space.\n\\(P_{disp}\\) is the pixel color in the display native space.\n\\(D\\) is the DAC inputs, one for each channel since each sub-pixel might have a different hardware-intrinsic OETF.\n\\(\\Phi_d(\\lambda)\\) is the emission power spectrum (derived from Equation 21.5 and Equation 21.7).\n\nIn reality, we could calibrate, offline, three look-up tables (LUTs), each of which maps each digital level in a \\(P_{P3}\\) channel to a corresponding DAC input (⑨ in Figure 21.1). In this way, after going through the display-intrinsic EOTF (⑩ in Figure 21.1) the luminance emitted by the display matches the intended luminance. The LUTs can be constructed by, at each P3 digital level, repeatedly changing the DAC input and measure the actual emitted luminance from the display until it matches that of the indended luminance. If the LUT size of concern, we could measure just a few digital values and interpolate between them in hardware.\nCascading all the equations from Equation 21.1 to Equation 21.8, we can see that the TMO we implement as the EETF (Equation 21.3) eventually dictates the mapping from the scene SPD to the displayed SPD, so the ultimately TMO can be thought of as:\n\\[\n    \\mathcal{M}: \\int\\Phi_s(\\lambda)V(\\lambda)\\text{d}\\lambda \\mapsto \\int\\Phi_d(\\lambda)V(\\lambda)\\text{d}\\lambda,\n\\]\nwhere \\(V(\\lambda)\\) is the luminous efficiency function.",
    "crumbs": [
      "Display",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Display Signal Processing</span>"
    ]
  },
  {
    "objectID": "display-signalprocessing.html#sec-disp-sp-tm",
    "href": "display-signalprocessing.html#sec-disp-sp-tm",
    "title": "21  Display Signal Processing",
    "section": "21.3 Practical Tone Mapping",
    "text": "21.3 Practical Tone Mapping\nTone mapping ultimately controls the OOTF of the end-to-end system, so ideally a TMO should manipulate absolute luminance information. As seen above, however, TMOs are implemented as the EETF. While the EETF ultimately determines the OOTF, it has no access to the absolute luminance information.\n\nFor instance, if we are given an sRGB image to display, an image pixel [10, 20, 30] in the sRGB space tells us nothing about the absolute luminance of each channel.\nEven if an image is captured through an HDR imaging workflow and encoded in an HDR format, e.g., OpenEXR (ILM 2025), which has a very high bit depth (even allows for floating point numbers!), the absolute luminance information is usually still not encoded. Perhaps the only exception is when images are generated using physically-based spectral rendering, where spectral radiance information is tracked throughout the rendering pipeline.\nWorse, we do not always know the target display’s luminance range, which is ultimatelly what matters since that is where the image will be displayed! This often the case when tone mapping is done in an camera signal processing pipeline that is agnostic to the viewing display.\n\nAbsent absolute luminance information, the TMO has to operate in normalized, luminance-linear spaces, as we have seen throughout Section 21.2. Some guesswork and heuristics are involved when implementing a good TMO. For instance, if the input image is encoded using sRGB, one fair assumption to make is that the image is to be displayed on a display with a peak luminance of 80 nits, which is the recommended luminance in the sRGB standard (that is rarely followed!). The recent ITU-R Rec. 2100 standard (ITU-R 2025) defines a reference EOTF and OETF using absolute luminance (and specifies that the peak display luminance must be at least 1,000 nits). This allows us to directly control the output luminance within EETF, assuming we encode image using Rec. 2100 and the viewing display supports it.\n\nAnother way we can control the absolute luminance is through software that allow us to interactively adjust the EETF, such as the famous Curves tool in Photoshop and Lightroom. With these tools, even though we are not explicitly told of the display luminance range, the absolute output luminance information is directly seen on the display, enabling us to judge for ourselves whether the result is satisfactory.\nFigure 21.4 shows three such examples in Lightroom, each of which has a tonal adjustment curve that maps an input pixel value in the normalized, luminance-linear input range (x-axis) to an output pixel value in the same range and color space (y-axis). The curves here are effectively the TMO \\(f\\) in Equation 21.3.\n\n\n\n\n\n\nFigure 21.4: Three tone mapping examples, each with a corresponding tonal adjustment curve, I set using Lightroom on an iPhone 12 Pro. The original image is a 10-bit (demosaicked and color corrected) image captured by a Google Pixel phone, obtained from the HDR+ Burst Photography Dataset (Hasinoff et al. 2016).\n\n\n\nWith a simple linear mapping in the first example, the image looks quite dark and dull. This is because most of the input pixel values are quite low (judging from the color histogram at the top), so essentially most of the pixels are mapped to low output digital values. We can raise the brightness by raising the tonal curve, as done in the second example. That curve essentially increases the contrast ratio of the low-to-mid luminance pixels and compress the contrast ratio of the mid-to-high luminance pixels.\nIn my last example, I have raised the tonal curve so much that many input digital levels are mapped to the same, maximum output digital levels, as if those pixels were “saturated” during imaging. What this does is to give the low-to-mid luminance pixels an even larger contrast ratio, so the details look more vivid. Perhaps surprisingly, this intentional saturation does not actually lead to visible “over-exposure” in the final image. Why? Look at the color histogram at the top of the third example: only a small fraction pixels are actually saturated, even though a relative wide range of pixel pixel values are mappped to saturation.\nThe tonal adjustment curve is also a place for creative expression even if we are not concerned with tone mapping per se. Readers familiar with the Curve tool in Photoshop must be familiar with the notion of an “S-curve” or an “inverse S-curve” (if not, see these articles). The former essentially increases the contrast ratio between the highlights and shadows in an image and the latter does the opposite. These adjustments are meant to enhance the visual experience (e.g., increasing the contrast improves the visibility of some otherwise less detectable details) at the cost of technically changing the relative luminance information in the physical scene.\n\n\n\n\n\n\nFigure 21.5: Four tone mapping examples form (Chen and Hasinoff 2020). The last example uses a local TMO from the HDR+ pipeline in Google Pixel phones, whereas the first three use glocal TMOs.\n\n\n\nAs another example, Figure 21.5 shows four tone mapping examples and their associated tonal adjustment curves. The first three use glocal TMOs similar to the three examples in Figure 21.4. The last example uses a local TMO from the HDR+ pipeline in Google Pixel phones (Hasinoff et al. 2016). We can see it uses a local TMO because there is no single mapping function from an input pixel value to an output pixel value. Instead, the adjustment “curve” is actually a heatmap showing, for each input pixel, the output pixel distribution after tone mapping. The local TMO is realized by dividing an image into tiles and designing a curve for each tile.",
    "crumbs": [
      "Display",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Display Signal Processing</span>"
    ]
  },
  {
    "objectID": "display-signalprocessing.html#sec-disp-sp-cm",
    "href": "display-signalprocessing.html#sec-disp-sp-cm",
    "title": "21  Display Signal Processing",
    "section": "21.4 Color Management",
    "text": "21.4 Color Management\nThe signal processing pipeline discussed above needs to work well across different workflows that might involve wildly different capturing devices (e.g., cameras, scanners) and output devices (e.g., displays, printers), each of which could have very different hardware-intrinsic transfer functions. If a camera gives us an RGB image, how do we know which color space are the pixels encoded in? How do we know what OETF was used to encode the pixel values in this image? What if the color space that encodes the image is different from that of the display?\nThe central task underlying all these question is how to ensure consistent color reproduction throughout different workflows? This is the job of color management, which requires a collaboration between every single piece that touches color in the workflow. Giorgianni and Madden (2009) and Sharma (2018) are two excellent references for color management.\n\n21.4.1 Profiles\nThe central concept of color management is the notion of a profile. The most commonly used standard for color management profiles is defined by the International Color Consortium (ICC). First, an image file should ideally have metadata, stored in an ICC profile (International Color Consortium 2019; Sharma 2018, chap. 5), that tells us what color space its pixel colors are encoded in or, better, the transformation matrix from the image’s color space to a device-independent color space, say the CIE XYZ space (e.g., \\(T_{XYZ\\_to\\_sRGB}\\) in Equation 21.2). The profile also specifies the transfer function that converts between digital values and luminance-linear values (\\(OETF\\) in Equation 21.2). The ICC profile can be embedded in common image file formats such as JPEG by a camera or by an image processing software.\n\n\n\n\n\n\nFigure 21.6: Screenshots taken from the ColorSync Utility showing the primaries in the ICC profile of my LG display (a) and the primaries (b), EOTF (c), and transformation matrix (d) in the ICC profile of my MacBook Pro LCD. The MacBook ICC profile matches that of a Display P3 profile, and my LG display ICC profile does not appear to match that of any reference color space.\n\n\n\nSecond, the display also has an ICC profile that can be read by the Operating System (OS). The profile presents a reference mode or a “virtual display” to the software. Among other things, the profile specifies a color space (primaries and white point) of the virtual display or, equivalently, the transformation between that color space and the XYZ space (e.g., \\(T_{XYZ\\_to\\_P3}\\) in Equation 21.4) and the transfer function used to turn digital pixel values to luminance-linear signals (\\(EOTF\\) in Equation 21.8).\nFigure 21.6 shows two ICC profiles that I read using the built-in ColorSync Utility on my MacBook Pro, which has an internal LCD and is also connected to an external LG display. Figure 21.6 (a) and (b) show the primaries of the color spaces of the two ICC profiles, respectively. Figure 21.6 (c) shows the EOTF of the MacBook’s profile, which is the same as that used in sRGB and Display P3 color spaces. Figure 21.6 (d) shows the transformation matrix, of the MacBook profile, from the display color space to the XYZ space (while considering white point correction; see below). Comparing my MacBook’s ICC profile with the default Display P3 profile, evidently my MacBook’s display presents itself as a Display P3 display.\nThe information in a display ICC profile is most likely different from, and typically weaker than, that of the actual display. For instance, the gamut of the display native color space is greater than a particular reference color space like sRGB or Display P3: the emission spetra of the primary LEDs are material dependent and usually result in colors more saturated than the primary colors of a reference space. Presenting a reference display model allows the image processing software to know how the image pixels it produces will be interpreted by the display. Otherwise, imaging how challenging it would be to develop a, say, tone mapping algorithm, without knowing the color space that a target display supports or what EOTF will be applied to the pixel values. The display hardware itself will apply the proper transformation (⑨ in Figure 21.1) between the virtual display presented in the ICC profile to its internal, native space.\nICC profiles use the notion of Tone Response Curve (TRC) to refer to the EOTF, which is invertible and the inversion becomes the OETF. We can use the TRC from an image’s ICC profile to convert pixel values into luminance-linear signals. Equivalently, this can be viewed as the camera having used the TRC to encode luminance-linear signals into digital pixel values. Similarly, we can assume that the display will use the TRC in its own profile to turn digital pixels to luminance-linear signals, which mean we should use the inversion of the TRC to encode pixel values.\nIn photographic film, the TRC models the mapping from exposure (luminance-linear) to film density (Fuji 2005; Kodak 1998), so in this context, the TRC is technically an OETF. Perhaps for this reason, in many digital imaging and display contexts, TRC is often used to refer to the OETF rather than the EOTF (imatest, n.d.). Either usage is acceptable, provided it is clearly stated, since the function itself is invertible.\nFinally, the software that manipulates image content must correctly read and interpret the image profile and the display profile, and perform the necessary decoding, encoding, and transformations. When processing an image with, say, an sRGC ICC profile, the processing software would first transform the sRGB colors to the XYZ space, and then transform the colors in the XYZ to the display’s color space using the display ICC profile. The correct transfer functions are also read from the profiles and applied properly. You can see that the XYZ space here serves to connect the input color space and the output color space. ICC calls such a space a Profile Connection Space (PCS).\n\n\n21.4.2 White Point Correction\nDuring the color space transformation, we usually perform an additional transformation so that sRGB white becomes the white point in the display space. This is called white point correction (WPC), which is based on chromatic adaptation discussed in Section 6.3. This is to accommodate the fact that the viewer might be under a different viewing condition than the condition under which the photo was originally edited. The viewing condition could affect the actual appearance of a color, so we must account for this shift in viewing condition through chromatic adaptation.\nWPC is in principle similar to white balancing in camera signal processing and uses the same transformation mechanism, which we discuss in Zhu (2022). We also refer you to Rowlands (2020), which discusses the interaction between WPC/white balance and color correction of RAW camera space.\n\n\n21.4.3 Gamut Mapping\nA display might support a color space whose gamut is smaller than that of the image’s encoding space. For instance, the display might support only sRGB while the image is encoded in DCI-P3, so some of the P3 colors might not be accurately reproduced. That is, \\(P_{P3\\_linear}\\) in Equation 21.4 might be outside the [0, 1] bound. The best thing we can do is to approximate an out-of-gamut color with an in-gamut color to minimize the color error. This is called gamut mapping. Morovič (2008) and Glassner (1995, chap. 3.6) describe the basic algorithms, with the former being more recent and comprehensive.\nThe simplest strategy would be to simply clamp out-of-range values, so a color of [12, 200, 300] would become [12, 200, 255]. Clearly, other than being extremely simple to implement, this strategy would introduce large color reproduction errors. ICC has defined four rendering intents, each of which corresponds to a gamut mapping algorithm (vaguely worded, and the implementation detail might vary).\nFor instance, the Absolute rendering intent leaves all the in-gamut colors unchanged but maps the out-of-gamut colors to the boundary of the color gamut. The Perceptual rendering intent can be implemented by uniformly projecting all the colors to the white point so that all the colors are in-gamut. You can imagine that while this maintains the relative color appearance between colors (which the Absolute rendering intent fails at), but it would also change in-gamut colors that could have been accurately rendered!\n\n\n\n\n\nAnderson, Matthew, Ricardo Motta, Srinivasan Chandrasekar, and Michael Stokes. 1996. “Proposal for a Standard Default Color Space for the Internet—Srgb.” In Color and Imaging Conference, 4:238–45. Society of Imaging Science; Technology.\n\n\nAshraf, Maliha, Rafał K Mantiuk, Alexandre Chapiro, and Sophie Wuerger. 2024. “castleCSF—a Contrast Sensitivity Function of Color, Area, Spatiotemporal Frequency, Luminance and Eccentricity.” Journal of Vision 24 (4): 5–5.\n\n\nBarten, Peter GJ. 2003. “Formula for the Contrast Sensitivity of the Human Eye.” In Image Quality and System Performance, 5294:231–38. SPIE.\n\n\nChen and Hasinoff. 2020. “Live HDR+ and Dual Exposure Controls on Pixel 4 and 4a.” https://research.google/blog/live-hdr-and-dual-exposure-controls-on-pixel-4-and-4a/.\n\n\nChen, Ethan, Jiwon Chang, and Yuhao Zhu. 2024. “Coolerspace: A Language for Physically Correct and Computationally Efficient Color Programming.” Proceedings of the ACM on Programming Languages 8 (OOPSLA2): 846–75.\n\n\nFuji. 2005. “Fujifilm Professional Data Guide.” https://asset.fujifilm.com/www/us/files/2020-03/3ab271f46f8d71c7e4c91bcedb7de050/ProfessionalFilmDataGuide.pdf.\n\n\nGiorgianni, Edward J, and Thomas E Madden. 2009. Digital Color Management: Encoding Solutions. Vol. 13. John Wiley & Sons.\n\n\nGlassner, Andrew S. 1995. Principles of Digital Image Synthesis. Elsevier.\n\n\nHasinoff, Samuel W, Dillon Sharlet, Ryan Geiss, Andrew Adams, Jonathan T Barron, Florian Kainz, Jiawen Chen, and Marc Levoy. 2016. “Burst Photography for High Dynamic Range and Low-Light Imaging on Mobile Cameras.” ACM Transactions on Graphics (ToG) 35 (6): 1–12.\n\n\nIEC. 1998. “IEC/4WD 61966-2-1: Colour Measurement and Management in Multimedia Systems and Equipment - Part 2-1: Default RGB Colour Space - sRGB.” https://web.archive.org/web/20141225172302/http://www2.units.it/ipl/students_area/imm2/files/Colore1/sRGB.pdf.\n\n\nILM. 2025. “OpenEXR Specification, v 3.4.0.” https://openexr.com/en/latest/.\n\n\nimatest. n.d. “Gamma, Tonal Response Curve, and related concepts.” https://www.imatest.com/imaging/tonal-response-gamma/.\n\n\nInternational Color Consortium. 2019. “Specification ICC.2:2019 (Profile version 5.0.0 - iccMAX).” https://color.org/specification/ICC.2-2019.pdf.\n\n\nITU-R. 2011a. “Recommendation ITU-R BT.1886: Reference electro-optical transfer function for flat panel displays used in HDTV studio production.” https://www.itu.int/dms_pubrec/itu-r/rec/bt/R-REC-BT.1886-0-201103-I!!PDF-E.pdf.\n\n\n———. 2011b. “Recommendation ITU-R BT.601-7: Studio encoding parameters of digital television for standard 4:3 and wide-screen 16:9 aspect ratios.” https://www.itu.int/dms_pubrec/itu-r/rec/bt/r-rec-bt.601-7-201103-i!!pdf-e.pdf.\n\n\n———. 2015a. “Recommendation ITU-R BT.2020-2: Parameter values for ultra-high definition television systems for production and international programme exchange.” https://www.itu.int/dms_pubrec/itu-r/rec/bt/R-REC-BT.2020-2-201510-I!!PDF-E.pdf.\n\n\n———. 2015b. “Recommendation ITU-R BT.709-6: Parameter values for the HDTV standardsfor production and international programme exchange.” https://www.itu.int/dms_pubrec/itu-r/rec/bt/R-REC-BT.709-6-201506-I!!PDF-E.pdf.\n\n\n———. 2025. “Recommendation ITU-R BT.2100-3: Image parameter values for high dynamic range television for use in production and international programme exchange.” https://www.itu.int/dms_pubrec/itu-r/rec/bt/R-REC-BT.2100-3-202502-I!!PDF-E.pdf.\n\n\nKodak. 1998. “KODAK EKTACHROME 100 Professional Film.” https://125px.com/docs/unsorted/kodak/e27.pdf.\n\n\nLang, Karl. 2007. “Rendering the Print: The Art of Photography.” Adobe System Technical Paper.\n\n\nMantiuk, Rafał, Grzegorz Krawczyk, Dorota Zdrojewska, Radosław Mantiuk, Karol Myszkowski, and Hans-Peter Seidel. 2015. “High Dynamic Range Imaging.” In Wiley Encyclopedia of Electrical and Electronics Engineering. Wiley.\n\n\nMiller, Michael E. 2019. Color in Electronic Display Systems. Springer.\n\n\nMorovič, Ján. 2008. Color Gamut Mapping. 2nd ed. John Wiley & Sons.\n\n\nPharr, Matt, Wenzel Jakob, and Greg Humphreys. 2018. Physically Based Rendering: From Theory to Implementation. 3rd ed. MIT Press.\n\n\nReinhard, Erik. 2010. High Dynamic Range Imaging Acquisition, Display, and Image-Based Lighting. 2nd ed. Morgan Kaufmann Publishers.\n\n\nRowlands, D Andrew. 2020. “Color Conversion Matrices in Digital Cameras: A Tutorial.” Optical Engineering 59 (11): 110801–1.\n\n\nSharma, Abhay. 2018. Understanding Color Management. John Wiley & Sons.\n\n\nStokes, Anderson, Chandrasekar, and Motta. 1996. “A Standard Default Color Space for the Internet - sRGB.” https://www.w3.org/Graphics/Color/sRGB.\n\n\nVESA. 2024. VESA High-Performance Monitor and Display Compliance Test Specification. Video Electronics Standards Association.\n\n\nWandell, Brian A. 1995. Foundations of Vision. Sinauer Associates.\n\n\nZhu, Yuhao. 2022. “Exploring Camera Color Space and Color Correction.” https://horizon-lab.org/colorvis/camcolor.html.",
    "crumbs": [
      "Display",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Display Signal Processing</span>"
    ]
  },
  {
    "objectID": "display-signalprocessing.html#footnotes",
    "href": "display-signalprocessing.html#footnotes",
    "title": "21  Display Signal Processing",
    "section": "",
    "text": "Case in point: you have probably never really felt too uncomfortable staring at a display but starring at white paper under noon sunlight is excruciating.↩︎\nITU-R refers to the Radiocommunication Sector of the International Telecommunication Union; these standards are formally designated with names such as ITU-R BT.2100, commonly shortened to Rec. 2100.↩︎\nWe encode luminance into digital values because the raw luminance data are continuous and would require floating-point representation, which cannot be sent directly to the display. Encoding reduces bandwidth demands and ensures compatibility with nearly all existing interface protocols.↩︎\ngiven that \\(V_{gs} &gt; V_{th}\\) for the TFT to operate in the saturation region.↩︎\nexcept maybe in the CRT case where its EOTF-1 roughly matches human luminance-to-brightness perception.↩︎",
    "crumbs": [
      "Display",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Display Signal Processing</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Acdx. 2009. “CIE 1931 XYZ Color Matching Functions; CC BY-SA\n4.0.” https://commons.wikimedia.org/wiki/File:CIE_1931_XYZ_Color_Matching_Functions.svg.\n\n\nAdelson, Edward H, and John YA Wang. 1992. “Single Lens Stereo\nwith a Plenoptic Camera.” IEEE Transactions on Pattern\nAnalysis and Machine Intelligence 14 (2): 99–106.\n\n\nAgarwal, Kushagra, Himanshu Rai, and Sandip Mondal. 2023. “Quantum\nDots: An Overview of Synthesis, Properties, and Applications.”\nMaterials Research Express 10 (6): 062001.\n\n\nAguilar, M, and WS Stiles. 1954. “Saturation of the Rod Mechanism\nof the Retina at High Levels of Stimulation.” Optica Acta:\nInternational Journal of Optics 1 (1): 59–65.\n\n\najay_suresh. 2021. “iPhone 12 cameras; CC\nBY-SA 2.0 license.” https://commons.wikimedia.org/wiki/File:Apple_iPhone_12_Pro_-_Cameras_(50535314721).jpg.\n\n\nAkahane, Nana, Shigetoshi Sugawa, Satoru Adachi, Kazuya Mori, Toshiyuki\nIshiuchi, and Koichi Mizobuchi. 2006. “A Sensitivity and Linearity\nImprovement of a 100-dB Dynamic Range CMOS Image Sensor Using a Lateral\nOverflow Integration Capacitor.” IEEE Journal of Solid-State\nCircuits 41 (4): 851–58.\n\n\nAllen, John. 2017. “Application of Patterned Illumination Using a\nDMD for Optogenetic Control of Signaling.” Nature\nMethods 1.\n\n\nAlpern, M, K Kitahara, and DH Krantz. 1983. “Perception of Colour\nin Unilateral Tritanopia.” The Journal of Physiology 335\n(1): 683–97.\n\n\nAnderson, Matthew, Ricardo Motta, Srinivasan Chandrasekar, and Michael\nStokes. 1996. “Proposal for a Standard Default Color Space for the\nInternet—Srgb.” In Color and Imaging Conference,\n4:238–45. Society of Imaging Science; Technology.\n\n\nAngueyra-Aristizábal, Juan M. 2014. “The Limits Imposed in Primate\nVision by Transduction in Cone Photoreceptors.” PhD thesis,\nUniversity of Washington Libraries.\n\n\nAnoneditor. 2007. “Illustration of the Foveon\nX3 sensor; CC BY-SA 3.0.” https://commons.wikimedia.org/wiki/File:Absorption-X3.png.\n\n\nAnonymous. 2009. “MacAdam Ellipses in the\nCIE1931 xy chromaticity diagram; CC BY-SA 3.0 license.” https://commons.wikimedia.org/wiki/File:CIExy1931_MacAdam.png.\n\n\nAntipa, Nick, Grace Kuo, Reinhard Heckel, Ben Mildenhall, Emrah Bostan,\nRen Ng, and Laura Waller. 2017. “DiffuserCam: Lensless\nSingle-Exposure 3D Imaging.” Optica 5 (1): 1–9.\n\n\nAoki, Masakazu, Haruhisa Ando, Shinya Ohba, Iwao Takemoto, Shusaku\nNagahara, Toshio Nakano, Masaharu Kubo, and Tsutomu Fujita. 1982.\n“2/3-Inch Format MOS Single-Chip Color Imager.” IEEE\nTransactions on Electron Devices 29 (4): 745–50.\n\n\nAPN MJM. 2011. “A calcite crystal displays\nthe double refractive properties while sitting on a sheet of graph\npaper; CC BY-SA 3.0 license.” https://commons.wikimedia.org/wiki/File:Crystal_on_graph_paper.jpg.\n\n\nArno / Coen. 2006. “Thermogram of a snake\nwrapped around a human arm; CC BY-SA 3.0 license.” https://commons.wikimedia.org/wiki/File:Wiki_stranglesnake.jpg.\n\n\nAshraf, Maliha, Rafał K Mantiuk, Alexandre Chapiro, and Sophie Wuerger.\n2024. “castleCSF—a Contrast Sensitivity Function of Color, Area,\nSpatiotemporal Frequency, Luminance and Eccentricity.”\nJournal of Vision 24 (4): 5–5.\n\n\nASTM International. 2013. “ASTM E284-13b\nStandard Terminology of Appearance.” https://webstore.ansi.org/standards/astm/astme28413b.\n\n\nAxel Jacobs. 2006. “Open window with armchair\nand manequin. Sample scene for HDRI (standard LDR, single image from a\nset of bracketed exposures); CC BY-SA 2.0 license.” https://commons.wikimedia.org/wiki/File:HDRI_Sample_Scene_Window_-_08.jpg.\n\n\nBarlow, Horace B. 1953. “Summation and Inhibition in the Frog’s\nRetina.” The Journal of Physiology 119 (1): 69.\n\n\n———. 1956. “Retinal Noise and Absolute Threshold.”\nJosa 46 (8): 634–39.\n\n\n———. 1957. “Increment Thresholds at Low Intensities Considered as\nSignal/Noise Discriminations.” The Journal of Physiology\n136 (3): 469.\n\n\n———. 1972. “Dark and Light Adaptation: Psychophysics.” In\nVisual Psychophysics, edited by Matthew Alpern, 1–28. Springer.\n\n\nBarlow, Horace B, Roo Fitzhugh, and SW Kuffler. 1957. “Change of\nOrganization in the Receptive Fields of the Cat’s Retina During Dark\nAdaptation.” The Journal of Physiology 137 (3): 338.\n\n\nBarten, Peter GJ. 2003. “Formula for the Contrast Sensitivity of\nthe Human Eye.” In Image Quality and System Performance,\n5294:231–38. SPIE.\n\n\nBass, Michael, Casimer DeCusatis, Jay Enoch, Vasudevan Lakshminarayanan,\nGuifang Li, Carolyn Macdonald, Virendra Mahajan, and Eric Van Stryland.\n2009. Handbook of Optics, Volume III: Vision and Vision Optics\n(Set). McGraw-Hill, Inc.\n\n\nBäuml, Karl-Heinz. 1993. “A Ratio Principle for a Red/Green and a\nYellow/Blue Channel?” Perception & Psychophysics 53\n(3): 338–44.\n\n\nBäuml, Karl-Heinz, and Brian A Wandell. 1996. “Color Appearance of\nMixture Gratings.” Vision Research 36 (18): 2849–64.\n\n\nBayer, Bryce E. 1976. “Color Imaging Array.”\n\n\nBaylor, Denis A. 1987. “Photoreceptor Signals and Vision: The\nProctor Lecture.” Investigative Ophthalmology & Visual\nScience 28 (1): 34–49.\n\n\nBaylor, Denis A, and MGF Fuortes. 1970. “Electrical Responses of\nSingle Cones in the Retina of the Turtle.” The Journal of\nPhysiology 207 (1): 77–92.\n\n\nBaylor, Denis A, and Alan L Hodgkin. 1973. “Detection and\nResolution of Visual Stimuli by Turtle Photoreceptors.” The\nJournal of Physiology 234 (1): 163–98.\n\n\nBaylor, Denis A, AL Hodgkin, and TD Lamb. 1974. “The Electrical\nResponse of Turtle Cones to Flashes and Steps of Light.” The\nJournal of Physiology 242 (3): 685–727.\n\n\nBaylor, Denis A, TD Lamb, and KW Yau. 1979a. “The Membrane Current\nof Single Rod Outer Segments.” The Journal of Physiology\n288 (1): 589–611.\n\n\nBaylor, Denis A, Trevor D Lamb, and KW Yau. 1979b. “Responses of\nRetinal Rods to Single Photons.” The Journal of\nPhysiology 288 (1): 613–34.\n\n\nBaylor, Denis A, G Matthews, and KW Yau. 1980. “Two Components of\nElectrical Dark Noise in Toad Retinal Rod Outer Segments.”\nThe Journal of Physiology 309 (1): 591–621.\n\n\nBaylor, Denis A, BJ Nunn, and JL Schnapf. 1984. “The Photocurrent,\nNoise and Spectral Sensitivity of Rods of the Monkey Macaca\nFascicularis.” The Journal of Physiology 357 (1):\n575–607.\n\n\n———. 1987. “Spectral Sensitivity of Cones of the Monkey Macaca\nFascicularis.” The Journal of Physiology 390 (1):\n145–60.\n\n\nBenFrantzDale. 2010a. “Effect of aperture on\nblur and DOF; CC BY-SA 3.0 license.” https://commons.wikimedia.org/wiki/File:Depth_of_field_illustration.svg.\n\n\n———. 2010b. “Ray diagram showing field\ncurvaure; CC BY-SA 3.0 license.” https://commons.wikimedia.org/wiki/File:Field_curvature.svg.\n\n\nBenRG. 2009. “CIE1931 xy chromaticity plot;\nreleased into the public domain by the copyright holder.”\nhttps://commons.wikimedia.org/wiki/File:CIE1931xy_blank.svg.\n\n\nBergen, James R, and Edward H Adelson. 1991. “The Plenoptic\nFunction and the Elements of Early Vision.” Computational\nModels of Visual Processing 1 (8): 3.\n\n\nBerson, David M, Felice A Dunn, and Motoharu Takao. 2002.\n“Phototransduction by Retinal Ganglion Cells That Set the\nCircadian Clock.” Science 295 (5557): 1070–73.\n\n\nBhandari, Ayush, Achuta Kadambi, and Ramesh Raskar. 2022.\nComputational Imaging. MIT Press.\n\n\nBharath, Anil. 2009. Introductory Medical Imaging. Morgan &\nClaypool.\n\n\nBhutajata. 2015. “Color temperature black\nbody; CC BY-SA 4.0 license.” https://commons.wikimedia.org/wiki/File:Color_temperature_black_body_800-12200K.svg.\n\n\nBirch, Jennifer. 2012. “Worldwide Prevalence of Red-Green Color\nDeficiency.” JOSA A 29 (3): 313–20.\n\n\nBiretta, John A, and Matt McMaster. 2008. Wide Field and Planetary\nCamera 2 Instrument Handbook v. 10.0. Space Telescope Science\nInstitute.\n\n\nBlakemore, CB, and WA Rushton. 1965. “Dark Adaptation and\nIncrement Threshold in a Rod Monochromat.” The Journal of\nPhysiology 181 (3): 612.\n\n\nBlankenbach, Karlheinz. 2016. “Active Matrix Driving.” In\nHandbook of Visual Display Technology, 2nd ed., 645–64.\nSpringer.\n\n\nBlankenbach, Karlheinz, Andreas Hudak, and Michael Jentsch. 2016.\n“Direct Drive, Multiplex, and Passive Matrix.” In\nHandbook of Visual Display Technology, 2nd ed., 621–44.\nSpringer.\n\n\nBlinn, James F. 1982. “Light Reflection Functions for Simulation\nof Clouds and Dusty Surfaces.” Acm SIGGRAPH Computer\nGraphics 16 (3): 21–29.\n\n\nBlume and Garbazza and Spitschan. 2019. “Schematic overview of photorecetors; CC BY-SA 4.0\nlicense.” https://commons.wikimedia.org/wiki/File:Overview_of_the_retina_photoreceptors_(a).png.\n\n\nBoas, David A, Dana H Brooks, Eric L Miller, Charles A DiMarzio, Misha\nKilmer, Richard J Gaudette, and Quan Zhang. 2001. “Imaging the\nBody with Diffuse Optical Tomography.” IEEE Signal Processing\nMagazine 18 (6): 57–75.\n\n\nBob Mellish. 2006. “Chromatic aberration\ndiagram; CC BY-SA 3.0 license.” https://commons.wikimedia.org/wiki/File:Chromatic_aberration_lens_diagram.svg.\n\n\nBoettner, Edward A, and J Reimer Wolter. 1962. “Transmission of\nthe Ocular Media.” Investigative Ophthalmology & Visual\nScience 1 (6): 776–83.\n\n\nBohren, Craig F, and Eugene E Clothiaux. 2006. Fundamentals of\nAtmospheric Radiation: An Introduction with 400 Problems. John\nWiley & Sons.\n\n\nBong, Kyeongryeol, Sungpill Choi, Changhyeon Kim, Donghyeon Han, and\nHoi-Jun Yoo. 2017. “A Low-Power Convolutional Neural Network Face\nRecognition Processor and a CIS Integrated with Always-on Face\nDetector.” IEEE Journal of Solid-State Circuits 53 (1):\n115–23.\n\n\nBong, Kyeongryeol, Sungpill Choi, Changhyeon Kim, Sanghoon Kang,\nYouchang Kim, and Hoi-Jun Yoo. 2017. “14.6 a 0.62 mW\nUltra-Low-Power Convolutional-Neural-Network Face-Recognition Processor\nand a CIS Integrated with Always-on Haar-Like Face Detector.” In\n2017 IEEE International Solid-State Circuits Conference\n(ISSCC), 248–49. IEEE.\n\n\nBoukhayma, Assim. 2018. Ultra Low-Noise CMOS Image Sensors.\nSpringer.\n\n\nBowmaker, James K. 1998. “Evolution of Colour Vision in\nVertebrates.” Eye 12 (3): 541–47.\n\n\n———. 2008. “Evolution of Vertebrate Visual Pigments.”\nVision Research 48 (20): 2022–41.\n\n\nBowmaker, James K, and HJk Dartnall. 1980. “Visual Pigments of\nRods and Cones in a Human Retina.” The Journal of\nPhysiology 298 (1): 501–11.\n\n\nBowmaker, JK. 1984. “Microspectrophotometry of Vertebrate\nPhotoreceptors: A Brief Review.” Vision Research 24\n(11): 1641–50.\n\n\nBowmaker, JK, HJ Dartnall, JN Lythgoe, and JD Mollon. 1978. “The\nVisual Pigments of Rods and Cones in the Rhesus Monkey, Macaca\nMulatta.” The Journal of Physiology 274 (1): 329–48.\n\n\nBoyle, Willard S, and George E Smith. 1970. “Charge Coupled\nSemiconductor Devices.” Bell System Technical Journal 49\n(4): 587–93.\n\n\nBrainard, DH. 1996. “Cone Contrast and Opponent Modulation Color\nSpaces.” In Human Color Vision, 2nd ed., 563–79. Optical\nSociety of America.\n\n\nBrayLockBoy. 2018. “An example of the Rolling\nshutter effect in action at Afton Down, Isle of Wight, taken by a camera\non a car travelling at approximately 50 miles per hour. CC BY-SA 4.0\nlicense.” https://commons.wikimedia.org/wiki/File:Rolling_Shutter_Effect_at_Afton_Down,_21_August_2018.jpg.\n\n\nBrettel, Hans, Françoise Viénot, and John D Mollon. 1997.\n“Computerized Simulation of Color Appearance for\nDichromats.” Josa a 14 (10): 2647–55.\n\n\nBriggs, Farran. 2020. “Role of Feedback Connections in Central\nVisual Processing.” Annual Review of Vision Science 6\n(1): 313–34.\n\n\nBrill, Michael H. 1998. “Erratum: How the CIE 1931 Color-Matching\nFunctions Were Derived from Wright-Guild Data.” Color\nResearch & Application 23 (4): 259–59.\n\n\nBroadbent, Arthur D. 2004. “A Critical Review of the Development\nof the CIE1931 RGB Color-Matching Functions.” Color Research\n& Application 29 (4): 267–72.\n\n\n———. 2008. “Calculation from the Original Experimental Data of the\nCIE 1931 RGB Standard Observer Spectral Chromaticity Coordinates and\nColor Matching Functions.” Québec, Canada:\nDépartement de génie Chimique,\nUniversité de Sherbrooke, 1–17.\n\n\nBrown, Paul K, and George Wald. 1964. “Visual Pigments in Single\nRods and Cones of the Human Retina.” Science 144 (3614):\n45–52.\n\n\nBruce MacEvoy. 2015. “The material attributes\nof paints.” https://www.handprint.com/HP/WCL/pigmt3.html#particlesize.\n\n\nBurns, Marie E, and Vadim Y Arshavsky. 2005. “Beyond Counting\nPhotons: Trials and Trends in Vertebrate Visual Transduction.”\nNeuron 48 (3): 387–401.\n\n\nBurns, Marie E, and Denis A Baylor. 2001. “Activation,\nDeactivation, and Adaptation in Vertebrate Photoreceptor Cells.”\nAnnual Review of Neuroscience 24 (1): 779–805.\n\n\nBurns, Marie E, and Trevor D Lamb. 2014. “Visual Transduction by\nRod and Cone Photoreceptors.” In The New Visual\nNeurosciences. MIT Press.\n\n\nCaerbannog. 2016. “Comparison of structures\nin vertebrate’s eye (left) with octopus’ eye (right); CC BY-SA 3.0\nlicense.” https://en.wikipedia.org/wiki/Blind_spot_(vision)#/media/File:Evolution_eye_2.svg.\n\n\nCanon. 2006. EF Lens Work III: The Eye of EOS. 8th ed. Canon\nInc. Lens Products Group.\n\n\nCburnett. 2006. “A Bayer pattern on a sensor;\nCC BY-SA 3.0.” https://commons.wikimedia.org/wiki/File:Bayer_pattern_on_sensor.svg.\n\n\nChandrasekhar, Subrahmanyan. 1960. Radiative Transfer. Courier\nCorporation.\n\n\nChen and Hasinoff. 2020. “Live HDR+ and Dual\nExposure Controls on Pixel 4 and 4a.” https://research.google/blog/live-hdr-and-dual-exposure-controls-on-pixel-4-and-4a/.\n\n\nChen, Cheng, Ziwen Wang, Jiajing Wu, Zhengtao Deng, Tao Zhang, Zhongmin\nZhu, Yifei Jin, et al. 2023. “Bioinspired, Vertically Stacked, and\nPerovskite Nanocrystal–Enhanced CMOS Imaging Sensors for Resolving UV\nSpectral Signatures.” Science Advances 9 (44): eadk3860.\n\n\nChen, Ethan, Jiwon Chang, and Yuhao Zhu. 2024. “Coolerspace: A\nLanguage for Physically Correct and Computationally Efficient Color\nProgramming.” Proceedings of the ACM on Programming\nLanguages 8 (OOPSLA2): 846–75.\n\n\nChris Rorres. n.d. “Burning Mirrors: Refuting\nthe Legend.” https://math.nyu.edu/Archimedes/Mirrors/legend/legend.html.\n\n\nClay Reid, R, and Jose-Manuel Alonso. 1995. “Specificity of\nMonosynaptic Connections from Thalamus to Visual Cortex.”\nNature 378 (6554): 281–84.\n\n\nCmglee. 2018. “Images of a garden with some\ntulips and narcissus; CC BY-SA 3.0.” https://commons.wikimedia.org/wiki/File:Colorful_spring_garden_Bayer_%2B_RGB.png.\n\n\n———. 2019. “Comparison of front- vs.\nback-illuminated sensors; CC BY-SA 4.0 license.” https://commons.wikimedia.org/wiki/File:Comparison_backside_illumination.svg.\n\n\nCook, Robert L, and Kenneth E. Torrance. 1982. “A Reflectance\nModel for Computer Graphics.” ACM Transactions on Graphics\n(ToG) 1 (1): 7–24.\n\n\nCornsweet, Tom. 1970. Visual Perception. Academic press.\n\n\nCornsweet, Tom N. 1962. “The Staircase-Method in\nPsychophysics.” The American Journal of Psychology 75\n(3): 485–91.\n\n\nCrawford, BH. 1937. “The Change of Visual Sensitivity with\nTime.” Proceedings of the Royal Society of London. Series\nB-Biological Sciences 123 (830): 69–89.\n\n\n———. 1947. “Visual Adaptation in Relation to Brief Conditioning\nStimuli.” Proceedings of the Royal Society of London. Series\nB-Biological Sciences 134 (875): 283–302.\n\n\n———. 1949. “The Scotopic Visibility Function.”\nProceedings of the Physical Society. Section B 62 (5): 321.\n\n\nCrescitelli, Frederick, and Herbert JA Dartnall. 1953. “Human\nVisual Purple.” Nature 172 (4370): 195–97.\n\n\nCristaldi, David JR, Salvatore Pennisi, and Francesco Pulvirenti. 2009.\nLiquid Crystal Display Drivers: Techniques and Circuits. Vol.\n2. Springer.\n\n\nCurcio, Christine A, Kenneth R Sloan, Robert E Kalina, and Anita E\nHendrickson. 1990. “Human Photoreceptor Topography.”\nJournal of Comparative Neurology 292 (4): 497–523.\n\n\nCurran919. 2022. “Color Blind Confusion\nLines; CC BY-SA 4.0 license.” https://commons.wikimedia.org/wiki/File:Color_Blind_Confusion_Lines.png.\n\n\nCVRL. n.d. “Photopigment curves based on the\nStockman & Sharpe (2000) cone fundamentals.” http://www.cvrl.org/database/text/pigments/ssabance.htm.\n\n\nDacey, Dennis M, and Barry B Lee. 1994. “The’blue-on’opponent\nPathway in Primate Retina Originates from a Distinct Bistratified\nGanglion Cell Type.” Nature 367 (6465): 731–35.\n\n\nDacey, Dennis M, Hsi-Wen Liao, Beth B Peterson, Farrel R Robinson,\nVivianne C Smith, Joel Pokorny, King-Wai Yau, and Paul D Gamlin. 2005.\n“Melanopsin-Expressing Ganglion Cells in Primate Retina Signal\nColour and Irradiance and Project to the LGN.” Nature\n433 (7027): 749–54.\n\n\nDacey, Dennis M, and Michael R Petersen. 1992. “Dendritic Field\nSize and Morphology of Midget and Parasol Ganglion Cells of the Human\nRetina.” Proceedings of the National Academy of Sciences\n89 (20): 9666–70.\n\n\nDaderot. 2012. “Kongens Have, Copenhagen,\nDenmark; CC0 1.0 license.” https://commons.wikimedia.org/wiki/File:Marble_ball_-_Kongens_Have_-_Copenhagen_-_DSC07898.JPG.\n\n\nDai, Qi, Qifeng Shan, Jing Wang, Sameer Chhajed, Jaehee Cho, E Fred\nSchubert, Mary H Crawford, Daniel D Koleske, Min-Ho Kim, and Yongjo\nPark. 2010. “Carrier Recombination Mechanisms and Efficiency Droop\nin GaInN/GaN Light-Emitting Diodes.” Applied Physics\nLetters 97 (13).\n\n\nDaniel Smith. 2016. “Calculating the Emission\nSpectra from Common Light Sources.” https://www.comsol.com/blogs/calculating-the-emission-spectra-from-common-light-sources/.\n\n\nDanilova, MV, and JD Mollon. 2025. “Effect of Stimulus Size on\nChromatic Discrimination.” Journal of the Optical Society of\nAmerica A 42 (5): B167–77.\n\n\nDartnall, Herbert J. A. 1972. “Photosensitivity.” In\nPhotochemistry of Vision, edited by Herbert J. A. Dartnall,\n122–45. Springer Berlin Heidelberg.\n\n\nDartnall, Herbert JA, James K Bowmaker, and John Dixon Mollon. 1983.\n“Human Visual Pigments: Microspectrophotometric Results from the\nEyes of Seven Persons.” Proceedings of the Royal Society of\nLondon. Series B. Biological Sciences 220 (1218): 115–30.\n\n\nDe Valois, RL, CJ Smith, ST Kitai, and AJ Karoly. 1958. “Response\nof Single Cells in Monkey Lateral Geniculate Nucleus to Monochromatic\nLight.” Science 127 (3292): 238–39.\n\n\nDe Valois, Russell L, Israel Abramov, and Gerald H Jacobs. 1966.\n“Analysis of Response Patterns of LGN Cells.” JOSA\n56 (7): 966–77.\n\n\nDe Vries, HL. 1943. “The Quantum Character of Light and Its\nBearing Upon Threshold of Vision, the Differential Sensitivity and\nVisual Acuity of the Eye.” Physica 10 (7): 553–64.\n\n\nDeglr6328. 2018. “Spectrum of a \"white\" LED\nshowing blue light directly emitted by the GaN LED and the stokes\nshifted yellowish light emitted by the Ce:YAG phosphor; CC BY-SA 3.0\nlicense.” https://commons.wikimedia.org/wiki/File:White_LED.png.\n\n\nDeng, Xiong, Yan Wu, AM Khalid, Xi Long, and Jean-Paul MG Linnartz.\n2017. “LED Power Consumption in Joint Illumination and\nCommunication System.” Optics Express 25 (16):\n18990–9003.\n\n\nDerrington, AM, and P Lennie. 1984. “Spatial and Temporal Contrast\nSensitivities of Neurones in Lateral Geniculate Nucleus of\nMacaque.” The Journal of Physiology 357 (1): 219–40.\n\n\nDerrington, Andrew M, John Krauskopf, and Peter Lennie. 1984.\n“Chromatic Mechanisms in Lateral Geniculate Nucleus of\nMacaque.” The Journal of Physiology 357 (1): 241–65.\n\n\nDeValois, Russell L, and Karen K DeValois. 1990. “Spatial\nVision.”\n\n\nDimmick, Forrest L, and Margaret R Hubbard. 1939a. “The Spectral\nComponents of Psychologically Unique Red.” The American\nJournal of Psychology 52 (3): 348–53.\n\n\n———. 1939b. “The Spectral Location of Psychologically Unique\nYellow, Green, and Blue.” The American Journal of\nPsychology 52 (2): 242–54.\n\n\nDMahalko. 2009a. “InFocus LP425z Single Chip\nDLP - 4-segment color wheel - Green Blue; CC BY-SA 3.0\nlicense.” https://commons.wikimedia.org/wiki/File:InFocus_LP425z_Single_Chip_DLP_-_4-segment_color_wheel_-_Green_Blue.JPG.\n\n\n———. 2009b. “InFocus LP425z Single Chip DLP -\n4-segment color wheel - Red Gray; CC BY-SA 3.0 license.”\nhttps://commons.wikimedia.org/wiki/File:InFocus_LP425z_Single_Chip_DLP_-_4-segment_color_wheel_-_Red_Gray.JPG.\n\n\n———. 2009c. “InFocus LP425z Single Chip DLP -\nDMD Light Path; CC BY-SA 3.0 license.” https://commons.wikimedia.org/wiki/File:InFocus_LP425z_Single_Chip_DLP_-_DMD_Light_Path.jpg.\n\n\nDo, Michael Tri Hoang, and KW Yau. 2010. “Intrinsically\nPhotosensitive Retinal Ganglion Cells.” Physiological\nReviews.\n\n\nDominic Alves. 2006. “Pinhole Size Chart; CC\nBY 2.0 license.” https://www.flickr.com/photos/dominicspics/4589206921.\n\n\nDong, Yue, Stephen Lin, Baining Guo, et al. 2013. Material\nAppearance Modeling: A Data-Coherent Approach. Springer.\n\n\nDonner, Kristian. 1992. “Noise and the Absolute Thresholds of Cone\nand Rod Vision.” Vision Research 32 (5): 853–66.\n\n\nDorsey, Julie, Holly Rushmeier, and François Sillion. 2010. Digital\nModeling of Material Appearance. Elsevier.\n\n\nDowling, John E, and Joseph L Dowling Jr. 2016. Vision: How It Works\nand What Can Go Wrong. MIT Press.\n\n\nDrBob. 2007. “Refractive index vs. wavelength\nfor BK7 glass; CC BY-SA 3.0 license.” https://en.wikipedia.org/wiki/File:Sellmeier-equation.svg.\n\n\nDuinkharjav, Budmonde, Kenneth Chen, Abhishek Tyagi, Jiayi He, Yuhao\nZhu, and Qi Sun. 2022. “Color-Perception-Guided Display Power\nReduction for Virtual Reality.” ACM Transactions on Graphics\n(TOG) 41 (6): 1–16.\n\n\nDunn, Felice A, Martin J Lankheet, and Fred Rieke. 2007. “Light\nAdaptation in Cone Vision Involves Switching Between Receptor and\nPost-Receptor Sites.” Nature 449 (7162): 603–6.\n\n\nDyck, Rudolph H, and Gene P Weckler. 1968. “Integrated Arrays of\nSilicon Photodetectors for Image Sensing.” IEEE Transactions\non Electron Devices 15 (4): 196–201.\n\n\nEason, G, AR Veitch, RM Nisbet, and FW Turnbull. 1978. “The Theory\nof the Back-Scattering of Light by Blood.” Journal of Physics\nD: Applied Physics 11 (10): 1463.\n\n\nEinstein, Albert. 1905a. “On a Heuristic Point of View about the\nCreation and Conversion of Light.” Annalen Der Physik 17\n(6): 132–48.\n\n\n———. 1905b. “Über Einen Die Erzeugung Und Verwandlung\nDes Lichtes Betreffenden Heuristischen Gesichtspunkt.” Albert\nEinstein-Gesellschaft.\n\n\nEki, Ryoji, Satoshi Yamada, Hiroyuki Ozawa, Hitoshi Kai, Kazuyuki\nOkuike, Hareesh Gowtham, Hidetomo Nakanishi, et al. 2021. “9.6 a\n1/2.3 Inch 12.3 Mpixel with on-Chip 4.97 TOPS/w CNN Processor\nBack-Illuminated Stacked CMOS Image Sensor.” In 2021 IEEE\nInternational Solid-State Circuits Conference (ISSCC), 64:154–56.\nIEEE.\n\n\nEl Gamal, Abbas, and Helmy Eltoukhy. 2005. “CMOS Image\nSensors.” IEEE Circuits and Devices Magazine 21 (3):\n6–20.\n\n\nEMVA. 2021. “EMVA Standard 1288 Standard for\nCharacterization of Image Sensors and Cameras.” https://www.emva.org/wp-content/uploads/EMVA1288General_4.0Release.pdf.\n\n\nEngel, Klaus, Markus Hadwiger, Joe M Kniss, Christof Rezk-Salama, and\nDaniel Weiskopf. 2006. Real-Time Volume Graphics. A K Peters,\nLtd.\n\n\nEnroth-Cugell, Christina, B Gevene Hertz, and P Lennie. 1977.\n“Cone Signals in the Cat’s Retina.” The Journal of\nPhysiology 269 (2): 273–96.\n\n\nEric Bajart. 2010. “Quantum efficiency of the\nCCD sensor ‘PC1’ in the Hubble Space Telescope’s Wide Field\nand Planetary Camera WFPC2; CC BY-SA 3.0.” https://commons.wikimedia.org/wiki/File:Quantum_efficiency_graph_for_WFPC2-en.svg.\n\n\nFain, Gordon L. 1976. “Sensitivity of Toad Rods: Dependence on\nWave-Length and Background Illumination.” The Journal of\nPhysiology 261 (1): 71–101.\n\n\nFain, Gordon L, Hugh R Matthews, M Carter Cornwall, and Yiannis\nKoutalos. 2001. “Adaptation in Vertebrate Photoreceptors.”\nPhysiological Reviews 81 (1): 117–51.\n\n\nFairchild, Mark D. 2013. Color Appearance Models. 3rd ed. John\nWiley & Sons.\n\n\nFairman, Hugh S, Michael H Brill, and Henry Hemmendinger. 1997.\n“How the CIE 1931 Color-Matching Functions Were Derived from\nWright-Guild Data.” Color Research & Application 22\n(1): 11–23.\n\n\nFarrell, Thomas J, Michael S Patterson, and Brian Wilson. 1992. “A\nDiffusion Theory Model of Spatially Resolved, Steady-State Diffuse\nReflectance for the Noninvasive Determination of Tissue Optical\nProperties in Vivo.” Medical Physics 19 (4): 879–88.\n\n\nFechner, Gustav Theodor. 1860. Elemente Der Psychophysik. Vol.\n2. Breitkopf u. Härtel.\n\n\nFeng, Yu, Tianrui Ma, Yuhao Zhu, and Xuan Zhang. 2024. “Blisscam:\nBoosting Eye Tracking Efficiency with Learned in-Sensor Sparse\nSampling.” In 2024 ACM/IEEE 51st Annual International\nSymposium on Computer Architecture (ISCA), 1262–77. IEEE.\n\n\nFesenko, Evgeniy E, Stanislav S Kolesnikov, and Arkadiy L Lyubarsky.\n1985. “Induction by Cyclic GMP of Cationic Conductance in Plasma\nMembrane of Retinal Rod Outer Segment.” Nature 313\n(6000): 310–13.\n\n\nFeynman, R. 1985. QED: The Strange Theory of Light and Matter by\nRichard Feynman. Princeton University Press.\n\n\nField, Greg D, Alexander Sher, Jeffrey L Gauthier, Martin Greschner,\nJonathon Shlens, Alan M Litke, and EJ Chichilnisky. 2007. “Spatial\nProperties and Functional Organization of Small Bistratified Ganglion\nCells in Primate Retina.” Journal of Neuroscience 27\n(48): 13261–72.\n\n\nFlatla, David R, Alan R Andrade, Ross D Teviotdale, Dylan L Knowles, and\nCraig Stewart. 2015. “ColourID.” In Proceedings of the\n33rd Annual ACM Conference on Human Factors in Computing Systems.\nACM.\n\n\nFong, Julian, Magnus Wrenninge, Christopher Kulla, and Ralf Habel. 2017.\n“Production Volume Rendering: Siggraph 2017 Course.” In\nACM SIGGRAPH 2017 Courses, 1–97.\n\n\nFossum, Eric R. 1993. “Active Pixel Sensors: Are CCDs\nDinosaurs?” In Charge-Coupled Devices and Solid State Optical\nSensors III, 1900:2–14. SPIE.\n\n\n———. 1997. “CMOS Image Sensors: Electronic\nCamera-on-a-Chip.” IEEE Transactions on Electron Devices\n44 (10): 1689–98.\n\n\nFossum, Eric R, and Donald B Hondongwa. 2014. “A Review of the\nPinned Photodiode for CCD and CMOS Image Sensors.” IEEE\nJournal of the Electron Devices Society.\n\n\nFossum, Eric R, Nobukazu Teranishi, and Albert JP Theuwissen. 2024.\n“Digital Image Sensor Evolution and New Frontiers.”\nAnnual Review of Vision Science 10 (1): 171–98.\n\n\nFowler, Boyd, Abbas El Gamal, and David XD Yang. 1994. “A CMOS\nArea Image Sensor with Pixel-Level a/d Conversion.” In\nProceedings of IEEE International Solid-State Circuits\nConference-ISSCC’94, 226–27. IEEE.\n\n\nFrisvad, Jeppe Revall, Soeren A Jensen, Jonas Skovlund Madsen, António\nCorreia, Li Yang, Søren Kimmer Schou Gregersen, Youri Meuret, and P-E\nHansen. 2020. “Survey of Models for Acquiring the Optical\nProperties of Translucent Materials.” In Computer Graphics\nForum, 39:729–55. 2. Wiley Online Library.\n\n\nFu, Yingbin. 2010. “Phototransduction in Rods and Cones.”\nIn WebVision: The Organization of the Retina and Visual System,\nedited by Helga Kolb, Eduardo Fernandez, and Ralph Nelson.\n\n\nFuji. 2005. “Fujifilm Professional Data\nGuide.” https://asset.fujifilm.com/www/us/files/2020-03/3ab271f46f8d71c7e4c91bcedb7de050/ProfessionalFilmDataGuide.pdf.\n\n\nFuortes, MGF, RD Gunkel, and WAH Rushton. 1961. “Increment\nThresholds in a Subject Deficient in Cone Vision.” The\nJournal of Physiology 156 (1): 179.\n\n\nGauza, Sebastian, Xinyu Zhu, Wiktor Piecek, Roman Dabrowski, and\nShin-Tson Wu. 2007. “Fast Switching Liquid Crystals for\nColor-Sequential LCDs.” Journal of Display Technology 3\n(3): 250–52.\n\n\nGeddes, Connor, David R Flatla, and Ciabhan L Connelly. 2023. “30\nYears of Solving the Wrong Problem: How Recolouring Tool Design Fails\nThose with Colour Vision Deficiency.” In Proceedings of the\n25th International ACM SIGACCESS Conference on Computers and\nAccessibility, 1–13.\n\n\nGeffroy, Bernard, Philippe Le Roy, and Christophe Prat. 2006.\n“Organic Light-Emitting Diode (OLED) Technology: Materials,\nDevices and Display Technologies.” Polymer International\n55 (6): 572–82.\n\n\nGegenfurtner, Karl R. 2003. “Cortical Mechanisms of Colour\nVision.” Nature Reviews Neuroscience 4 (7): 563–72.\n\n\nGilbert, Charles D, and Wu Li. 2013. “Top-down Influences on\nVisual Processing.” Nature Reviews Neuroscience 14 (5):\n350–63.\n\n\nGiorgianni, Edward J, and Thomas E Madden. 2009. Digital Color\nManagement: Encoding Solutions. Vol. 13. John Wiley & Sons.\n\n\nGlassner, Andrew S. 1995. Principles of Digital Image\nSynthesis. Elsevier.\n\n\nGlrx. 2018. “Ray diagram illustrating a form\nof coma aberration; CC BY-SA 3.0 license.” https://commons.wikimedia.org/wiki/File:Lens-coma.svg.\n\n\nGnash. 2017. “Rendering of the Extremely\nLarge Telescope from 2009; CC BY-SA 4.0 license.” https://commons.wikimedia.org/wiki/File:Latest_Rendering_of_the_E-ELT.jpg.\n\n\nGogliettino, Alex R, Sasidhar S Madugula, Lauren E Grosberg, Ramandeep S\nVilkhu, Jeff Brown, Huy Nguyen, Alexandra Kling, et al. 2023.\n“High-Fidelity Reproduction of Visual Signals by Electrical\nStimulation in the Central Primate Retina.” Journal of\nNeuroscience 43 (25): 4625–41.\n\n\nGoldstein, E. Bruce. 2009. Sensation and Perception. 8th ed.\nCengage Learning.\n\n\nGoldstein, E. Bruce, and R. James Brockmole. 2017. Sensation and\nPerception. 10th ed. Cengage Learning.\n\n\nGómez-Robledo, Luis, EM Valero, R Huertas, MA Martı́nez-Domingo, and\nJavier Hernández-Andrés. 2018. “Do EnChroma Glasses Improve Color\nVision for Colorblind Subjects?” Optics Express 26 (22):\n28693–703.\n\n\nGoodale, Melvyn A, A David Milner, Lorna S Jakobson, and David P Carey.\n1991. “A Neurological Dissociation Between Perceiving Objects and\nGrasping Them.” Nature 349 (6305): 154–56.\n\n\nGortler, Steven J, Radek Grzeszczuk, Richard Szeliski, and Michael F\nCohen. 1996. “The Lumigraph.” In ACM Transactions on\nGraphics (ToG), 43–54. ACM New York, NY, USA.\n\n\nGouras, P, and E Zrenner. 1979. “Enhancement of Luminance Flicker\nby Color-Opponent Mechanisms.” Science 205 (4406):\n587–89.\n\n\nGraham, Clarence Henry, and Yun Hsia. 1958. “Color Defect and\nColor Theory: Studies of Normal and Color-Blind Persons, Including a\nSubject Color-Blind in One Eye but Not in the Other.”\nScience 127 (3300): 675–82.\n\n\nGranit, Ragnar. 1941. “The Retinal Mechanism of Color\nReception.” Journal of the Optical Society of America 31\n(9): 570–80.\n\n\n———. 1943. “A Physiological Theory of Colour Perception.”\nNature 151 (3818): 11–14.\n\n\n———. 1945a. “The Colour Receptors of the Mammalian Retina.”\nJournal of Neurophysiology 8 (3): 195–210.\n\n\n———. 1945b. “The Electrophysiological Analysis of the Fundamental\nProblem of Colour Reception.” Proceedings of the Physical\nSociety 57 (6): 447–63.\n\n\nGreen, Martin A, and Mark J Keevers. 1995. “Optical Properties of\nIntrinsic Silicon at 300 k.” Progress in Photovoltaics:\nResearch and Applications 3 (3): 189–92.\n\n\nGreene, Ned, and Paul S Heckbert. 1986. “Creating Raster Omnimax\nImages from Multiple Perspective Views Using the Elliptical Weighted\nAverage Filter.” IEEE Computer Graphics and Applications\n6 (6): 21–27.\n\n\nGross, Markus, and Hanspeter Pfister. 2011. Point-Based\nGraphics. Elsevier.\n\n\nGruhl, Thomas, Tobias Weinert, Matthew J Rodrigues, Christopher J Milne,\nGiorgia Ortolani, Karol Nass, Eriko Nango, et al. 2023. “Ultrafast\nStructural Changes Direct the First Molecular Events of Vision.”\nNature 615 (7954): 939–44.\n\n\nGuenter, Brian, Mark Finch, Steven Drucker, Desney Tan, and John Snyder.\n2012. “Foveated 3D Graphics.” ACM Transactions on\nGraphics (TOG) 31 (6): 1–10.\n\n\nGuild, John. 1931. “The Colorimetric Properties of the\nSpectrum.” Philosophical Transactions of the Royal Society of\nLondon. Series A, Containing Papers of a Mathematical or Physical\nCharacter 230 (681-693): 149–87.\n\n\nHandWiki. 2024. “Kepler space telescope focal\nplane; CC BY-SA 3.0 license.” https://handwiki.org/wiki/index.php?curid=2015813.\n\n\nHansen, Thorsten, Lars Pracejus, and Karl R Gegenfurtner. 2009.\n“Color Perception in the Intermediate Periphery of the Visual\nField.” Journal of Vision 9 (4): 26–26.\n\n\nHartline, H Keffer. 1938. “The Response of Single Optic Nerve\nFibers of the Vertebrate Eye to Illumination of the Retina.”\nAmerican Journal of Physiology-Legacy Content 121 (2): 400–415.\n\n\n———. 1939. “Excitation and Inhibition of the\" Off\" Response in\nVertebrate Optic Nerve Fibers.” Am. J. Physiol 126:527.\n\n\n———. 1940a. “The Effects of Spatial Summation in the Retina on the\nExcitation of the Fibers of the Optic Nerve.” American\nJournal of Physiology-Legacy Content 130 (4): 700–711.\n\n\n———. 1940b. “The Receptive Fields of Optic Nerve Fibers.”\nAmerican Journal of Physiology-Legacy Content 130 (4): 690–99.\n\n\n———. 1949. “Inhibition of Activity of Visual Receptors by\nIlluminating Nearby Retinal Areas in the Limulus Eye.”\nFederation Proceedings 8 (1): 69.\n\n\nHartline, H Keffer, and Clarence Henry Graham. 1932. “Nerve\nImpulses from Single Receptors in the Eye.” Journal of\nCellular & Comparative Physiology.\n\n\nHartline, H Keffer, Henry G Wagner, and Floyd Ratliff. 1956.\n“Inhibition in the Eye of Limulus.” The Journal of\nGeneral Physiology 39 (5): 651–73.\n\n\nHaruta, Tsutomu, Tsutomu Nakajima, Jun Hashizume, Taku Umebayashi,\nHiroshi Takahashi, Kazuo Taniguchi, Masami Kuroda, et al. 2017.\n“4.6 a 1/2.3 Inch 20Mpixel 3-Layer Stacked CMOS Image Sensor with\nDRAM.” In 2017 IEEE International Solid-State Circuits\nConference (ISSCC), 76–77. IEEE.\n\n\nHasinoff, Samuel W, Dillon Sharlet, Ryan Geiss, Andrew Adams, Jonathan T\nBarron, Florian Kainz, Jiawen Chen, and Marc Levoy. 2016. “Burst\nPhotography for High Dynamic Range and Low-Light Imaging on Mobile\nCameras.” ACM Transactions on Graphics (ToG) 35 (6):\n1–12.\n\n\nHattar, Samer, H-W Liao, Motoharu Takao, David M Berson, and KW Yau.\n2002. “Melanopsin-Containing Retinal Ganglion Cells: Architecture,\nProjections, and Intrinsic Photosensitivity.” Science\n295 (5557): 1065–70.\n\n\nHecht, Eugene. 2016. Optics. 5th ed. Pearson.\n\n\nHecht, Selig, Charles Haig, and Aurin M Chase. 1937. “The\nInfluence of Light Adaptation on Subsequent Dark Adaptation of the\nEye.” The Journal of General Physiology 20 (6): 831–50.\n\n\nHecht, Selig, Simon Shlaer, and Maurice Henri Pirenne. 1942.\n“Energy, Quanta, and Vision.” The Journal of General\nPhysiology 25 (6): 819–40.\n\n\nHeckbert, Paul S. 1989. “Fundamentals of Texture Mapping and Image\nWarping. Master’s Thesis.” University of California,\nBerkeley.\n\n\nHegarty, James, John S Brunhaver, Zachary DeVito, Jonathan Ragan-Kelley,\nNoy Cohen, Steven Bell, Artem Vasilyev, Mark Horowitz, and Pat Hanrahan.\n2014. “Darkroom: Compiling High-Level Image Processing Code into\nHardware Pipelines.” ACM Trans. Graph. 33 (4): 144–41.\n\n\nHering, Ewald. 1878. Zur Lehre Vom Lichtsinne: Sechs Mittheilungen\nan Die Kaiser. Akad. Der Wissenschaften in Wien. C. Gerold’s Sohn.\n\n\n———. 1964. Outlines of a Theory of the Light Sense (Translation by\nJameson and Hurvish; Originally Published in 1878). Harvard\nUniversity Press.\n\n\nHirata, Tomoki, Hironobu Murata, Hideaki Matsuda, Yojiro Tezuka, and\nShiro Tsunai. 2021. “7.8 a 1-Inch 17Mpixel 1000fps\nBlock-Controlled Coded-Exposure Back-Illuminated Stacked CMOS Image\nSensor for Computational Imaging and Adaptive Dynamic Range\nControl.” In 2021 IEEE International Solid-State Circuits\nConference (ISSCC), 64:120–22. IEEE.\n\n\nHodgkin, Alan L, and Andrew F Huxley. 1952. “A Quantitative\nDescription of Membrane Current and Its Application to Conduction and\nExcitation in Nerve.” The Journal of Physiology 117 (4):\n500.\n\n\nHoffman, David M, Ahna R Girshick, Kurt Akeley, and Martin S Banks.\n2008. “Vergence–Accommodation Conflicts Hinder Visual Performance\nand Cause Visual Fatigue.” Journal of Vision 8 (3):\n33–33.\n\n\nHofmann, Klaus Peter, and Trevor D Lamb. 2023. “Rhodopsin,\nLight-Sensor of Vision.” Progress in Retinal and Eye\nResearch 93:101116.\n\n\nHong, Fangfang, Ruby Bouhassira, Jason Chow, Craig Sanders, Michael\nShvartsman, Phillip Guan, Alex H Williams, and David H Brainard. 2025.\n“Comprehensive Characterization of Human Color Discrimination\nThresholds.” bioRxiv, 2025–07.\n\n\nHong, HyungKi, HyunHo Shin, and InJae Chung. 2007. “In-Plane\nSwitching Technology for Liquid Crystal Display Television.”\nJournal of Display Technology 3 (4): 361–70.\n\n\nHsu, Tzu-Hsiang, Yi-Ren Chen, Ren-Shuo Liu, Chung-Chuan Lo, Kea-Tiong\nTang, Meng-Fan Chang, and Chih-Cheng Hsieh. 2020. “A 0.5-v\nReal-Time Computational CMOS Image Sensor with Programmable Kernel for\nFeature Extraction.” IEEE Journal of Solid-State\nCircuits 56 (5): 1588–96.\n\n\nHsu, Yi-Te, and Robert S Molday. 1993. “Modulation of the\ncGMP-Gated Channel of Rod Photoreceptor Cells by Calmodulin.”\nNature 361 (6407): 76–79.\n\n\nHu, Chenming. 2009. Modern Semiconductor Devices for Integrated\nCircuits. Prentice Hall.\n\n\nHuang, Yuge, En-Lin Hsiang, Ming-Yang Deng, and Shin-Tson Wu. 2020.\n“Mini-LED, Micro-LED and OLED Displays: Present Status and Future\nPerspectives.” Light: Science & Applications 9 (1):\n105.\n\n\nHubel, David H. 1995. Eye, Brain, and Vision. Scientific\nAmerican Library/Scientific American Books.\n\n\nHubel, David H, and Torsten N Wiesel. 1959. “Receptive Fields of\nSingle Neurones in the Cat’s Striate Cortex.” J Physiol\n148 (3): 574–91.\n\n\n———. 1962. “Receptive Fields, Binocular Interaction and Functional\nArchitecture in the Cat’s Visual Cortex.” The Journal of\nPhysiology 160 (1): 106.\n\n\n———. 1968. “Receptive Fields and Functional Architecture of Monkey\nStriate Cortex.” The Journal of Physiology 195 (1):\n215–43.\n\n\nHuggett, Anthony, Chris Silsby, Sergi Cami, and Jeff Beck. 2009.\n“A Dual-Conversion-Gain Video Sensor with Dewarping and Overlay on\na Single Chip.” In 2009 IEEE International Solid-State\nCircuits Conference-Digest of Technical Papers, 52–53. IEEE.\n\n\nHunt, David M, Kanwaljit S Dulai, Jill A Cowing, Catherine Julliot, John\nD Mollon, James K Bowmaker, Wen-Hsiung Li, and David Hewett-Emmett.\n1998. “Molecular Evolution of Trichromacy in Primates.”\nVision Research 38 (21): 3299–3306.\n\n\nHurley, Ann C, G David Lange, and Peter H Hartline. 1978. “The\nAdjustable ‘Pinhole Camera’ Eye of Nautilus.”\nJournal of Experimental Zoology 205 (1): 37–43.\n\n\nHurvich, Leo M, and Dorothea Jameson. 1955. “Some Quantitative\nAspects of an Opponent-Colors Theory. II. Brightness, Saturation, and\nHue in Normal and Dichromatic Vision.” JOSA 45 (8):\n602–16.\n\n\n———. 1957. “An Opponent-Process Theory of Color Vision.”\nPsychological Review 64 (6p1): 384.\n\n\nICDM. 2025. Information Display Measurement Standard, v 1.3.\nSociety for Information Display.\n\n\nIdrees, Saad, Michael B Manookin, Fred Rieke, Greg D Field, and Joel\nZylberberg. 2024. “Biophysical Neural Adaptation Mechanisms Enable\nArtificial Neural Networks to Capture Dynamic Retinal\nComputation.” Nature Communications 15 (1): 5957.\n\n\nIEC. 1998. “IEC/4WD 61966-2-1: Colour\nMeasurement and Management in Multimedia Systems and Equipment - Part\n2-1: Default RGB Colour Space - sRGB.” https://web.archive.org/web/20141225172302/http://www2.units.it/ipl/students_area/imm2/files/Colore1/sRGB.pdf.\n\n\nIida, S, Y Sakano, T Asatsuma, M Takami, I Yoshiba, N Ohba, H Mizuno, et\nal. 2018. “A 0.68 e-Rms Random-Noise 121dB Dynamic-Range Sub-Pixel\nArchitecture CMOS Image Sensor with LED Flicker Mitigation.” In\n2018 IEEE International Electron Devices Meeting (IEDM), 10–12.\nIEEE.\n\n\nIkeno, Rimon, Kazuya Mori, Masayuki Uno, Ken Miyauchi, Toshiyuki\nIsozaki, Isao Takayanagi, Junichi Nakamura, et al. 2022. “A\n4.6-μm, 127-dB Dynamic Range,\nUltra-Low Power Stacked Digital Pixel Sensor with Overlapped Triple\nQuantization.” IEEE Transactions on Electron Devices 69\n(6): 2943–50.\n\n\nILM. 2025. “OpenEXR Specification, v\n3.4.0.” https://openexr.com/en/latest/.\n\n\nimatest. n.d. “Gamma, Tonal Response Curve,\nand related concepts.” https://www.imatest.com/imaging/tonal-response-gamma/.\n\n\nImmel, David S, Michael F Cohen, and Donald P Greenberg. 1986. “A\nRadiosity Method for Non-Diffuse Environments.” Acm Siggraph\nComputer Graphics 20 (4): 133–42.\n\n\nInductiveload. 2009. “Mathematical function\nof an airy disk pattern; released into the public domain by the\ncopyright holder.” https://commons.wikimedia.org/wiki/File:Airy_Pattern.svg.\n\n\nIngram, Norianne T, Alapakkam P Sampath, and Gordon L Fain. 2016.\n“Why Are Rods More Sensitive Than Cones?” The Journal\nof Physiology 594 (19): 5415–26.\n\n\nInternational Color Consortium. 2019. “Specification ICC.2:2019 (Profile version 5.0.0 -\niccMAX).” https://color.org/specification/ICC.2-2019.pdf.\n\n\nIRDS. 2024. “International Roadmap for Devices and\nSystems.” https://irds.ieee.org/.\n\n\nIshimaru, Akira. 1977. “Theory and Application of Wave Propagation\nand Scattering in Random Media.” Proceedings of the IEEE\n65 (7): 1030–61.\n\n\nIshimaru, Akira et al. 1978. Wave Propagation and Scattering in\nRandom Media. Vol. 2. Academic press New York.\n\n\nITU-R. 2011a. “Recommendation ITU-R BT.1886:\nReference electro-optical transfer function for flat panel displays used\nin HDTV studio production.” https://www.itu.int/dms_pubrec/itu-r/rec/bt/R-REC-BT.1886-0-201103-I!!PDF-E.pdf.\n\n\n———. 2011b. “Recommendation ITU-R BT.601-7:\nStudio encoding parameters of digital television for standard 4:3 and\nwide-screen 16:9 aspect ratios.” https://www.itu.int/dms_pubrec/itu-r/rec/bt/r-rec-bt.601-7-201103-i!!pdf-e.pdf.\n\n\n———. 2015a. “Recommendation ITU-R BT.2020-2:\nParameter values for ultra-high definition television systems for\nproduction and international programme exchange.” https://www.itu.int/dms_pubrec/itu-r/rec/bt/R-REC-BT.2020-2-201510-I!!PDF-E.pdf.\n\n\n———. 2015b. “Recommendation ITU-R BT.709-6:\nParameter values for the HDTV standardsfor production and international\nprogramme exchange.” https://www.itu.int/dms_pubrec/itu-r/rec/bt/R-REC-BT.709-6-201506-I!!PDF-E.pdf.\n\n\n———. 2025. “Recommendation ITU-R BT.2100-3:\nImage parameter values for high dynamic range television for use in\nproduction and international programme exchange.” https://www.itu.int/dms_pubrec/itu-r/rec/bt/R-REC-BT.2100-3-202502-I!!PDF-E.pdf.\n\n\nIvo Kruusamägi. 2010. “Cone cell anatomy; CC\nBY-SA 3.0 license.” https://commons.wikimedia.org/wiki/File:Cone_cell_en.png.\n\n\nJacobs, Gerald H. 2008. “Primate Color Vision: A Comparative\nPerspective.” Visual Neuroscience 25 (5-6): 619–33.\n\n\n———. 2009. “Evolution of Colour Vision in Mammals.”\nPhilosophical Transactions of the Royal Society B: Biological\nSciences 364 (1531): 2957–67.\n\n\nJameson, Dorothea, and Leo M Hurvich. 1955. “Some Quantitative\nAspects of an Opponent-Colors Theory. I. Chromatic Responses and\nSpectral Saturation.” JOSA 45 (7): 546–52.\n\n\nJensen, Henrik Wann, Stephen R Marschner, Marc Levoy, and Pat Hanrahan.\n2001. “A Practical Model for Subsurface Light Transport.”\nACM SIGGRAPH Computer Graphics, 511–18.\n\n\nJeon, Eun Jeong, Anoop Kumar Srivastava, Miyoung Kim, Kwang-Un Jeong,\nJeongmin Choi, Gi-Dong Lee, and Seung Hee Lee. 2009. “Temperature\nDependence of the Electro-Optic Characteristics in the Liquid Crystal\nDisplay Switching Modes.” Journal of Information Display\n10 (4): 175–79.\n\n\nJiang, Haomiao, Joyce Farrell, and Brian Wandell. 2016. “A\nSpectral Estimation Theory for Color Appearance Matching.”\nElectronic Imaging 28:1–4.\n\n\nJohannes Vermeer. 1668a. “The Art of\nPainting; released into the public domain.” https://en.wikipedia.org/wiki/File:Jan_Vermeer_-_The_Art_of_Painting_-_Google_Art_Project.jpg.\n\n\n———. 1668b. “The Astronomer; released into\nthe public domain.” https://en.wikipedia.org/wiki/File:Johannes_Vermeer_-_The_Astronomer_-_1668.jpg.\n\n\nJohnsen, Sönke. 2012. The Optics of Life: A Biologist’s Guide to\nLight in Nature. Princeton University Press.\n\n\nJohnston-Feller, Ruth. 2001. Color Science in the Examination of\nMuseum Objects: Nondestructive Procedures. Getty Publications.\n\n\nJones, Andrew, Ian McDowall, Hideshi Yamada, Mark Bolas, and Paul\nDebevec. 2007. “Rendering for an Interactive 360 Light Field\nDisplay.” In ACM SIGGRAPH 2007 Papers, 40–es.\n\n\nJordan, Gabriele, Samir S Deeb, Jenny M Bosten, and John D Mollon. 2010.\n“The Dimensionality of Color Vision in Carriers of Anomalous\nTrichromacy.” Journal of Vision 10 (8): 12–12.\n\n\nJudd, Deane B. 1949. “The Color Perceptions of Deuteranopic and\nProtanopic Observers.” JOSA 39 (3): 252–56.\n\n\n———. 1951. “Report of US Secretariat Committee on Colorimetry and\nArtificial Daylight.” CIE Proceedings, 1951 1:11.\n\n\nJudd, Deane B, David L MacAdam, Günter Wyszecki, HW Budde, HR Condit, ST\nHenderson, and JL Simonds. 1964. “Spectral Distribution of Typical\nDaylight as a Function of Correlated Color Temperature.”\nJosa 54 (8): 1031–40.\n\n\nJudd, Deane B, and Günter Wyszecki. 1975. Color in Business,\nScience, and Industry. 3rd ed. John Wiley & Sons.\n\n\nJürgen Königs. 2006. “Analog pinhole\nphotography with multiple exposure; CC BY-SA 4.0 license.”\nhttps://commons.wikimedia.org/wiki/File:1)_Fenstertisch_mit_R%C3%BChrger%C3%A4t.jpg.\n\n\nKajiya, James T. 1986. “The Rendering Equation.” In\nProceedings of the 13th Annual Conference on Computer Graphics and\nInteractive Techniques, 143–50.\n\n\nKajiya, James T, and Brian P Von Herzen. 1984. “Ray Tracing Volume\nDensities.” ACM SIGGRAPH Computer Graphics 18 (3):\n165–74.\n\n\nKameda, Shinjiro. 2012. “Solid State Image Pickup Device Having\nOptical Black Pixels with Temperature Characteristics Above and Below\nTemperature Characteristics of Aperture Pixels.” Google Patents.\n\n\nKaraimer, Hakki Can, and Michael S Brown. 2016. “A Software\nPlatform for Manipulating the Camera Imaging Pipeline.” In\nComputer Vision–ECCV 2016: 14th European Conference, Amsterdam, the\nNetherlands, October 11–14, 2016, Proceedings, Part i 14, 429–44.\nSpringer.\n\n\nKaufman, Arie, and Klaus Mueller. 2003. “Volume Visualization and\nVolume Graphics.” Technical Report; Stony Brook\nUniversity.\n\n\nKerbl, Bernhard, Georgios Kopanas, Thomas Leimkühler, and George\nDrettakis. 2023. “3d Gaussian Splatting for Real-Time Radiance\nField Rendering.” ACM Trans. Graph. 42 (4): 139–31.\n\n\nKim, Seong-Jin, Kwang-Hyun Lee, Sang-Wook Han, and Euisik Yoon. 2005.\n“A 200/Spl Times/160 Pixel CMOS Fingerprint Recognition SoC with\nAdaptable Column-Parallel Processors.” In ISSCC. 2005 IEEE\nInternational Digest of Technical Papers. Solid-State Circuits\nConference, 2005., 250–596. IEEE.\n\n\nKipp Jones. 2005. “The sand/dust from the\nharmattan coming from the Sahara gives the Nigeria’s National Mosque in\nAbuja, a nice glow; CC BY-SA 2.0 license.” https://commons.wikimedia.org/wiki/File:MosqueinAbuja.jpg.\n\n\nKlein, GA. 2010. “Industrial Color Physics.” Springer\nScience+ Business Media.\n\n\nKnoblauch, Kenneth, and Matthew J McMahon. 1995. “Discrimination\nof Binocular Color Mixtures in Dichromacy: Evaluation of the\nMaxwell–Cornsweet Conjecture.” JOSA A 12 (10): 2219–29.\n\n\nKobayashi, Masahiro, Yusuke Onuki, Kazunari Kawabata, Hiroshi Sekine,\nToshiki Tsuboi, Takashi Muto, Takeshi Akiyama, et al. 2017. “4.5A\n1.8e-Rms Temporal Noise over 110 dB Dynamic Range 3.4μm Pixel Pitch Global-Shutter\nCMOS Image Sensor with Dual-Gain Amplifiers SS-ADC, Light Guide\nStructure, and Multiple-Accumulation Shutter.” IEEE Journal\nof Solid-State Circuits 53 (1): 219–28.\n\n\nKodak. 1998. “KODAK EKTACHROME 100 Professional\nFilm.” https://125px.com/docs/unsorted/kodak/e27.pdf.\n\n\nKolb, Craig, Don Mitchell, and Pat Hanrahan. 1995. “A Realistic\nCamera Model for Computer Graphics.” In Proceedings of the\n22nd Annual Conference on Computer Graphics and Interactive\nTechniques, 317–24.\n\n\nKolb, Helga, Eduardo Fernandez, and Ralph Nelson. 2005. “The\nOrganization of the Retina and Visual System.” Webvision-the\nOrganization of the Retina and Visual System.\n\n\nKondo, Toru, Yoshiaki Takemoto, Kenji Kobayashi, Mitsuhiro Tsukimura,\nNaohiro Takazawa, Hideki Kato, Shunsuke Suzuki, et al. 2015. “A 3D\nStacked CMOS Image Sensor with 16Mpixel Global-Shutter Mode and 2Mpixel\n10000fps Mode Using 4 Million Interconnections.” In 2015\nSymposium on VLSI Circuits (VLSI Circuits), C90–91. IEEE.\n\n\nKosigrim. 2007. “Rod cell anatomy; released\ninto the public domain by the copyright holder.” https://commons.wikimedia.org/wiki/File:Rod%26Cone.jpg.\n\n\nKozlowski, Lester J, J Luo, WE Kleinhans, and T Liu. 1998.\n“Comparison of Passive and Active Pixel Schemes for CMOS Visible\nImagers.” In Infrared Readout Electronics IV,\n3360:101–10. SPIE.\n\n\nKraft, TW, DM Schneeweis, and JL Schnapf. 1993. “Visual\nTransduction in Human Rod Photoreceptors.” The Journal of\nPhysiology 464 (1): 747–65.\n\n\nKrauskopf, John, and Gegenfurtner Karl. 1992. “Color\nDiscrimination and Adaptation.” Vision Research 32 (11):\n2165–75.\n\n\nKries, Johannes von. 1905. “Übersicht Der Tatsachen,\nErgebnisse für Die Theoretische Auffassung Des Sehorgans:\nZonentheorie.” In Handbuch Der Physiologie Des Menschen,\nDritter Band: Physiologie Der Sinne, edited by Willibald Nagel, 3rd\ned., 269--274. Vieweg und Sohn, Braunschweig.\n\n\nKropf, Allen. 1982. “Photosensitivity and Quantum Efficiency of\nPhotoisomerization in Rhodopsin and Retinal.” In Methods in\nEnzymology, 81:384–92. Elsevier.\n\n\nKubelka, Paul. 1948. “New Contributions to the Optics of Intensely\nLight-Scattering Materials. Part i.” Josa 38 (5):\n448–57.\n\n\nKubelka, Paul, and Franz Munk. 1931a. “An Article on Optics of\nPaint Layers (Translated by Stephen h. Westin).” Z. Tech.\nPhys 12 (593-601): 259–74.\n\n\n———. 1931b. “Ein Beitrag Zur Optik Der Farbanstriche.”\nZ. Tech. Phys 12:593–601.\n\n\nKuffler, Stephen W. 1952. “Neurons in the Retina: Organization,\nInhibition and Excitation Problems.” In Cold Spring Harbor\nSymposia on Quantitative Biology, 17:281–92. Cold Spring Harbor\nLaboratory Press.\n\n\n———. 1953. “Discharge Patterns and Functional Organization of\nMammalian Retina.” Journal of Neurophysiology 16 (1):\n37–68.\n\n\nKumagai, Oichi, Atsumi Niwa, Katsuhiko Hanzawa, Hidetaka Kato,\nShinichiro Futami, Toshio Ohyama, Tsutomu Imoto, et al. 2018. “A\n1/4-Inch 3.9 Mpixel Low-Power Event-Driven Back-Illuminated Stacked CMOS\nImage Sensor.” In 2018 IEEE International Solid-State\nCircuits Conference-(ISSCC), 86–88. IEEE.\n\n\nKumagai, Y, R Yoshita, N Osawa, H Ikeda, K Yamashita, T Abe, S Kudo, et\nal. 2018. “Back-Illuminated 2.74μm-Pixel-Pitch Global Shutter\nCMOS Image Sensor with Charge-Domain Memory Achieving 10k e-Saturation\nSignal.” In 2018 IEEE International Electron Devices Meeting\n(IEDM), 10–16. IEEE.\n\n\nKwon, Minho, Seunghyun Lim, Hyeokjong Lee, Il-Seon Ha, Moo-Young Kim,\nIl-Jin Seo, Suho Lee, et al. 2020. “A Low-Power 65/14nm Stacked\nCMOS Image Sensor.” In 2020 IEEE International Symposium on\nCircuits and Systems (ISCAS), 1–4. IEEE.\n\n\nLabGuy’s World. 2014. “Goldmark 1 - A Field Sequential Color\nTV Project.” https://labguysworld.com/Goldmark1_001.htm.\n\n\nLam, Edmund Y. 2015. “Computational Photography with Plenoptic\nCamera and Light Field Capture: Tutorial.” Journal of the\nOptical Society of America A 32 (11): 2021–32.\n\n\nLamb, TD. 1980. “Spontaneous Quantal Events Induced in Toad Rods\nby Pigment Bleaching.” Nature 287 (5780): 349–51.\n\n\nLamb, TD, PA McNaughton, and K-W Yau. 1981. “Spatial Spread of\nActivation and Background Desensitization in Toad Rod Outer\nSegments.” The Journal of Physiology 319 (1): 463–96.\n\n\nLamb, TD, and Edward N Pugh Jr. 2004. “Dark Adaptation and the\nRetinoid Cycle of Vision.” Progress in Retinal and Eye\nResearch 23 (3): 307–80.\n\n\nLamb, Trevor D. 2013. “Evolution of Phototransduction, Vertebrate\nPhotoreceptors and Retina.” Progress in Retinal and Eye\nResearch 36:52–119.\n\n\n———. 2016. “Why Rods and Cones?” Eye 30 (2):\n179–85.\n\n\n———. 2020. “Evolution of the Genes Mediating Phototransduction in\nRod and Cone Photoreceptors.” Progress in Retinal and Eye\nResearch 76:100823.\n\n\n———. 2022. “Photoreceptor Physiology and Evolution: Cellular and\nMolecular Basis of Rod and Cone Phototransduction.” The\nJournal of Physiology 600 (21): 4585–4601.\n\n\nLamb, Trevor D, Shaun P Collin, and Edward N Pugh. 2007.\n“Evolution of the Vertebrate Eye: Opsins, Photoreceptors, Retina\nand Eye Cup.” Nature Reviews Neuroscience 8 (12):\n960–76.\n\n\nLamb, Trevor D, and Edward N Pugh. 2006. “Phototransduction, Dark\nAdaptation, and Rhodopsin Regeneration: The Proctor Lecture.”\nInvestigative Ophthalmology & Visual Science 47 (12):\n5138–52.\n\n\nLambert, Jean-Henri. 1760. Photometria Sive de Mensura Et Gradibus\nLuminis, Colorum Et Umbrae. Sumptibus viduae Eberhardi Klett, typis\nChristophori Petri Detleffsen.\n\n\nLanevski, Dmitri, Farshid Manoocheri, and Erkki Ikonen. 2022.\n“Gonioreflectometer for Measuring 3D Spectral BRDF of Horizontally\nAligned Samples with Traceability to SI.” Metrologia 59\n(2): 025006.\n\n\nLang, Karl. 2007. “Rendering the Print: The Art of\nPhotography.” Adobe System Technical Paper.\n\n\nLanman, Douglas, and David Luebke. 2013. “Near-Eye Light Field\nDisplays.” ACM Transactions on Graphics (TOG) 32 (6):\n1–10.\n\n\nLarimer, James, Carol M Cicerone, et al. 1974. “Opponent-Process\nAdditivity-i: Red/Green Equilibria.” Vision Research 14\n(11): 1127–40.\n\n\nLarimer, James, David H Krantz, and Carol M Cicerone. 1975.\n“Opponent Process Additivity—II. Yellow/Blue Equilibria and\nNonlinear Models.” Vision Research 15 (6): 723–31.\n\n\nLaValle, Steven M. 2023. Virtual Reality. Cambridge university\npress.\n\n\nLazzerini Ospri, Lorenzo, Glen Prusky, and Samer Hattar. 2017.\n“Mood, the Circadian System, and Melanopsin Retinal Ganglion\nCells.” Annual Review of Neuroscience 40 (1): 539–56.\n\n\nLee, Barry B. 2008. “The Evolution of Concepts of Color\nVision.” Neurociencias 4 (4): 209.\n\n\nLee, BB, PR Martin, and A Valberg. 1988. “The Physiological Basis\nof Heterochromatic Flicker Photometry Demonstrated in the Ganglion Cells\nof the Macaque Retina.” The Journal of Physiology 404\n(1): 323–47.\n\n\nLee, SH, SL Lee, and HY Kim. 1998. “Electro-Optic Characteristics\nand Switching Principle of a Nematic Liquid Crystal Cell Controlled by\nFringe-Field Switching.” Applied Physics Letters 73\n(20): 2881–83.\n\n\nLeek, Marjorie R. 2001. “Adaptive Procedures in Psychophysical\nResearch.” Perception & Psychophysics 63 (8):\n1279–92.\n\n\nLennie, Peter, Joel Pokorny, and Vivianne C Smith. 1993.\n“Luminance.” JOSA A 10 (6): 1283–93.\n\n\nLevoy, Marc. 1988. “Display of Surfaces from Volume Data.”\nIEEE Computer Graphics and Applications 8 (3): 29–37.\n\n\nLevoy, Marc, and Pat Hanrahan. 1996. “Light Field\nRendering.” In ACM Transactions on Graphics (ToG),\n31–42. ACM New York, NY, USA.\n\n\nLevoy, Marc, and Turner Whitted. 1985. “The Use of Points as a\nDisplay Primitive.” Technical Report; University of North\nCarolina at Chapel Hill.\n\n\nLi, Yang. 2003. “Ink-Paper Interaction-a Study in Ink-Jet Color\nReproduction.” Institute of\nTechnology-Linköpings University. Norrköping,\nSweden: UniTryck.\n\n\nLiao, Fuyou, Zheng Zhou, Beom Jin Kim, Jiewei Chen, Jingli Wang,\nTianqing Wan, Yue Zhou, et al. 2022. “Bioinspired in-Sensor Visual\nAdaptation for Accurate Perception.” Nature Electronics\n5 (2): 84–91.\n\n\nLippmann, Gabriel. 1908. “Epreuves Reversibles Donnant La\nSensation Du Relief.” J. Phys. Theor. Appl. 7 (1):\n821–25.\n\n\nLiu, Chiao, Lyle Bainbridge, Andrew Berkovich, Song Chen, Wei Gao,\nTsung-Hsun Tsai, Kazuya Mori, et al. 2020. “A 4.6 μm, 512× 512, Ultra-Low Power Stacked Digital Pixel\nSensor with Triple Quantization and 127dB Dynamic Range.” In\n2020 IEEE International Electron Devices Meeting (IEDM), 16–11.\nIEEE.\n\n\nLiu, Chiao, Andrew Berkovich, Song Chen, Hans Reyserhove, Syed Shakib\nSarwar, and Tsung-Hsun Tsai. 2019. “Intelligent Vision\nSystems–Bringing Human-Machine Interface to AR/VR.” In 2019\nIEEE International Electron Devices Meeting (IEDM), 10–15. IEEE.\n\n\nLiu, Chiao, Song Chen, Tsung-Hsun Tsai, Barbara De Salvo, and Jorge\nGomez. 2022. “Augmented Reality-the Next Frontier of Image Sensors\nand Compute Systems.” In 2022 IEEE International Solid-State\nCircuits Conference (ISSCC), 65:426–28. IEEE.\n\n\nMa, Ruiqing. 2016. “Active Matrix for OLED Displays.” In\nHandbook of Visual Display Technology, 2nd ed., 1821–41.\nSpringer.\n\n\nMa, Tianrui. 2024. “Efficient Data-Driven Machine Vision: A\nCo-Design of Circuit, Algorithm, and Architecture for Edge Vision\nSensors.” PhD thesis, Washington University in St. Louis.\n\n\nMa, Tianrui, Yu Feng, Xuan Zhang, and Yuhao Zhu. 2023. “Camj:\nEnabling System-Level Energy Modeling and Architectural Exploration for\nin-Sensor Visual Computing.” In Proceedings of the 50th\nAnnual International Symposium on Computer Architecture, 1–14.\n\n\nMacAdam, David L. 1942. “Visual Sensitivities to Color Differences\nin Daylight.” Josa 32 (5): 247–74.\n\n\n———. 1943. “Specification of Small Chromaticity\nDifferences.” Josa 33 (1): 18–26.\n\n\nMacLeod, Donald IA, and Peter Lennie. 1976. “Red-Green Blindness\nConfined to One Eye.” Vision Research 16 (7): 691–702.\n\n\nMantiuk, Rafał K, Maliha Ashraf, and Alexandre Chapiro. 2022.\n“stelaCSF: A Unified Model of Contrast Sensitivity as the Function\nof Spatio-Temporal Frequency, Eccentricity, Luminance and Area.”\nACM Transactions on Graphics (TOG) 41 (4): 1–16.\n\n\nMantiuk, Rafał, Grzegorz Krawczyk, Dorota Zdrojewska, Radosław Mantiuk,\nKarol Myszkowski, and Hans-Peter Seidel. 2015. “High Dynamic Range\nImaging.” In Wiley Encyclopedia of Electrical and Electronics\nEngineering. Wiley.\n\n\nMarco Polo. 2007. “CIE1931 RGB CMF; released\ninto the public domain by the copyright holder.” https://commons.wikimedia.org/wiki/File:CIE1931_RGBCMF.svg.\n\n\nMarks, WB, William H Dobelle, and Edward F MacNichol Jr. 1964.\n“Visual Pigments of Single Primate Cones.” Science\n143 (3611): 1181–83.\n\n\nMarschner, Stephen R, Stephen H Westin, Eric PF Lafortune, and Kenneth E\nTorrance. 2000. “Image-Based Bidirectional Reflectance\nDistribution Function Measurement.” Applied Optics 39\n(16): 2592–2600.\n\n\nMashford, Benjamin S, Matthew Stevenson, Zoran Popovic, Charles\nHamilton, Zhaoqun Zhou, Craig Breen, Jonathan Steckel, et al. 2013.\n“High-Efficiency Quantum-Dot Light-Emitting Devices with Enhanced\nCharge Injection.” Nature Photonics 7 (5): 407–12.\n\n\nMathiasRav. 2009. “Volume rendering of a\nmouse skull (CT) using shear warp algorithm; CC BY-SA 3.0\nlicense.” https://en.wikipedia.org/wiki/File:VolRenderShearWarp.gif.\n\n\nMatthews, HR, RLW Murphy, GL Fain, and TD Lamb. 1988.\n“Photoreceptor Light Adaptation Is Mediated by Cytoplasmic Calcium\nConcentration.” Nature 334 (6177): 67–69.\n\n\nMatusik, Wojciech. 2003. “A Data-Driven Reflectance Model.”\nPhD thesis, Massachusetts Institute of Technology.\n\n\nMax, Nelson. 1995. “Optical Models for Direct Volume\nRendering.” IEEE Transactions on Visualization and Computer\nGraphics 1 (2): 99–108.\n\n\nMaxwell, James Clerk. 1857. “XVIII.—Experiments on Colour, as\nPerceived by the Eye, with Remarks on Colour-Blindness.”\nEarth and Environmental Science Transactions of the Royal Society of\nEdinburgh 21 (2): 275–98.\n\n\nMayerhöfer, Thomas G, Susanne Pahlow, and Jürgen Popp. 2020. “The\nBouguer-Beer-Lambert Law: Shining Light on the Obscure.”\nChemPhysChem 21 (18): 2029–46.\n\n\nMdf. 2005. “A simulation of spherical\naberration in an optical system with a circular, unobstructed aperture\nadmitting a monochromatic point source; released into the public domain\nby the copyright holder.” https://commons.wikimedia.org/wiki/File:Spherical-aberration-disk.jpg.\n\n\n———. 2006. “Imaging as a convolution against\nthe PSF; released into the public domain by the copyright\nholder.” https://commons.wikimedia.org/wiki/File:Convolution_Illustrated_eng.png.\n\n\nMelbourne, William G. 2004. Radio Occultations Using Earth\nSatellites: A Wave Theory Treatment. 1st ed. John Wiley & Sons.\n\n\nMelentijevic. 2015. “DSLR Internal Cut Filter / Lowpass\nFilter / Hot Mirror Transmission Curves.” https://kolarivision.com/articles/internal-cut-filter-transmission/.\n\n\nMeyer, Gary W, and Donald P Greenberg. 1988. “Color-Defective\nVision and Computer Graphics Displays.” IEEE Computer\nGraphics and Applications 8 (5): 28–40.\n\n\nMglg. 2008. “Conceptual ray diagrams of\nspherically aberrated lenses; released into the public domain by the\ncopyright holder.” https://commons.wikimedia.org/wiki/File:Spherical_aberration_2.svg.\n\n\nMichael Schmid. 2008. “Graphic illustratic\nthe astigmatism phenomenon; CC BY-SA 3.0 license.” https://commons.wikimedia.org/wiki/File:Meridional%2BSaggittalEbene_1.svg.\n\n\nMildenhall, Ben, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron,\nRavi Ramamoorthi, and Ren Ng. 2021. “Nerf: Representing Scenes as\nNeural Radiance Fields for View Synthesis.” Communications of\nthe ACM 65 (1): 99–106.\n\n\nMilić, Neda, Dragoljub Novaković, and Branko Milosavljević. 2015.\n“Enhancement of Image Content for Observers with Colour Vision\nDeficiencies.” Color Image and Video Enhancement,\n315–43.\n\n\nMiller, Michael E. 2019. Color in Electronic Display Systems.\nSpringer.\n\n\nMills, Allan A, and R Clift. 1992. “Reflections of the’burning\nMirrors of Archimedes’. With a Consideration of the Geometry and\nIntensity of Sunlight Reflected from Plane Mirrors.” European\nJournal of Physics 13 (6): 268.\n\n\nMilo, Ron, and Rob Phillips. 2015. Cell Biology by the Numbers.\nGarland Science.\n\n\nMishkin, Mortimer, Leslie G Ungerleider, and Kathleen A Macko. 1983.\n“Object Vision and Spatial Vision: Two Cortical Pathways.”\nTrends in Neurosciences 6:414–17.\n\n\nMiyauchi, Ken, Kazuya Mori, Toshinori Otaka, Toshiyuki Isozaki, Naoto\nYasuda, Alex Tsai, Yusuke Sawai, Hideki Owada, Isao Takayanagi, and\nJunichi Nakamura. 2020. “A Stacked Back Side-Illuminated Voltage\nDomain Global Shutter CMOS Image Sensor with a 4.0 μm Multiple Gain Readout\nPixel.” Sensors 20 (2): 486.\n\n\nMollon, John D. 1982. “Colour Vision and Colour Blindness.”\nIn The Senses, edited by Horace B Barlow and John D Mollon,\n165–91. Cambridge University Press.\n\n\n———. 2003. “Introduction: Thomas Young and the Trichromatic Theory\nof Colour Vision.” In Normal & Defective Colour\nVision, edited by John D Mollon, Joel Pokorny, and Ken Knoblauch,\n19--34. Oxford University Press.\n\n\nMorovič, Ján. 2008. Color Gamut Mapping. 2nd ed. John Wiley\n& Sons.\n\n\nMorshedian, Ala, and Gordon L Fain. 2015. “Single-Photon\nSensitivity of Lamprey Rods with Cone-Like Outer Segments.”\nCurrent Biology 25 (4): 484–87.\n\n\nMurakami, Hirotaka, Eric Bohannon, John Childs, Grace Gui, Eric Moule,\nKatsuhiko Hanzawa, Tomofumi Koda, et al. 2022. “A 4.9 Mpixel\nProgrammable-Resolution Multi-Purpose CMOS Image Sensor for Computer\nVision.” In 2022 IEEE International Solid-State Circuits\nConference (ISSCC), 65:104–6. IEEE.\n\n\nMuratore, Dante G, and EJ Chichilnisky. 2020. “Artificial Retina:\nA Future Cellular-Resolution Brain-Machine Interface.” In\nNANO-CHIPS 2030: On-Chip AI for an Efficient Data-Driven World,\n443–65. Springer.\n\n\nMurmann, Boris. 2014. “ADC Performance Survey\n1997-2024.” https://github.com/bmurmann/ADC-survey.\n\n\nMyndex. 2022. “A comparison of RGB gamuts of\nsRGB, P3, Rec2020, etc. using the CIE1931 chromaticity diagram; CC BY-SA\n4.0 license.” https://commons.wikimedia.org/wiki/File:CIE1931xy_gamut_comparison_of_sRGB_P3_Rec2020.svg.\n\n\nNaka, KI, and William AH Rushton. 1966. “S-Potentials from Colour\nUnits in the Retina of Fish (Cyprinidae).” The Journal of\nPhysiology 185 (3): 536–55.\n\n\nNakamura, Junichi. 2006. Image Sensors and Signal Processing for\nDigital Still Cameras. CRC press.\n\n\nNakatani, K, and KW Yau. 1988. “Calcium and Light Adaptation in\nRetinal Rods and Cones.” Nature 334 (6177): 69–71.\n\n\nNassi, Jonathan J, and Edward M Callaway. 2009. “Parallel\nProcessing Strategies of the Primate Visual System.” Nature\nReviews Neuroscience 10 (5): 360–72.\n\n\nNathans, Jeremy. 1992. “Rhodopsin: Structure, Function, and\nGenetics.” Biochemistry 31 (21): 4923–31.\n\n\nNathans, Jeremy, Darcy Thomas, and David S Hogness. 1986.\n“Molecular Genetics of Human Color Vision: The Genes Encoding\nBlue, Green, and Red Pigments.” Science 232 (4747):\n193–202.\n\n\nNelson, Philip. 2017. “From Photon to Neuron: Light, Imaging,\nVision.”\n\n\nNewton, Isaac. 1704. Opticks, or, a Treatise of the Reflections,\nRefractions, Inflections & Colours of Light. London: Smith;\nWalford.\n\n\nNg, Ren. 2006. “Digital Light Field Photography.” PhD\nthesis, Stanford University.\n\n\nNicodemus, FE, JC Richmond, JJ Hsia, IW Ginsberg, and T Limperis. 1977.\nGeometrical Considerations and Nomenclature for Reflectance.\nVol. 160. US Department of Commerce, National Bureau of Standards\nWashington, DC, USA.\n\n\nNikonov, S, TD Lamb, and EN Pugh Jr. 2000. “The Role of Steady\nPhosphodiesterase Activity in the Kinetics and Sensitivity of the\nLight-Adapted Salamander Rod Photoresponse.” The Journal of\nGeneral Physiology 116 (6): 795.\n\n\nNitta, Yoshikazu, Yoshinori Muramatsu, Kiyotaka Amano, Takayuki Toyama,\nK Mishina, Atsushi Suzuki, Tadayuki Taura, et al. 2006.\n“High-Speed Digital Double Sampling with Analog CDS on Column\nParallel ADC Architecture for Low-Noise Active Pixel Sensor.” In\n2006 IEEE International Solid State Circuits Conference-Digest of\nTechnical Papers, 2024–31. IEEE.\n\n\nNoble, Peter JW. 1968. “Self-Scanned Silicon Image Detector\nArrays.” IEEE Transactions on Electron Devices 15 (4):\n202–9.\n\n\nNormann, Richard A, and I Perlman. 1979. “The Effects of\nBackground Illumination on the Photoresponses of Red and Green\nCones.” The Journal of Physiology 286 (1): 491–507.\n\n\nNorren, Dirk V, and Johannes J Vos. 1974. “Spectral Transmission\nof the Human Ocular Media.” Vision Research 14 (11):\n1237–44.\n\n\nNovák, Jan, Iliyan Georgiev, Johannes Hanika, Jaroslav Křivánek, and\nWojciech Jarosz. 2018. “Monte Carlo\nMethods for Physically Based Volume Rendering.” In ACM\nSIGGRAPH 2018 Courses.\n\n\nNumFOCUS. n.d. “Colour: open-source Python\npackage for colour science.” https://github.com/colour-science/colour.\n\n\nO’Connor, Daniel H, Miki M Fukui, Mark A Pinsk, and Sabine Kastner.\n2002. “Attention Modulates Responses in the Human Lateral\nGeniculate Nucleus.” Nature Neuroscience 5 (11): 1203–9.\n\n\nOhta, Jun. 2020. Smart CMOS Image Sensors and Applications. CRC\npress.\n\n\nOkano, Toshiyuki, DAISUKE KoJIMA, Yoshitaka Fukada, Yoshinori Shichida,\nand Toru Yoshizawa. 1992. “Primary Structures of Chicken Cone\nVisual Pigments: Vertebrate Rhodopsins Have Evolved Out of Cone Visual\nPigments.” Proceedings of the National Academy of\nSciences 89 (13): 5932–36.\n\n\nOkano, Toshiyuki, Toru Yoshizawa, and Yoshitaka Fukada. 1994.\n“Pinopsin Is a Chicken Pineal Photoreceptive Molecule.”\nNature 372 (6501): 94–97.\n\n\nOmmnomnomgulp. 2008. “A focal plane shutter\nfiring at 1/500 of a second with the ‘gap’ clearly visible.\nThis shutter is on a Nikon film SLR. CC BY-SA 3.0\nlicense.” https://commons.wikimedia.org/wiki/File:1_500_Sec_Focal_P_Shut.jpg.\n\n\nOmniVision. n.d. “OD6631 AMOLED Driver Brief.”\n\n\nOnishi, Akishi, Satoshi Koike, Miki Ida-Hosonuma, Hiroo Imai, Yoshinori\nShichida, Osamu Takenaka, Akitoshi Hanazawa, et al. 2002.\n“Variations in Long-and Middle-Wavelength-Sensitive Opsin Gene\nLoci in Crab-Eating Monkeys.” Vision Research 42 (3):\n281–92.\n\n\nOren, Michael, and Shree K Nayar. 1995. “Generalization of the\nLambertian Model and Implications for Machine Vision.”\nInternational Journal of Computer Vision 14:227–51.\n\n\nPAR. 2012. “Plankian locus; released into the\npublic domain by the copyright holder.” https://commons.wikimedia.org/wiki/File:PlanckianLocus.png.\n\n\nPatney, Anjul, Marco Salvi, Joohwan Kim, Anton Kaplanyan, Chris Wyman,\nNir Benty, David Luebke, and Aaron Lefohn. 2016. “Towards Foveated\nRendering for Gaze-Tracked Virtual Reality.” ACM Transactions\non Graphics (TOG) 35 (6): 1–12.\n\n\nPatterson, EJ, RR Mastey, JA Kuchenbecker, Jessica Rowlan, Jay Neitz,\nMaureen Neitz, and Joseph Carroll. 2022. “Effects of\nColor-Enhancing Glasses on Color Vision in Congenital Red-Green Color\nDeficiencies.” Optics Express 30 (17): 31182–94.\n\n\nPfister, Hanspeter, Matthias Zwicker, Jeroen Van Baar, and Markus Gross.\n2000. “Surfels: Surface Elements as Rendering Primitives.”\nIn Proceedings of the 27th Annual Conference on Computer Graphics\nand Interactive Techniques, 335–42.\n\n\nPharr, Matt, Wenzel Jakob, and Greg Humphreys. 2018. Physically\nBased Rendering: From Theory to Implementation. 3rd ed. MIT Press.\n\n\n———. 2023. Physically Based Rendering: From Theory to\nImplementation. 4th ed. MIT Press.\n\n\nPoirson, Allen B, and Brian A Wandell. 1993. “Appearance of\nColored Patterns: Pattern–Color Separability.” JOSA A 10\n(12): 2458–70.\n\n\nPolyak, Stephen Lucian. 1941. The Retina. Univ. Chicago Press.\n\n\nPrabhu B Doss. 2007. “Tso Kiagar Lake Ladakh;\nCC BY 2.0 license.” https://commons.wikimedia.org/wiki/File:Tso_Kiagar_Lake_Ladakh.jpg.\n\n\nProkudin-Gorsky, Sergey. 1948. “Library of\nCongress Prokudin-Gorskii Collection.” https://www.loc.gov/collections/prokudin-gorskii/about-this-collection/.\n\n\nPugh Jr, EN, S Nikonov, and TD Lamb. 1999. “Molecular Mechanisms\nof Vertebrate Photoreceptor Light Adaptation.” Current\nOpinion in Neurobiology 9 (4): 410–18.\n\n\nPurves, Dale, George J. Augustine, David Fitzpatrick, William Hall,\nAnthony-Samuel LaMantia, Richard D. Mooney, Michael L. Platt, and\nLeonard E. White. 2017. Neurosciences. 6th ed. Oxford\nUniversity Press.\n\n\nQian, Lei, Ying Zheng, Jiangeng Xue, and Paul H Holloway. 2011.\n“Stable and Efficient Quantum-Dot Light-Emitting Diodes Based on\nSolution-Processed Multilayer Structures.” Nature\nPhotonics 5 (9): 543–48.\n\n\nRabal, Ana M, Alejandro Ferrero, Joaquın Campos, José Luis Fontecha,\nAlicia Pons, Antonio Manuel Rubiño, and Antonio Corróns. 2012.\n“Automatic Gonio-Spectrophotometer for the Absolute Measurement of\nthe Spectral BRDF at in-and Out-of-Plane and Retroreflection\nGeometries.” Metrologia 49 (3): 213.\n\n\nRamamoorthi, Ravi. 2009. “Precomputation-Based Rendering.”\nFoundations and Trends in Computer Graphics and\nVision 3 (4): 281–369.\n\n\nRamanath, Rajeev, Wesley E Snyder, Youngjun Yoo, and Mark S Drew. 2005.\n“Color Image Processing Pipeline.” IEEE Signal\nProcessing Magazine 22 (1): 34–43.\n\n\nRaydium. 2011. “RM68090 Data Sheet: Single\nChip Driver with 262K color for 240RGBx320 a-Si TFT LCD.”\n\n\nRayleigh, Lord. 1879. “XXXI. Investigations in Optics, with\nSpecial Reference to the Spectroscope.” The London,\nEdinburgh, and Dublin Philosophical Magazine and Journal of Science\n8 (49): 261–74.\n\n\nReinhard, Erik. 2010. High Dynamic Range Imaging Acquisition,\nDisplay, and Image-Based Lighting. 2nd ed. Morgan Kaufmann\nPublishers.\n\n\nReinhard, Erik, Erum Arif Khan, Ahmet Oguz Akyuz, and Garrett Johnson.\n2008. Color Imaging: Fundamentals and Applications. CRC Press.\n\n\nReppert, Steven M, Haisun Zhu, and Richard H White. 2004.\n“Polarized Light Helps Monarch Butterflies Navigate.”\nCurrent Biology 14 (2): 155–58.\n\n\nRieke, F, and Denis A Baylor. 1996. “Molecular Origin of\nContinuous Dark Noise in Rod Photoreceptors.” Biophysical\nJournal 71 (5): 2553–72.\n\n\nRieke, Foster, and Denis A Baylor. 1998. “Single-Photon Detection\nby Rod Cells of the Retina.” Reviews of Modern Physics\n70 (3): 1027.\n\n\nRieke, Fred, and Denis A Baylor. 2000. “Origin and Functional\nImpact of Dark Noise in Retinal Cones.” Neuron 26 (1):\n181–86.\n\n\nRodieck, Robert W. 1998. The First Steps in Seeing. Sinauer\nAssociates.\n\n\nRodriguez-Pardo, Carlos Eduardo, and Gaurav Sharma. 2011.\n“Dichromatic Color Perception in a Two Stage Model: Testing for\nCone Replacement and Cone Loss Models.” In 2011 IEEE 10th\nIVMSP Workshop: Perception and Visual Signal Analysis, 12–17. IEEE.\n\n\nRose, Albert. 1948. “The Sensitivity Performance of the Human Eye\non an Absolute Scale.” Journal of the Optical Society of\nAmerica 38 (2): 196–208.\n\n\nRowlands, D Andrew. 2020a. “Color Conversion Matrices in Digital\nCameras: A Tutorial.” Optical Engineering 59 (11):\n110801–1.\n\n\n———. 2020b. Physics of Digital Photography. 2nd ed. IOP\nPublishing.\n\n\nRushton, William AH. 1972. “Visual Pigments in Man.” In\nPhotochemistry of Vision, edited by Herbert J. A. Dartnall,\n364–94. Springer Berlin Heidelberg.\n\n\nRushton, William Albert Hugh. 1961. “Rhodopsin Measurement and\nDark-Adaptation in a Subject Deficient in Cone Vision.” The\nJournal of Physiology 156 (1): 193.\n\n\n———. 1963. “Cone Pigment Kinetics in the Protanope.”\nThe Journal of Physiology 168 (2): 374.\n\n\n———. 1965. “The Ferrier Lecture, 1962 Visual Adaptation.”\nProceedings of the Royal Society of London. Series B. Biological\nSciences 162 (986): 20–46.\n\n\nRushton, WilliamA H. 1972. “Review Lecture. Pigments and Signals\nin Colour Vision.” The Journal of Physiology 220 (3):\n1P.\n\n\nSabella, Paolo. 1988. “A Rendering Algorithm for Visualizing 3D\nScalar Fields.” ACM SIGGRAPH Computer Graphics 22 (4):\n51–58.\n\n\nSakakibara, Masaki, Yusuke Oike, Takafumi Takatsuka, Akihiko Kato,\nKatsumi Honda, Tadayuki Taura, Takashi Machida, et al. 2012. “An\n83dB-Dynamic-Range Single-Exposure Global-Shutter CMOS Image Sensor with\nin-Pixel Dual Storage.” In 2012 IEEE International\nSolid-State Circuits Conference, 380–82. IEEE.\n\n\nSakmann, Bert, and Otto D Creutzfeldt. 1969. “Scotopic and Mesopic\nLight Adaptation in the Cat’s Retina.” Pflügers\nArchiv 313:168–85.\n\n\nSakurambo. 2007. “A computer-generated image\nof an Airy disk (the grayscale intensities have been adjusted to enhance\nthe brightness of the outer rings of the Airy pattern); released into\nthe public domain by the copyright holder.” https://commons.wikimedia.org/wiki/File:Airy-pattern.svg.\n\n\nSato, Keita, Takahiro Yamashita, Keiichi Kojima, Kazumi Sakai, Yuki\nMatsutani, Masataka Yanagawa, Yumiko Yamano, et al. 2018.\n“Pinopsin Evolved as the Ancestral Dim-Light Visual Opsin in\nVertebrates.” Communications Biology 1 (1): 156.\n\n\nSaunderson, JL. 1942. “Calculation of the Color of Pigmented\nPlastics.” JOSA 32 (12): 727–36.\n\n\nSchnapf, JL, TW Kraft, and Denis A Baylor. 1987. “Spectral\nSensitivity of Human Cone Photoreceptors.” Nature 325\n(6103): 439–41.\n\n\nSchnapf, JL, BJ Nunn, M Meister, and Denis A Baylor. 1990. “Visual\nTransduction in Cones of the Monkey Macaca Fascicularis.” The\nJournal of Physiology 427 (1): 681–713.\n\n\nSchneeweis, David M, and Julie L Schnapf. 1995. “Photovoltage of\nRods and Cones in the Macaque Retina.” Science 268\n(5213): 1053–56.\n\n\n———. 1999. “The Photovoltage of Macaque Cone Photoreceptors:\nAdaptation, Noise, and Kinetics.” Journal of\nNeuroscience 19 (4): 1203–16.\n\n\nSchrödinger, Erwin. 1925. “Über Das\nVerhältnis Der Vierfarben-Zur Dreifarbentheorie.”\nSitzungberichte Abt 2a, Mathematik, Astronomie, Physik, Meteorologie\nUnd Mechanik Akademie Der Wissenschaften in Wien,\nMathematisch-Naturwissenschaftliche Klasse 134:471–90.\n\n\n———. 1994. “On the Relationship of Four-Color Theory to\nThree-Color Theory (Translation by National Translation Center;\nOriginally Published in 1925).” Color Research and\nApplication 19 (1): 37.\n\n\nSchubert, E Fred. 2006. Light-Emitting Diodes. 2nd ed.\nCambridge University Press.\n\n\nSchweiger, Martin, SR Arridge, M Hiraoka, and DT Delpy. 1995. “The\nFinite Element Method for the Propagation of Light in Scattering Media:\nBoundary and Source Conditions.” Medical Physics 22\n(11): 1779–92.\n\n\nSelket. 2007. “The ventral vs. dorsal stream;\nCC BY-SA 3.0.” https://commons.wikimedia.org/wiki/File:Ventral-dorsal_streams.svg.\n\n\nService, Phil. 2016. “The Wright – Guild Experiments and the\nDevelopment of the CIE 1931 RGB and XYZ Color Spaces.”\n\n\nShannon, Claude Elwood. 1949. “Communication in the Presence of\nNoise.” Proceedings of the IRE 37 (1): 10–21.\n\n\nSharkD. 2010a. “HSL color solid cylinder; CC\nBY-SA 3.0 license.” https://commons.wikimedia.org/wiki/File:HSL_color_solid_cylinder_saturation_gray.png.\n\n\n———. 2010b. “RGB Color Cube; CC BY-SA 3.0\nlicense.” https://commons.wikimedia.org/wiki/File:RGB_Cube_Show_lowgamma_cutout_a.png.\n\n\nSharma, Abhay. 2018. Understanding Color Management. John Wiley\n& Sons.\n\n\nSharma, Gaurav. 2003. “Color Fundamentals for Digital\nImaging.” In Digital Color Imaging Handbook, edited by\nGaurav Sharma, 14–127. CRC Press.\n\n\nSharma, Gaurav, Wencheng Wu, and Edul N Dalal. 2005. “The\nCIEDE2000 Color-Difference Formula: Implementation Notes, Supplementary\nTest Data, and Mathematical Observations.” Color Research\n& Application 30 (1): 21–30.\n\n\nSharpe, Lindsay T, Andrew Stockman, Wolfgang Jagla, and Herbert Jägle.\n2005. “A Luminous Efficiency Function, v*(λ), for Daylight Adaptation.”\nJournal of Vision 5 (11): 3–3.\n\n\n———. 2011. “A Luminous Efficiency Function, VD65*(λ), for Daylight Adaptation: A\nCorrection.” Color Research & Application 36 (1):\n42–46.\n\n\nSharpe, Lindsay T, Andrew Stockman, Herbert Jägle, and Jeremy Nathans.\n1999. “Opsin Genes, Cone Photopigments, Color Vision, and Color\nBlindness.” Color Vision: From Genes to Perception\n351:3–52.\n\n\nSherman, SM, and Christof Koch. 1986. “The Control of\nRetinogeniculate Transmission in the Mammalian Lateral Geniculate\nNucleus.” Experimental Brain Research 63:1–20.\n\n\nShevell, Steven K. 1977. “Saturation in Human Cones.”\nVision Research 17 (3): 427–34.\n\n\nShevell, Steven K, and Paul R Martin. 2017. “Color Opponency:\nTutorial.” JOSA A 34 (7): 1099–1108.\n\n\nShichida, Yoshinori, and Take Matsuyama. 2009. “Evolution of\nOpsins and Phototransduction.” Philosophical Transactions of\nthe Royal Society B: Biological Sciences 364 (1531): 2881–95.\n\n\nShirasaki, Yasuhiro, Geoffrey J Supran, Moungi G Bawendi, and Vladimir\nBulović. 2013. “Emergence of Colloidal Quantum-Dot Light-Emitting\nTechnologies.” Nature Photonics 7 (1): 13–23.\n\n\nShockley, William. 1949. “The Theory of p-n Junctions in\nSemiconductors and p-n Junction Transistors.” Bell System\nTechnical Journal 28 (3): 435–89.\n\n\nSimunovic, MP. 2010. “Colour Vision Deficiency.”\nEye 24 (5): 747–55.\n\n\nSjschen. 2025. “Volume rendered CT scan of a\nforearm with different color schemes for muscle, fat, bone, and blood;\nreleased into the public domain by the copyright holder.”\nhttps://commons.wikimedia.org/wiki/File:CTWristImage.png.\n\n\nSloan, Louise L, and Lorraine Wollach. 1948. “A Case of Unilateral\nDeuteranopia.” JOSA 38 (6): 502–9.\n\n\nSmith, Alvy Ray. 1995. “Alpha and the History of Digital\nCompositing.” Citeseer.\n\n\nSmith, Vivianne C, and Joel Pokorny. 1975. “Spectral Sensitivity\nof the Foveal Cone Photopigments Between 400 and 500 Nm.”\nVision Research 15 (2): 161–71.\n\n\nSnodderly, DM, PK Brown, FC Delori, and JD Auran. 1984. “The\nMacular Pigment. I. Absorbance Spectra, Localization, and Discrimination\nfrom Other Yellow Pigments in Primate Retinas.” Investigative\nOphthalmology & Visual Science 25 (6): 660–73.\n\n\nSolhusvik, Johannes, T Willassent, Sindre Mikkelsen, Mathias Wilhelmsen,\nSohei Manabe, Duli Mao, Zhaoyu He, Keiji Mabuchi, and TA Hasegawa. 2019.\n“1280× 960 2.8 μm HDR CIS with DCG and Split-Pixel\nCombined.” In Proceedings of the International Image Sensor\nWorkshop (IISW), Snowbird, UT, USA, 23–27.\n\n\nStam, Jos. 1995. “Multiple Scattering as a Diffusion\nProcess.” In Rendering Techniques’ 95: Proceedings of the\nEurographics Workshop in Dublin, Ireland, June 12–14, 1995 6,\n41–50. Springer.\n\n\nStark, Laurence, Jeffrey M Raynor, Frederic Lalanne, and Robert K\nHenderson. 2018. “A Back-Illuminated Voltage-Domain Global Shutter\nPixel with Dual in-Pixel Storage.” IEEE Transactions on\nElectron Devices 65 (10): 4394–4400.\n\n\nSteve Fareham. 2007. “Heart of the City water\nfeature Sheffield; CC BY-SA 2.0 license.” https://commons.wikimedia.org/wiki/File:Heart_of_the_City_water_feature_Sheffield_-_geograph.org.uk_-_618552.jpg.\n\n\nStevens, Stanley S. 1957. “On the Psychophysical Law.”\nPsychological Review 64 (3): 153.\n\n\nStevens, Stanley Smith. 1961. “To Honor Fechner and Repeal His\nLaw: A Power Function, Not a Log Function, Describes the Operating\nCharacteristic of a Sensory System.” Science 133 (3446):\n80–86.\n\n\nStiles, Walter Stanley, and Jennifer M Burch. 1959. “NPL\nColour-Matching Investigation: Final Report (1958).” Optica\nActa: International Journal of Optics 6 (1): 1–26.\n\n\nStiles, WS. 1939. “The Directional Sensitivity of the Retina and\nthe Spectral Sensitivities of the Rods and Cones.”\nProceedings of the Royal Society of London. Series B-Biological\nSciences 127 (846): 64–105.\n\n\n———. 1959. “Color Vision: The Approach Through Increment-Threshold\nSensitivity.” Proceedings of the National Academy of\nSciences 45 (1): 100–114.\n\n\n———. 1964. “Appendix by WS Stiles: Foveal Threshold Sensitivity on\nFields of Different Colors.” Science 145 (3636):\n1016–17.\n\n\nStiles, WS, and JM Burch. 1955. “Interim Report to the Commission\nInternationale de l’eclairage, Zurich, 1955, on the National Physical\nLaboratory’s Investigation of Colour-Matching (1955).” Optica\nActa: International Journal of Optics 2 (4): 168–81.\n\n\nStockman, Andrew, and Lindsay T Sharpe. 2000. “The Spectral\nSensitivities of the Middle-and Long-Wavelength-Sensitive Cones Derived\nfrom Measurements in Observers of Known Genotype.” Vision\nResearch 40 (13): 1711–37.\n\n\nStockman, Andrew, Lindsay T Sharpe, and Clemens Fach. 1999. “The\nSpectral Sensitivity of the Human Short-Wavelength Sensitive Cones\nDerived from Thresholds and Color Matches.” Vision\nResearch 39 (17): 2901–27.\n\n\nStokes, Anderson, Chandrasekar, and Motta. 1996. “A Standard Default Color Space for the Internet -\nsRGB.” https://www.w3.org/Graphics/Color/sRGB.\n\n\nStoppa, David, Andrea Simoni, Lorenzo Gonzo, Massimo Gottardi, and G-F\nDalla Betta. 2002. “Novel CMOS Image Sensor with a 132-dB Dynamic\nRange.” IEEE Journal of Solid-State Circuits 37 (12):\n1846–52.\n\n\nSugawa, Shigetoshi, Nana Akahane, Satoru Adachi, Kazuya Mori, Toshiyuki\nIshiuchi, and Koichi Mizobuchi. 2005. “A 100 dB Dynamic Range CMOS\nImage Sensor Using a Lateral Overflow Integration Capacitor.” In\nISSCC. 2005 IEEE International Digest of Technical Papers.\nSolid-State Circuits Conference, 2005., 352–603. IEEE.\n\n\nSvaetichin, G. 1953. “The Cone Action Potential.” Acta\nPhysiol Scand 29 29:565–600.\n\n\nSvaetichin, G. 1956. “Spectral Response Curves from Single\nCones.” Acta Physiol Scand 39:17–46.\n\n\nSvaetichin, G., and Edward F MacNichol Jr. 1958. “Retinal\nMechanisms for Chromatic and Achromatic Vision.” Annals of\nthe New York Academy of Sciences 74 (2): 385–404.\n\n\nSwain, PK, and David Cheskis. 2008. “Back-Illuminated Image\nSensors Come to the Forefront.” Photonics Spectra 42\n(8): 46.\n\n\nTakanaka, Yoko, Toshiyuki Okano, Masayuki Iigo, and Yoshitaka Fukada.\n1998. “Light-Dependent Expression of Pinopsin Gene in Chicken\nPineal Gland.” Journal of Neurochemistry 70 (3): 908–13.\n\n\nTakayanagi, Isao, Ken Miyauchi, Shunsuke Okura, Kazuya Mori, Junichi\nNakamura, and Shigetoshi Sugawa. 2019. “A 120-Ke- Full-Well\nCapacity 160-μv/e- Conversion\nGain 2.8-μm\nBackside-Illuminated Pixel with a Lateral Overflow Integration\nCapacitor.” Sensors 19 (24): 5572.\n\n\nTakayanagi, Isao, Norio Yoshimura, Kazuya Mori, Shinichiro Matsuo,\nShunsuke Tanaka, Hirofumi Abe, Naoto Yasuda, et al. 2018. “An over\n90 dB Intra-Scene Single-Exposure Dynamic Range CMOS Image Sensor Using\na 3.0 μm Triple-Gain Pixel\nFabricated in a Standard BSI Process.” Sensors 18 (1):\n203.\n\n\nTallfred. 2005. “Text blurred by different\nfocal positions of an astigmatic lens; 3-clause BSD\nLicense.” https://commons.wikimedia.org/wiki/File:Astigmatism_text_blur.png.\n\n\nTamura, Tarnura, K Nakatani, and KW Yau. 1991. “Calcium Feedback\nand Sensitivity Regulation in Primate Rods.” The Journal of\nGeneral Physiology 98 (1): 95–130.\n\n\nTeranishi, Nobukazu. 2015. “Effect and Limitation of Pinned\nPhotodiode.” IEEE Transactions on Electron Devices 63\n(1): 10–15.\n\n\nTeranishi, Nobukazu, Akiyoshi Kohono, Yasuo Ishihara, Eiji Oda, and\nKouichi Arai. 1982. “No Image Lag Photodiode Structure in the\nInterline CCD Image Sensor.” In 1982 International Electron\nDevices Meeting, 324–27. IEEE.\n\n\nThompson, William, Roland Fleming, Sarah Creem-Regehr, and Jeanine Kelly\nStefanucci. 2011. Visual Perception from a Computer Graphics\nPerspective. CRC press.\n\n\nThomson, Stuart. 2018. “Determination of\nChromaticity Coordinates and Bandgaps of III-V LEDs Using\nElectroluminescence Spectroscopy.” https://www.edinst.com/wp-content/uploads/2018/08/Electroluminescence-spectroscopy-of-LEDs.pdf.\n\n\nThorseth. 2015. “Spectral power distribution\nof a 25 W incandescent light bulb; CC BY-SA 4.0 license.”\nhttps://commons.wikimedia.org/wiki/File:Spectral_power_distribution_of_a_25_W_incandescent_light_bulb.png.\n\n\nThycoop/photographs. 2022. “Small park in\nlogatec made with matchbox pinhole camera on kodak portra 400 film; CC\nBY-SA 4.0 license.” https://commons.wikimedia.org/wiki/File:Small_park_in_logatec_matchbox_pinhole_camera.jpg.\n\n\nTom.vettenburg. 2017. “Illustration of the\noptical transfer function and its relation to image quality; CC BY-SA\n4.0 license.” https://en.wikipedia.org/wiki/File:Illustration_of_the_optical_transfer_function_and_its_relation_to_image_quality.svg.\n\n\nTorrance, Kenneth E, and Ephraim M Sparrow. 1967. “Theory for\nOff-Specular Reflection from Roughened Surfaces.” Josa\n57 (9): 1105–14.\n\n\nTournier, Arnaud, F Roy, Y Cazaux, F Lalanne, P Malinge, M Mcdonald, G\nMonnot, and N Roux. 2018. “A HDR 98dB 3.2μm Charge Domain Global Shutter\nCMOS Image Sensor.” In 2018 IEEE International Electron\nDevices Meeting (IEDM), 10–14. IEEE.\n\n\nTreutwein, Bernhard. 1995. “Adaptive Psychophysical\nProcedures.” Vision Research 35 (17): 2503–22.\n\n\nTrussell, H Joel, and Michael J Vrhel. 2008. Fundamentals of Digital\nImaging. Cambridge University Press.\n\n\nTsugawa, H, H Takahashi, R Nakamura, T Umebayashi, T Ogita, H Okano, K\nIwase, et al. 2017. “Pixel/DRAM/Logic 3-Layer Stacked CMOS Image\nSensor Technology.” In 2017 IEEE International Electron\nDevices Meeting (IEDM), 3–2. IEEE.\n\n\nTsujimura, Takatoshi. 2017. OLED Display Fundamentals and\nApplications. 2nd ed. John Wiley & Sons.\n\n\nUngerleider, Leslie G, and Mortimer Mishkin. 1982. “Two Cortical\nVisual Systems.” In Analysis of Visual Behavior, edited\nby David J Ingle, Melvyn A Goodale, Richard JW Mansfield, et al.,\n549–86. Mit Press Cambridge, MA.\n\n\nVESA. 2024. VESA High-Performance Monitor and Display Compliance\nTest Specification. Video Electronics Standards Association.\n\n\nViénot, Françoise, Hans Brettel, and John D Mollon. 1999. “Digital\nVideo Colourmaps for Checking the Legibility of Displays by\nDichromats.” Color Research & Application 24 (4):\n243–52.\n\n\nVolz, Hans G, and Frederick T Simon. 2001. Industrial Color\nTesting. Vol. 2. Wiley-VCH New York.\n\n\nVonHaarberg. 2018a. “Illustration of a\ndiffuse BRDF; CC0 1.0 license.” https://commons.wikimedia.org/wiki/File:BRDF_diffuse.svg.\n\n\n———. 2018b. “Illustration of a glossy BRDF;\nCC0 1.0 license.” https://commons.wikimedia.org/wiki/File:BRDF_glossy.svg.\n\n\n———. 2018c. “Illustration of a mirror BRDF;\nCC0 1.0 license.” https://commons.wikimedia.org/wiki/File:BRDF_mirror.svg.\n\n\nVos, Johannes J. 1978. “Colorimetric and Photometric Properties of\na 2 Fundamental Observer.” Color Research &\nApplication 3 (3): 125–28.\n\n\nWadhwa, Neal, Rahul Garg, David E Jacobs, Bryan E Feldman, Nori\nKanazawa, Robert Carroll, Yair Movshovitz-Attias, Jonathan T Barron,\nYael Pritch, and Marc Levoy. 2018. “Synthetic Depth-of-Field with\na Single-Camera Mobile Phone.” ACM Transactions on Graphics\n(ToG) 37 (4): 1–13.\n\n\nWald, George. 1933. “Vitamin A in the\nRetina.” Nature 132 (3330): 316–17.\n\n\n———. 1945. “Human Vision and the Spectrum.”\nScience 101 (2635): 653–58.\n\n\n———. 1964. “The Receptors of Human Color Vision: Action Spectra of\nThree Visual Pigments in Human Cones Account for Normal Color Vision and\nColor-Blindness.” Science 145 (3636): 1007–16.\n\n\n———. 1968. “Molecular Basis of Visual Excitation.”\nScience 162 (3850): 230–39.\n\n\nWalter, Bruce, Stephen R Marschner, Hongsong Li, and Kenneth E Torrance.\n2007. “Microfacet Models for Refraction Through Rough\nSurfaces.” Rendering Techniques 2007:18th.\n\n\nWandell, Brian A. 1995. Foundations of Vision. Sinauer\nAssociates.\n\n\nWann, John P, Simon Rushton, and Mark Mon-Williams. 1995. “Natural\nProblems for Stereoscopic Depth Perception in Virtual\nEnvironments.” Vision Research 35 (19): 2731–36.\n\n\nWard, Gregory J. 1992. “Measuring and Modeling Anisotropic\nReflection.” In Proceedings of the 19th Annual Conference on\nComputer Graphics and Interactive Techniques, 265–72.\n\n\nWeckler, Gene P. 1967. “Operation of Pn Junction Photodetectors in\na Photon Flux Integrating Mode.” IEEE Journal of Solid-State\nCircuits 2 (3): 65–73.\n\n\nWestover, Lee. 1990. “Footprint Evaluation for Volume\nRendering.” In Proceedings of the 17th Annual Conference on\nComputer Graphics and Interactive Techniques, 367–76.\n\n\nWetzstein, Gordon, Douglas R Lanman, Matthew Waggener Hirsch, and Ramesh\nRaskar. 2012. “Tensor Displays: Compressive Light Field Synthesis\nUsing Multilayer Displays with Directional Backlighting.”\n\n\nWiesel, Torsten N, and David H Hubel. 1966. “Spatial and Chromatic\nInteractions in the Lateral Geniculate Body of the Rhesus\nMonkey.” Journal of Neurophysiology 29 (6): 1115–56.\n\n\nWillassen, Trygve, Johannes Solhusvik, Robert Johansson, Sohrab Yaghmai,\nHoward Rhodes, Sohei Manabe, Duli Mao, et al. 2015. “A 1280× 1080 4.2 μm Split-Diode Pixel Hdr Sensor in\n110 Nm Bsi Cmos Process.” In Proceedings of the International\nImage Sensor Workshop, Vaals, the Netherlands, 8–11.\n\n\nWilliams, Peter L, and Nelson Max. 1992. “A Volume Density Optical\nModel.” In Proceedings of the 1992 Workshop on Volume\nVisualization, 61–68.\n\n\nWodnicki, Robert, Gordon W Roberts, and Martin D Levine. 1995. “A\nFoveated Image Sensor in Standard CMOS Technology.” In\nProceedings of the IEEE 1995 Custom Integrated Circuits\nConference, 357–60. IEEE.\n\n\nWolfWings. 2008a. “Barrel distortion visual\nexample; released into the public domain by the copyright\nholder.” https://commons.wikimedia.org/wiki/File:Barrel_distortion.svg.\n\n\n———. 2008b. “Pincushion distortion visual\nexample; released into the public domain by the copyright\nholder.” https://commons.wikimedia.org/wiki/File:Pincushion_distortion.svg.\n\n\nWrenninge, Magnus, and Nafees Bin Zafar. 2011. “Production Volume\nRendering: Siggraph 2011 Course.” In ACM SIGGRAPH 2011\nCourses, 1–71.\n\n\nWright, WD. 1928. “A Trichromatic Colorimeter with Spectral\nPrimaries.” Transactions of the Optical Society 29 (5):\n225.\n\n\n———. 1930. “A Re-Determination of the Mixture Curves of the\nSpectrum.” Transactions of the Optical Society 31 (4):\n201.\n\n\nWright, William David. 1929. “A Re-Determination of the\nTrichromatic Coefficients of the Spectral Colours.”\nTransactions of the Optical Society 30 (4): 141.\n\n\nWyszecki, Günther, and WS Stiles. 1982. Color Science: Concepts and\nMethods, Quantitative Data and Formulae. John wiley & sons.\n\n\nXu, Han, Ningchao Lin, Li Luo, Qi Wei, Runsheng Wang, Cheng Zhuo,\nXunzhao Yin, Fei Qiao, and Huazhong Yang. 2021. “Senputing: An\nUltra-Low-Power Always-on Vision Perception Chip Featuring the Deep\nFusion of Sensing and Computing.” IEEE Transactions on\nCircuits and Systems I: Regular Papers 69 (1): 232–43.\n\n\nXu, Jiangtao, Liuqin Shu, Zhiyuan Gao, Quanmin Chen, and Kaiming Nie.\n2022. “Analysis and Parameter Optimization of High Dynamic Range\nPixels for Split Photodiode in CMOS Image Sensors.” IEEE\nSensors Journal 22 (7): 6748–54.\n\n\nYantis, Steven, and Richard A Abrams. 2017. Sensation and\nPerception. 2nd ed. Worth Publishers.\n\n\nYasutomi, Keita, Shinya Itoh, and Shoji Kawahito. 2011. “A\nTwo-Stage Charge Transfer Active Pixel CMOS Image Sensor with Low-Noise\nGlobal Shuttering and a Dual-Shuttering Mode.” IEEE\nTransactions on Electron Devices 58 (3): 740–47.\n\n\nYau, KW. 1994. “Phototransduction Mechanism in Retinal Rods and\nCones. The Friedenwald Lecture.” Investigative Ophthalmology\n& Visual Science 35 (1): 9–32.\n\n\nYau, KW, and Denis A Baylor. 1989. “Cyclic GMP-Activated\nConductance of Retinal Photoreceptor Cells.” Annual Review of\nNeuroscience 12 (1): 289–327.\n\n\nYau, KW, and Roger C Hardie. 2009. “Phototransduction Motifs and\nVariations.” Cell 139 (2): 246–64.\n\n\nYifan, Wang, Felice Serena, Shihao Wu, Cengiz Öztireli, and Olga\nSorkine-Hornung. 2019. “Differentiable Surface Splatting for\nPoint-Based Geometry Processing.” ACM Transactions On\nGraphics (TOG) 38 (6): 1–14.\n\n\nYokoyama, Toshifumi, Masafumi Tsutsui, Yoshiaki Nishi, Ikuo Mizuno,\nVeinger Dmitry, and Assaf Lahav. 2018. “High Performance 2.5μm Global Shutter Pixel with New\nDesigned Light-Pipe Structure.” In 2018 IEEE International\nElectron Devices Meeting (IEDM), 10–15. IEEE.\n\n\nYoung, Christopher, Alex Omid-Zohoor, Pedram Lajevardi, and Boris\nMurmann. 2019. “A Data-Compressive 1.5/2.75-Bit Log-Gradient QVGA\nImage Sensor with Multi-Scale Readout for Always-on Object\nDetection.” IEEE Journal of Solid-State Circuits 54\n(11): 2932–46.\n\n\nYoung, Thomas. 1802. “II. The Bakerian Lecture. On the Theory of\nLight and Colours.” Philosophical Transactions of the Royal\nSociety of London, no. 92, 12–48.\n\n\nZhang, Yang, Fan Mao, Huawei Mu, Minwei Huang, Yongbo Bao, Lili Wang,\nNai-Kei Wong, et al. 2021. “The Genome of Nautilus Pompilius\nIlluminates Eye Evolution and Biomineralization.” Nature\nEcology & Evolution 5 (7): 927–38.\n\n\nZhu, Yuhao. 2020. “How the CIE 1931 RGB Color\nMatching Functions Were Developed from the Initial Color Matching\nExperiments.” https://yuhaozhu.com/blog/cmf.html.\n\n\n———. 2021. “Principles and Practices of\nChromatic Adaptation.” https://yuhaozhu.com/blog/chromatic-adaptation.html.\n\n\n———. 2022a. “A pedagogical ISP pipeline in\nPython.” https://github.com/horizon-research/isp.\n\n\n———. 2022b. “Exploring Camera Color Space and\nColor Correction.” https://horizon-lab.org/colorvis/camcolor.html.\n\n\n———. 2022c. “Interative Tutorial: Building a\nColor Cube.” https://horizon-lab.org/colorvis/colorcube.html.\n\n\n———. 2022d. “Interative Tutorial: Building a\nColor Space From Cone Fundamentals.” https://horizon-lab.org/colorvis/cone2cmf.html.\n\n\n———. 2022e. “Interative Tutorial:\nChromaticity, Gamut, and the Scary World of Imaginary\nColors.” https://horizon-lab.org/colorvis/chromaticity.html.\n\n\n———. 2022f. “Interative Tutorial: CIE 1931 XYZ Color\nSpace.” https://horizon-lab.org/colorvis/xyz.html.\n\n\n———. 2022g. “Interative Tutorial:\nUnderstanding and Modeling Color Blindness.” https://horizon-lab.org/colorvis/colorblind.html.\n\n\n———. 2022h. “Interative Tutorial: Visualizing Human Visual\nGamut.” https://horizon-lab.org/colorvis/gamutvis.html.\n\n\nZhu, Yuhao, Ethan Chen, Colin Hascup, Yukang Yan, and Gaurav Sharma.\n2024. “Computational Trichromacy Reconstruction: Empowering the\nColor-Vision Deficient to Recognize Colors Using Augmented\nReality.” In Proceedings of the 37th Annual ACM Symposium on\nUser Interface Software and Technology, 1–17.\n\n\nZwicker, Matthias, Hanspeter Pfister, Jeroen Van Baar, and Markus Gross.\n2001a. “EWA Volume Splatting.” In Proceedings\nVisualization, 2001. VIS’01., 29–538. IEEE.\n\n\n———. 2001b. “Surface Splatting.” In Proceedings of the\n28th Annual Conference on Computer Graphics and Interactive\nTechniques, 371–78.",
    "crumbs": [
      "References"
    ]
  }
]